# 图3.4：知识图谱与预训练模型融合方法

```
方法一：知识增强的预训练 (ERNIE)

输入序列: [CLS] 苹果 发布 iPhone 15 [SEP]

ERNIE处理:
┌─────────────────────────────────────────────────────────────┐
│ 原始:   苹果 [MASK] [MASK] iPhone15                        │
│           ↓                                                 │
│ 实体层: 苹果 -Brand→ Apple                                   │
│         iPhone15 -Type→ 智能手机                            │
│           ↓                                                 │
│ 增强:  [苹果-Apple] [发布] [iPhone15-智能手机]              │
│                                                             │
│ 训练目标: MLM + 实体级别掩码预测                            │
└─────────────────────────────────────────────────────────────┘

方法二：K-BERT - 知识注入

原始输入: 苹果发布了iPhone 15

K-BERT处理:
┌─────────────────────────────────────────────────────────────┐
│ 知识树 (Knowledge Tree):                                   │
│                                                             │
│   苹果 ──品牌──→ Apple                                      │
│   iPhone15 ──是──→ 智能手机                                │
│   iPhone15 ──系列──→ iPhone                                │
│                                                             │
│ 融合后的输入序列:                                           │
│ [CLS] 苹果 [SEP] 苹果-品牌-Apple [SEP]                     │
│ 发布 [SEP] iPhone15 [SEP] iPhone15-是-智能手机 [SEP]        │
│                                                             │
│ Visible Matrix (控制知识可见性):                            │
│ [1 1 1 1 1 1]                                               │
│ [1 1 1 0 0 0]  (苹果-Apple知识仅对苹果可见)                 │
│ [1 1 1 1 1 1]                                               │
│ [1 0 0 1 1 1]  (iPhone15-智能手机知识对iPhone15可见)        │
│ [1 1 1 1 1 1]                                               │
└─────────────────────────────────────────────────────────────┘
