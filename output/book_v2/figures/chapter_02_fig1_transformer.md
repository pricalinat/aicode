# 图2.1：Transformer架构图

```
输入层:
┌─────────────────────────────────────────────────┐
│  Input Embedding + Positional Encoding          │
└─────────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────────┐
│  Encoder Stack (N=6 layers)                    │
│  ┌───────────────────────────────────────────┐  │
│  │ Layer 1: Multi-Head Attention + FFN      │  │
│  │ Layer 2: Multi-Head Attention + FFN      │  │
│  │ ...                                        │  │
│  │ Layer 6: Multi-Head Attention + FFN      │  │
│  └───────────────────────────────────────────┘  │
└─────────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────────┐
│  Decoder Stack (N=6 layers)                     │
│  ┌───────────────────────────────────────────┐  │
│  │ Layer 1: Masked MHA + Cross Attn + FFN   │  │
│  │ Layer 2: Masked MHA + Cross Attn + FFN   │  │
│  │ ...                                        │  │
│  │ Layer 6: Masked MHA + Cross Attn + FFN   │  │
│  └───────────────────────────────────────────┘  │
└─────────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────────┐
│  Linear + Softmax Output                        │
└─────────────────────────────────────────────────┘
```
