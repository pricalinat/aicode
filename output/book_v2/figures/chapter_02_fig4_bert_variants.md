# 图2.4：BERT变体模型对比

| 模型 | 参数规模 | 层数 | 隐藏维度 | 注意力头 | 主要创新 |
|------|---------|------|---------|---------|----------|
| BERT-Base | 110M | 12 | 768 | 12 | 基准模型 |
| BERT-Large | 340M | 24 | 1024 | 16 | 更大规模 |
| RoBERTa | 125M/355M | 12/24 | 768/1024 | 12/16 | 动态掩码，更长训练 |
| ALBERT | 12M/60M | 12/24 | 768/1024 | 12/16 | 参数共享，因式分解 |
| DistilBERT | 66M | 6 | 768 | 12 | 知识蒸馏 |
| ELECTRA | 14M/110M | 12/24 | 256/768 | 4/16 | 替换Token检测 |

技术对比:
┌─────────────────────────────────────────────────────────────────┐
│  MLM (Masked LM):        [MASK] → 预测被掩盖的token           │
│  RTD (Replaced Token):  预测token是否被替换                    │
│  SOP (Sentence Order):   预测句子顺序而非是否连续              │
└─────────────────────────────────────────────────────────────────┘

训练效率对比 (基于GLUE基准):
BERT-Base:    ████████████ 100%
RoBERTa:      ██████████████ 105%  
ALBERT:       ████████████ 98%
DistilBERT:   ██████████ 85%
ELECTRA:      ██████████████ 107%
