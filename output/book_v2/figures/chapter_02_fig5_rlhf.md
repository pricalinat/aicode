# 图2.5：ChatGPT RLHF训练流程

```
┌─────────────────────────────────────────────────────────────────────┐
│                    RLHF (人类反馈强化学习)                          │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  Step 1: 有监督微调 (SFT)                                          │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  人类标注的对话数据                                          │   │
│  │  Q: 如何更换空调滤芯?                                        │   │
│  │  A: 首先关闭电源，找到滤芯位置...                            │   │
│  │       ↓                                                      │   │
│  │  使用GPT-3.5微调，学习模仿人类回复                          │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                         ↓                                           │
│  Step 2: 奖励模型训练 (RM)                                         │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  对同一问题生成多个回复                                      │   │
│  │  Q: 如何更换空调滤芯?                                        │   │
│  │  A1: 首先... (人类打分: 5/5) ✓优秀                          │   │
│  │  A2: 关闭电源，拔掉插头... (人类打分: 3/5)                  │   │
│  │  A3: 滤芯在... (人类打分: 1/5) ✗差                          │   │
│  │       ↓                                                      │   │
│  │  训练Reward Model预测人类偏好                               │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                         ↓                                           │
│  Step 3: PPO强化学习                                              │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  使用PPO算法优化SFT模型                                     │   │
│  │       ↓                                                      │   │
│  │  最大化Reward Model的奖励                                   │   │
│  │       ↓                                                      │   │
│  │  同时加入KL散度惩罚，防止偏离SFT太远                         │   │
│  │       ↓                                                      │   │
│  │  最终得到ChatGPT模型                                        │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```
