# 图2.2：自注意力机制计算流程

```
输入序列: [The, cat, sat, on, the, mat]

Step 1: 为每个token生成Q, K, V向量
┌────────────────────────────────────────────────────┐
│ Token   →   Query(Q)   Key(K)   Value(V)         │
│ "cat"   →   q_cat      k_cat     v_cat          │
│ "sat"   →   q_sat      k_sat     v_sat          │
│ ...     →   ...        ...       ...             │
└────────────────────────────────────────────────────┘

Step 2: 计算注意力分数 (Scaled Dot-Product)
┌────────────────────────────────────────────────────┐
│ Attention Score = Q × K^T / √d_k                  │
│                                                      │
│ q_cat · k_cat   q_cat · k_sat   q_cat · k_on ... │
│ q_sat · k_cat   q_sat · k_sat   q_sat · k_on ... │
│ ...          ...        ...       ...            │
└────────────────────────────────────────────────────┘

Step 3: Softmax归一化
┌────────────────────────────────────────────────────┐
│ Softmax(QK^T/√d_k)                                 │
│                                                      │
│  0.45    0.30    0.15   ...  (attention weights) │
└────────────────────────────────────────────────────┘

Step 4: 加权求和得到输出
┌────────────────────────────────────────────────────┐
│ Output = Attention Weights × V                     │
│                                                      │
│ o_cat = 0.45×v_cat + 0.30×v_sat + 0.15×v_on...  │
└────────────────────────────────────────────────────┘
```
