{
  "paper_id": "2308.01308",
  "title": "Masked Sequence Basket",
  "category": "product_matching",
  "year": 2023,
  "timestamp": "2026-03-01T14:37:24.444955",
  "summary": "This paper defines next novel basket recommendation NNBR, a variant of next basket recommendation that recommends a next basket made only of items a user has never purchased before. The authors motivate NNBR as both practically useful in grocery shopping and a clearer way to evaluate exploration, noting that many existing next basket recommendation methods perform well largely by predicting repeat items rather than truly discovering novel ones.\n\n*BTBR with a proper masking and swapping strategy is the new state-of-the-art method w.r.t. the NNBR task.*  \n*BTBR with a properly selected masking and swapping strategy can substantially improve NNBR performance.*\n\nTo address NNBR, the paper proposes BTBR, a simple bi directional transformer model that flattens a basket sequence into an item sequence while giving all items in the same basket a shared basket position embedding, aiming to preserve item to item correlations within and across baskets without learning heavy basket representations. The training design is central, with multiple masking objectives intended to control what signals the model learns from repeat and novel items:\n- Item level random masking: standard cloze style random masking over the flattened sequence\n- Item level select masking: select items, then mask all occurrences of each selected item to prevent shortcut learning from repetition\n- Basket level all masking: mask the entire last basket and predict all items in it\n- Basket level explore masking: remove repeat items in the last basket and mask only novel items there\n- Joint masking: pretrain with item level masking, then fine tune with basket level masking\nThe paper also adds an item basket swapping augmentation during item level training, moving some items to nearby baskets to enrich within basket interactions while risking temporal noise.\n\nExperiments on three public grocery datasets TaFeng, Dunnhumby, and Instacart evaluate Recall and nDCG for top K novel items, comparing against several representative baselines and two baseline training regimes that either keep all labels or keep only novel labels. Key findings are that baseline performance varies widely across datasets, and that training baselines only on novel labels often helps but can also hurt, suggesting repeat item labels sometimes carry useful correlation signals. BTBR improves NNBR results substantially, but the best masking strategy depends on dataset characteristics such as how strict temporal order seems to be, with item select masking consistently beating item random masking and swapping helping on some datasets while harming on others. Joint pretrain then fine tune is presented as a robust default that consistently beats the best baseline across datasets, even when it does not always surpass the best single strategy per dataset; the paper closes by arguing that blindly optimizing only for exploration can be suboptimal and that separating repetition and exploration is important when designing grocery recommender systems.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}