{
  "paper_id": "2105.00388",
  "title": "Billion Scale Pkgm",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper presents PKGM, a pre-trained knowledge graph model built to serve knowledge from a billion-scale e-commerce product knowledge graph as vectors, so downstream systems can use item knowledge without pulling triples or designing bespoke knowledge-infusion pipelines. The authors motivate this with operational pain points in production KG usage, plus KG incompleteness that can bias downstream tasks, and they target three core query needs: whether an entity has a relation, what the tail entity is for a given head and relation, and completing missing tails during serving. *Currently, our product KG contains 70+ billion triples and 3+ million rules.*\n\nPKGM has two modules that mimic common KG queries in continuous space. The Triple Query Module uses a TransE-style translation objective to encode triples, learning entity and relation embeddings so that for positive triples h + r is close to t, with negative sampling and a margin-based ranking loss. The Relation Query Module models relation existence by learning a relation-specific transformation matrix for each relation, scoring whether an entity should have a relation via the distance between Mrh and the relation embedding, treating the zero vector as the existence target. After training, the model provides two kinds of service vectors for any entity–relation pair: a predicted tail embedding from h + r and an existence indicator vector from Mrh − r, enabling implicit query answering and completion by vector arithmetic rather than symbolic triple retrieval. *We could get the inferred tail entity t even triple (h, r, t) /∈ K, which greatly overcomes the incomplete-ness disadvantages of KG.*\n\nThe paper also proposes general integration recipes for embedding-based downstream models: for models that ingest sequences of embeddings, append PKGM service vectors to the sequence; for models with a single entity embedding, concatenate paired service vectors per relation and average across relations into one summary vector, then concatenate with the original embedding. Experiments pre-train on a large subgraph PKG-sub with 142,634,045 items, 426 relations, and 1,366,109,966 triples (after filtering low-frequency attributes), selecting 10 key relations per item category; training uses TensorFlow and Graph-learn with 64-d embeddings, batch size 1000, 2 epochs, an 88GB model, and a reported 15-hour distributed run. Downstream results show consistent gains on (1) item classification using BERT on Chinese titles over 1293 categories with constrained per-class samples, where PKGM variants improve Hit@k and accuracy and relation-query vectors often help more; (2) item alignment framed as paraphrase identification with BERT across three product categories, where PKGM improves ranking metrics and accuracy, especially in lower-data settings; and (3) item recommendation using NCF on a Taobao interaction sample of 29,015 users and 37,847 items, where PKGM-augmented models improve HR@k and NDCG@k, again with relation-query signals contributing the largest boosts. The conclusion emphasizes uniform vector-space knowledge service, completion capability, and triple-data independence, and suggests extending PKGM to more tasks and alternative service-vector application methods.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2105.00388_billion_scale_pkgm.pdf"
}