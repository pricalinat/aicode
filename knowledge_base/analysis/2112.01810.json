{
  "paper_id": "2112.01810",
  "title": "Siamese Bert",
  "category": "product_matching",
  "status": "success",
  "summary": "The paper presents a real time relevance ranking approach for web search using a siamese transformer architecture built on an Electra small language model, designed to meet tight latency constraints by precomputing document embeddings offline and comparing them with a query embedding at serving time. It reports deployment in a commercial Czech search engine, where adding the model signal as a feature improves ranking quality, and it positions the work as filling a gap in non English, resource lighter relevance research by focusing on Czech.\n\n*The model is already deployed in a commercial search engine and it improves production performance by more than 3%.* The authors main deliverables are (1) a new Czech relevance dataset DaReCzech with about 1.6 million query document pairs and human assigned relevance levels, and (2) a Czech pretrained Electra small model Small E Czech trained on a large in house Czech web corpus. DaReCzech records include a query, URL, document title, and a document representation formed by concatenating normalized title terms, a processed URL, and a cleaned body text extract; the dataset is split into Train big, Train small, Dev, and Test with no query overlap, and evaluation emphasizes Precision at 10 aligned with production measurement.\n\n*The dataset consists of more than 1.6M annotated query-documents pairs, which makes it one of the largest available datasets for this task.* Methodologically, the paper contrasts a high quality but expensive query doc cross encoder with the faster siamese bi encoder, then narrows the quality gap via a custom interaction module beyond cosine similarity, a weighted combination of Electra layer outputs for the CLS embedding, knowledge distillation using the query doc model as teacher, initialization from teacher weights, and a small two model ensemble. On DaReCzech, the strongest non ensemble siamese variant slightly surpasses the existing production GBRT baseline when used as an added feature, and the ensemble feature yields a larger lift; ablations show body text extract contributes the most among document parts, more training data steadily improves results, and richer interaction modules improve quality at a speed cost. The paper also evaluates alternative base models, reports that larger monolingual Czech models can raise offline quality but may be harder to serve, and measures ONNX execution and quantization, noting the interaction module can be accelerated with small quality change while embedding quantization has a larger tradeoff.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2112.01810_siamese_bert.pdf"
}