{
  "paper_id": "2402.13587",
  "title": "Multimodal Ict Ecommerce",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper proposes a new e-commerce product description generation setting that uses a product image plus a small set of marketing keywords, aiming to produce descriptions that are both accurate to product-specific visual details and explicitly aligned with the keywords. The authors argue that conventional multimodal encoder-decoder pipelines trained at scale tend to produce generic, sometimes inaccurate copy because same-category products share similar wording and models overfit common phrases. *ModICT significantly improves the accuracy by up to 3.3% on Rouge-L and diversity by up to 9.4% on D-5*.\n\nThe core method, ModICT, is a parameter-efficient multimodal in-context tuning approach that retrieves a visually similar product and uses its image, marketing keywords, and human-written description as an in-context reference to guide generation. *we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces the similar product sample as the reference*. Key design points include:\n- Similar-sample retrieval: use a frozen Chinese CLIP visual encoder to retrieve the most similar same-category training item via cosine similarity on global image features.\n- Multimodal reference construction: map image features into the language representation space with a learnable Feature Transformer to form a short visual prefix, then combine the similar itemâ€™s keywords and description as context alongside the target image and keywords.\n- Efficient tuning with frozen backbones: keep the visual encoder and the main language model generation parameters frozen, and only train the modules that create multimodal references and dynamic prompts; for seq2seq models, tune part of the encoder, and for decoder-only LLMs, add continuous prompt vectors projected by a small adapter MLP.\n\nTo evaluate, the authors build MD2T, a three-category Chinese dataset (Cases and Bags, Clothing, Home Appliances) derived from an existing multimodal e-commerce summarization corpus, where marketing keywords are constructed using dictionary-guided segmentation and filtering to remove easily visual attributes like color or shape. Across automatic metrics (BLEU, ROUGE, BERTScore) and diversity metrics based on Distinct n-grams, ModICT generally outperforms baselines such as MMPG, M-kplug, Oscar, and Oscar-GPT, with ablations indicating that the in-context tuning component mainly boosts diversity while adapters can boost accuracy but sometimes reduce diversity. Additional analyses report that more in-context examples tend to increase diversity with a slight accuracy drop, smaller training sets can still work well in some categories, and human evaluation (coherence, accuracy, richness, relevance) favors ModICT over baselines while still trailing human-written descriptions in richness.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2402.13587_multimodal_ict_ecommerce.pdf"
}