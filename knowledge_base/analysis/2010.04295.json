{
  "paper_id": "2010.04295",
  "title": "Widget Captioning",
  "category": "mini_program_service",
  "year": 2020,
  "timestamp": "2026-03-01T13:59:56.340316",
  "summary": "This paper introduces widget captioning, a multimodal generation task that produces short natural-language descriptions for individual mobile UI elements that lack accessible captions. The goal is to improve mobile accessibility and enable language-based interaction by generating element-level text from both the UI screenshot and the UI structure represented as a view hierarchy, while leveraging on-screen context from neighboring elements.\n\n*Our dataset contains 162,859 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens.* The authors build a large Android UI corpus based on RICO and an additional crawling process, filter for screens with reliable view hierarchies, and focus on visible, clickable leaf-node elements. They define missing captions using Android accessibility fields, analyze how missing captions vary by element category, and collect multiple crowd-written captions per target element using an interface that highlights the element on the screenshot and asks for concise, functional descriptions; they report an average of 2.66 captions per element and show that most captions are 2 to 3 words, with substantial long-tail diversity that favors sequence generation over fixed-phrase classification.\n\nThe modeling approach follows an encoder-decoder design: a Transformer encodes structural and contextual information across all elements on a screen, a ResNet encodes a cropped element image, and a Transformer decoder generates the caption token by token. Element representations combine image features with structured properties such as widget text (or a special missing-text embedding), widget type, clickability, normalized bounds, and hierarchy position features; captions for multiple missing elements on the same screen are decoded in parallel during inference. Experiments use app-wise train validation test splits to prevent style leakage across splits, evaluate with standard captioning metrics, and show that adding structural and especially screen-context encoding improves accuracy over pixel-only and template-style baselines; human verification on the test set reports substantially higher endorsement for the full context-aware model than for an image-only model. *The overall endorsement of raters for generated captions is 78.64% for the full model and 62.42% for the PixelOnly model.* Error analysis highlights confusions from nearby elements, similar-looking controls, overly generic predictions, and cases where outputs are semantically correct but penalized by automatic metrics, and the conclusion points to improved encoders and better coverage of rare long-tail phrases as key future directions.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}