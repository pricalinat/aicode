{
  "paper_id": "2505.08128",
  "title": "Efficient Ab",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T13:51:59.018476",
  "summary": "# Beyond Basic A/B Testing: Improving Statistical Efficiency for Business Growth\n\n## Summary\n\nThis paper, authored by researchers at LinkedIn, addresses critical limitations of standard t-test-based A/B testing approaches commonly used in industry. The authors demonstrate that traditional methods suffer from low statistical power in business settings due to small sample sizes, non-Gaussian distributions (heavy-tailed, right-skewed revenue metrics), zero-inflation in conversion data, and the need to measure Return-on-Investment (ROI). The paper proposes a unified framework combining estimating equations, U statistics, and doubly robust methods to achieve superior statistical efficiency while maintaining theoretical guarantees.\n\n## Key Challenges in Business A/B Testing\n\nBusiness-setting experiments face four distinct challenges that standard approaches fail to address adequately. First, sample sizes are typically small because increasing experimentation incurs direct business costs. Second, core metrics like revenue exhibit right-skewed heavy-tailed distributions that violate normality assumptions. Third, conversion events are sparse, resulting in zero-inflated data where over 95% of users generate no revenue. Fourth, ROI measurement requires principled trade-offs between revenue and cost metrics, which heuristics like separate t-tests cannot handle properly.\n\n## Core Statistical Methods\n\nThe paper develops several complementary approaches. **Regression Adjustment (RA)** leverages covariates to reduce variance and control for confounding, achieving efficiency gains when covariates are independent of treatment assignment and explain response variance—this mechanism also underlies CUPED methods. **Generalized Estimating Equations (GEE)** exploit repeated measurements over time, with efficiency gains scaling with the number of measurement points and dependent on the correlation structure among observations. **Mann-Whitney U statistics** provide robustness to non-Gaussian distributions, with Pitman efficiency analysis showing substantial gains over t-tests for heavy-tailed distributions (including infinite efficiency for Cauchy distributions). **Zero-Trimmed U (ZTU)** specifically addresses zero-inflation by trimming equal proportions of zeros before ranking, achieving higher power than standard Mann-Whitney U when positive value sparsity is high.\n\nThe flagship contribution is the **Doubly Robust Generalized U (DRGU)**, which combines covariate adjustment, distribution robustness, and flexible treatment effect definitions in one framework. DRGU uses any monotonic kernel function to define treatment effect, achieves the semi-parametric efficiency bound (minimum variance among all regular estimators), and maintains consistency when either the propensity score or outcome model is correctly specified—the key doubly robust property.\n\n## Theoretical Results\n\nThe authors establish several important theoretical results. They derive asymptotic relative efficiency formulas comparing each method to t-tests under various distributions. For DRGU, they prove that the estimator attains the semi-parametric efficiency bound and provide asymptotic normality results. They also develop computationally efficient algorithms for large-scale implementation, decoupling optimization (mini-batch Fisher scoring) from inference (Monte Carlo variance estimation with anchor-partner schemes), reducing computational complexity from O(n²) to O(n) without losing asymptotic efficiency.\n\n## Empirical Findings\n\nSimulation studies demonstrate substantial improvements. Under confounding, t-tests exhibit severe type I error inflation while RA maintains correct error rates with higher power. GEE consistently outperforms snapshot regression across sample sizes. For zero-inflated heavy-tailed data, ZTU achieves 2-10x power gains over t-tests while controlling type I error.\n\nThree real-world LinkedIn applications validate the methods. In email marketing, ZTU detected a significant +0.94% conversion value lift (p<0.001) where t-test failed (p=0.249). For feed targeting, regression adjustment revealed a 1.84% ROI lift (p<0.001) that simple t-tests missed. In paid search campaigns with only 64 units, GEE achieved marginal significance (p=0.051) versus snapshot regression (p=0.184), and DRGU attained significance (p=0.045) by leveraging the heavy-tailed distribution.\n\nThe authors open-sourced the implementation as the robustInfer package, available on GitHub.\n\n*For normal distribution, r(U, τ) = 3/π; for Laplace, r(U, τ) = 1.5; for lognormal, r(U, τ) increases exponentially with variance parameter*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}