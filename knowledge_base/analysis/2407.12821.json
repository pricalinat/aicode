{
  "paper_id": "2407.12821",
  "title": "Autoflow",
  "category": "mini_program_service",
  "year": 2024,
  "timestamp": "2026-03-01T13:48:15.896126",
  "summary": "# AutoFlow: Automated Workflow Generation for Large Language Model Agents\n\n## Overview\n\nThis paper introduces AutoFlow, a novel framework designed to automatically generate workflows for LLM-based AI agents, eliminating the need for manual workflow design which requires significant effort and domain expertise. The framework represents workflows in natural language programs using the CoRE (Code Representation and Execution) language, which enables easier comprehension and interaction. AutoFlow employs a workflow optimization procedure that iteratively refines workflow quality through reinforcement learning, ensuring robustness and reliability in the generated workflows.\n\n## Key Contributions\n\nThe paper makes three main contributions. First, it introduces AutoFlow as a framework that automatically generates workflows in natural language, reducing human effort while allowing LLMs to precisely interpret the workflows. Second, it proposes two learning methods to incorporate reinforcement learning into workflow generation: a fine-tuning method for open-source LLMs using LoRA adapters, and an in-context learning method for closed-source LLMs like GPT-4. Third, it conducts comprehensive experiments on benchmark datasets validating that AutoFlow produces workflows with higher valid plan rates and better overall performance compared to manually designed workflows##.\n\n Technical Approach\n\nAutoFlow uses the CoRE language system to structure workflows with four components: Step Name (unique identification), Step Type (Process, Decision, or Terminal), Step Instruction (natural language description), and Step Connection (execution flow). The fine-tuning approach uses LoRA adapters for open-source LLMs like Mixtral-8x7B, employing REINFORCE as the reinforcement learning algorithm with average task score as the reward. For closed-source LLMs, the in-context learning method directly includes reward values in prompts to guide workflow refinement without parameter updates. The framework uses one LLM as the workflow generator and another as the interpreter that executes the workflow on validation data to calculate rewards.\n\n## Experimental Results\n\nExperiments conducted on the OpenAGI benchmark demonstrate significant improvements. When using Mixtral as the interpreter LLM, AutoFlow achieved over 40% improvement in average score compared to the best baseline (CoRE), reaching 0.3597 versus 0.2483. When using GPT-4 as the interpreter, AutoFlow showed over 5% improvement, achieving 0.6501 average score compared to CoRE's 0.6104. An interesting finding was that cross-system combinations (Mixtral generator with GPT-4 interpreter, or vice versa) produced synergistic effects that outperformed same-system combinations, suggesting complementary strengths between different LLM systems.\n\n## Related Work and Context\n\nThe research builds upon several areas: LLM agents and workflows (including reasoning approaches like Chain-of-Thought and planning methods), Automated Machine Learning concepts (which inspired the automation approach), and natural language programming systems like CoRE. The paper identifies limitations including the potential inefficiency of reinforcement learning compared to gradient-based methods, and suggests future work exploring teacher-student or adversarial learning paradigms for the generator-interpreter collaboration.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}