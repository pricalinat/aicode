{
  "paper_id": "2302.03328",
  "title": "Rl Multitask",
  "category": "product_matching",
  "year": 2023,
  "timestamp": "2026-03-01T13:37:34.355425",
  "summary": "# Multi-Task Recommendations with Reinforcement Learning\n\nThis paper, published at WWW'23, introduces RMTL, a reinforcement learning enhanced multi-task learning framework that addresses two fundamental limitations in existing recommendation systems: the neglect of session-wise interaction patterns and the challenge of balancing multiple objectives through static loss weights.\n\n## Core Problem\n\nCurrent multi-task learning (MTL) recommendation models suffer from two critical shortcomings. First, most models process user-item interactions as isolated item-wise data points, ignoring the sequential nature of user behavior within sessions—particularly relevant for short-video and e-commerce platforms where clicks and conversions occur during specific user sessions. Second, traditional MTL approaches rely on linear scalarization with constant weights for loss functions, which fails to capture the dynamic relationship between tasks and cannot guarantee convergence to a global optimum. These limitations prevent MTL models from fully leveraging the temporal patterns inherent in real-world recommendation scenarios.\n\n## RMTL Framework Architecture\n\nThe proposed RMTL framework transforms the multi-task recommendation problem into a Markov Decision Process (MDP) and employs an actor-critic reinforcement learning structure to dynamically adjust loss function weights for each training batch. The framework consists of three main components: a state representation network that extracts user-item features through embedding layers and multi-layer perceptrons; an actor network implemented as a two-tower MTL backbone that generates predictions for CTR and CTCVR tasks simultaneously; and dual critic networks that evaluate action values and produce adaptive weights for the combined loss function.\n\nThe key innovation lies in how the loss weights are computed. Rather than using fixed constants, the framework calculates weights as a linear transformation of the Q-value output from the critic networks: ω = 1 - λ * Q(s_t, a_k,t; φ_k), where λ is a punish variable. This enables the system to automatically adjust the importance of each task based on the current state and predicted actions, allowing the model to respond dynamically to different interaction contexts within user sessions.\n\n## Experimental Results\n\nThe researchers evaluated RMTL on two real-world commercial datasets—RetailRocket and Kuairand—comparing against five baseline MTL models including SingleTask, SharedBottom, ESMM, MMoE, and PLE. The results demonstrated that RMTL consistently outperformed all baseline models across both CTR and CTCVR prediction tasks. On the RetailRocket dataset, RMTL achieved AUC improvements of 0.003-0.005 over corresponding baseline models. The framework also validated its compatibility by successfully integrating with multiple existing MTL architectures (ESMM, MMoE, PLE), and showed excellent transferability—critic networks pretrained on one MTL model could improve the performance of other MTL models when applied.\n\n## Significance\n\nThis work represents a significant advancement in multi-task recommendation systems by introducing a general framework that can be applied to most existing MTL models without requiring fundamental architectural changes. The ability to dynamically adjust task weights based on reinforcement learning signals provides a more principled approach to handling the inherent conflicts between different recommendation objectives, such as optimizing for clicks versus conversions. The source code has been made publicly available to facilitate reproducibility and further research in this direction.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}