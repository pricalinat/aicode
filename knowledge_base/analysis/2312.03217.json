{
  "paper_id": "2312.03217",
  "title": "Rethinking Ecommerce Search",
  "category": "product_matching",
  "status": "success",
  "summary": "## Summary\n\nThis position paper argues that modern e-commerce search and recommendation are bottlenecked by two persistent problems: interpreting short, ambiguous queries and understanding products in a knowledge-intensive way that goes beyond catalog fields. It critiques the dominant strategy of converting unstructured sources like reviews and web articles into structured signals via information extraction as expensive and often low quality, and it notes that even seemingly simple queries can be misread by todayâ€™s systems. *E-commerce search presents two fundamental challenges. The first is understanding the user intent behind short search queries, and the second is understanding products in the catalog.*\n\nThe core proposal flips the usual pipeline: convert structured and semi-structured enterprise data into natural language so it can be mixed into the text corpora used to train or fine-tune large language models, then answer search and recommendation needs through an LLM-driven question answering interface. The authors describe a database plus LLM architecture where entities (products, brands, etc.) are rendered into annotated texts that embed entity IDs; at inference time, the model outputs relevant IDs, which are then used to look up current facts like price and availability in the live database before applying application-specific ranking and business logic. *Rather than converting all information into a structured form and then conducting search over a database, we convert all structured and semi-structured data into text and then answer queries through a large language model.*\n\nKey mechanisms and implications:\n- Universal ID representation for LLMs: discusses direct IDs, hierarchical IDs leveraging taxonomies, and using rare random tokens to avoid unintended prior meanings, paired with an external mapping back to true database IDs.\n- Converting data to annotated text: proposes template-based generation at multiple levels, including manually authored templates per column or relationship, query-based templates derived from joins and aggregates (including user engagement and co-purchase patterns), and LLM-assisted generation where diverse descriptions for a few records are distilled into reusable templates.\n- Training and inference approach: favors fine-tuning with a language modeling objective over annotated texts to transfer domain knowledge without requiring labeled datasets, while optionally injecting broader domain-relevant world knowledge; at inference, prompts can request outputs containing product IDs in structured formats and then rely on downstream parsing or entity linking.\n- Research directions and tradeoffs: emphasizes production latency as a major barrier for generative models and explores an encoder-based multi-task classification alternative that outputs scores over product IDs in one pass, along with model compression options (distillation, pruning, quantization). It also outlines personalization via incorporating user history from a feature store into prompts and warns about continual updates causing catastrophic forgetting, suggesting taxonomy-level ingestion to limit what must be learned.\n- Positioning vs related work: contrasts the training-based ingestion approach with retrieval augmented generation, noting context-window and refinement limitations for e-commerce scenarios with many relevant items, and explains why text-to-SQL alone struggles with questions requiring common knowledge beyond schema-bound facts.\n\nThe paper concludes that synthesizing LLM world knowledge with database-backed, time-varying facts could simplify fragmented IR stacks while improving query understanding and knowledge-intensive product matching, but it highlights open engineering and research challenges before this can be practical at scale.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2312.03217_rethinking_ecommerce_search.pdf"
}