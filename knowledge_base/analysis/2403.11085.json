{
  "paper_id": "2403.11085",
  "title": "Paper",
  "category": "mini_program_service",
  "year": 2024,
  "timestamp": "2026-03-01T14:01:15.251391",
  "summary": "The paper introduces **mms**, a benchmark for evaluating how well tool-augmented large language models act as planners for multi-step, multi-modal tasks. It targets realistic scenarios where solving a user request requires chaining multiple tools across modalities, and it is designed to let researchers compare planning strategies, plan representations, and feedback mechanisms under execution. *Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans.* The authors position mms as addressing a gap in prior benchmarks that either lack executable tools, rely on simulated tool execution, or use placeholder inputs that prevent realistic execution feedback.\n\nmms contains **4,427** generated query plan pairs, with **1,565** plans that all three human annotators verified as correct and executable, and a **balanced evaluation subset of 882** examples. Tasks span **1 to 3 tools** per plan, with the balanced subset reported as **70 one-tool**, **159 two-tool**, and **653 three-tool** queries, drawn from **317** representative tool graphs averaging **2.78** unique queries each. The toolset includes **33 executable tools** across three categories: **13 multimodal ML models**, **11 image processing modules**, and **9 free public APIs**, with plans provided in both **JSON** and mapped **Python code** formats; the dataset is built via tool-graph sampling, pairing with real inputs from multiple existing datasets, GPT-4 query generation, rule-based plan generation using tool metadata, and human verification plus filtering to improve tool distribution.\n\nUsing a modular planning agent (LLM plus parser, verifier, and executor) implemented with AutoGen, the paper evaluates **10 LLMs** across **multi-step vs step-by-step planning**, **JSON vs code plan formats**, and **parsing, verification, and execution feedback**, using metrics for tool selection and invocation (tool-F1, argname-F1, and executability pass rate). The main findings are that **multi-step planning** consistently improves tool selection and often pass rate versus step-by-step planning, while **verification and execution feedback** substantially increase executability and argument-name accuracy but can slightly reduce tool-F1 due to incorrect tool changes during fixes. *Verification and execution feedback improve LLMs ability to generate overall executable plans and predict the correct argument names but dont necessarily improve their tool selection ability.* The authors also report that **JSON plans** yield much higher pass rates than code generation even when tool-F1 is similar, and they note limitations such as focusing on sequential (not dynamic) plans, limited exploration of prompt-style variations, and challenges in evaluating open-ended execution results at scale.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}