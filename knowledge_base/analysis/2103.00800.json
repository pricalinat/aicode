{
  "paper_id": "2103.00800",
  "title": "Query Rewriting Cycle",
  "category": "product_matching",
  "status": "success",
  "summary": "This ICDE 2021 paper tackles semantic matching failures in e-commerce search, where relevant items are missed because user queries and item titles use different terms or styles. It proposes a neural query rewriting system that converts a hard or vague user query into a more standard, retrieval-friendly query, using abundant click logs as supervision. The core idea is to model query rewriting as a cycle of two translations: query to item title, then item title back to query, so the system can learn from query–title clicks even without large volumes of clean query-to-query rewrite labels.\n\nMethod-wise, the system trains a forward query-to-title model and a backward title-to-query model, then jointly optimizes them with an added cycle-consistency likelihood that encourages the round trip to reconstruct the original query. Because exact marginalization over all possible titles is intractable, training approximates the cycle term using a small set of top-k synthetic titles generated by the forward model after a warmup phase. For inference, it generates k synthetic titles for a query, expands each into k candidate rewritten queries, and ranks candidates using the combined forward–backward probabilities. To avoid low-diversity outputs from greedy or beam decoding, it introduces a top-n sampling decoder that forces distinct first tokens and then samples from the top n tokens at later steps to balance likelihood and diversity.\n\nThe paper also focuses on production constraints: it precomputes rewrites offline for top popular queries into a key-value store, and for full online coverage proposes a faster direct query-to-query model plus a latency-oriented architecture choice (keeping a transformer encoder while using an RNN decoder) to reduce inference time to about 30 ms on a 32-core CPU. On the retrieval side, it reduces inverted-index overhead by merging the original and rewritten queries into a single syntax tree rather than building one per rewrite. Experiments use 60 days of click logs (about 300 million query–item pairs), show joint training improves translate-back metrics and human-judged relevance versus separate training, and report online A/B results with QRR +0.5219%, UCVR +1.1054%, and GMV −0.0397%.\n\n*Since the summer of 2020, the proposed model has been launched into our search engine production, serving hundreds of millions of users.*\n\n*we optimize the syntax tree construction to still keep only one tree by merging all the rewritten queries and the original query.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2103.00800_query_rewriting_cycle.pdf"
}