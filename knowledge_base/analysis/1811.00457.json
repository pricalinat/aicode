{
  "paper_id": "1811.00457",
  "title": "Test Roll",
  "category": "ecommerce_evaluation",
  "year": 2018,
  "timestamp": "2026-03-01T13:49:42.516830",
  "summary": "This paper presents a new framework for designing A/B tests that explicitly maximizes profit rather than relying on traditional hypothesis testing. The authors derive a closed-form formula for the optimal test sample size, demonstrating that profit-maximizing tests are substantially smaller than those recommended by conventional statistical methods.\n\n**Core Problem**: Traditional A/B testing uses hypothesis testing to determine sample sizes based on statistical significance and power, but this approach ignores the opportunity cost of testing (where some customers receive suboptimal treatments during the test) and fails to account for limited population sizes. The authors reframe the problem as a trade-off between learning during the test phase and earning during the deployment phase.\n\n**Key Derivation**: Under Normal response distributions with Normal priors, the profit-maximizing sample size is n* = N(s²/σ² + 3s²/σ² - 3s²/σ²) / 4, where N is population size, s is response standard deviation, and σ represents prior uncertainty about treatment effects. This formula produces sample sizes that grow sub-linearly with response noise and proportionally to √N, unlike hypothesis testing which grows linearly with variance and independently of population size.\n\n**Main Findings**: The profit-maximizing approach yields dramatically smaller test sizes than hypothesis testing, particularly when responses are noisy or populations are small. The method naturally handles unequal treatment group sizes through asymmetric priors, explaining the common marketing practice of using small holdout groups. *The regret (profit loss compared to perfect information) is O(√N), comparable to Thompson sampling multi-armed bandits while maintaining simpler operational implementation.*\n\n**Empirical Applications**: The authors apply their method to three marketing contexts:\n- **Website testing**: With N=100,000 and click-through data from 2,101 past tests, optimal sample size is 2,284 per group versus 18,468 for hypothesis testing—a 10x reduction\n- **Display advertising**: With N=1,000,000 and high response variance (s=103.77), optimal size is 11,391 per group versus 4.8 million for hypothesis testing\n- **Catalog holdout tests**: Using asymmetric priors from 30 past tests, the optimal design allocates 588 to control and 1,884 to treatment, capturing the prior belief that catalogs increase sales\n\n**Theoretical Comparison**: The paper proves that hypothesis testing has regret Ω(N) (grows linearly with population), while profit-maximizing test & roll achieves O(√N) regret—the same asymptotic bound as Thompson sampling. This means as populations grow larger, the relative advantage of smaller profit-maximizing tests increases.\n\n**Practical Implications**: Marketers should run smaller tests than traditional statistical methods recommend. The framework provides principled guidance for setting holdout group sizes based on prior beliefs about treatment effects. The method fits within existing A/B testing infrastructure without requiring dynamic allocation algorithms.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}