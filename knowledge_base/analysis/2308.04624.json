{
  "paper_id": "2308.04624",
  "title": "Chatbot Benchmark",
  "category": "mini_program_service",
  "status": "success",
  "summary": "The paper proposes an end-to-end chatbot benchmark aimed at evaluating both accuracy and usefulness for LLM-powered chatbots, motivated by the risk of hallucinations producing plausible but incorrect answers. The core idea is to compare a chatbot answer against an expert human golden answer using semantic similarity, specifically cosine similarity between sentence embeddings, rather than relying mainly on word overlap. The authors argue this user-centric setup better reflects real-world support scenarios, while still being measurable and repeatable.\n\nIt situates the E2E benchmark among common evaluation approaches and metrics such as human evaluation, precision and recall for relevance, perplexity, GLUE, BLEU, and ROUGE, noting tradeoffs like subjectivity and cost for human ratings and limited semantic sensitivity for n-gram metrics. For their experiments, they benchmark a publicly available product support chatbot and compute cosine similarity using two embedding methods, Universal Sentence Encoder and Sentence Transformers, then compare those results to ROUGE variants. They report the E2E cosine-similarity scores behave more consistently than ROUGE scores, and show that Sentence Transformers is more sensitive to prompt-engineering improvements than ROUGE, while Universal Sentence Encoder exhibits a roughly 0.5 baseline bias even against random-word negatives.\n\n*We propose here a semantic-similarity based End-to-End E2E Benchmark for benchmarking performance of chatbots.*\n\n*the metric associated with the E2E benchmark, which uses cosine similarity performed well in evaluating chatbots.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2308.04624_Chatbot_Benchmark.pdf"
}