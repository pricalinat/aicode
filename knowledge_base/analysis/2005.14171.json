{
  "paper_id": "2005.14171",
  "title": "Ubr4Ctr",
  "category": "ecommerce_evaluation",
  "status": "success",
  "summary": "The paper tackles a practical bottleneck in click-through rate prediction with sequential user behavior: real systems often truncate long behavior histories (commonly to tens of recent events) to meet serving-time and compute constraints, which can miss long-term dependency and periodicity while still admitting noise if the full history is used. It proposes User Behavior Retrieval for CTR prediction, or UBR4CTR, which reframes the problem as retrieving a small, highly relevant subset of past behaviors from the entire history for each prediction target (user, item, context), then predicting CTR from those retrieved behaviors rather than from the most recent N events.\n\nUBR4CTR has two modules: a learnable retrieval module and a prediction module. The retrieval module uses a self-attention based feature selection model to choose which target features (besides mandatory user id) should form a query, and then issues that query to a search-engine style archive where each behavior is treated as a document and each feature value as a term, stored via a feature-based inverted index; candidate behaviors are intersected by query terms and ranked with BM25, returning top S behaviors. The prediction module is an attention-based deep network that weights retrieved behaviors to form a user representation and feeds it, along with target features, into an MLP to output the click probability. *retrieve more relevant user behaviors than just use the most recent behaviors in user response prediction.*\n\nTraining alternates between optimizing the prediction network and optimizing the feature-selection policy for retrieval:\n- Prediction module: optimized with cross-entropy style log-likelihood plus regularization while holding retrieval fixed.\n- Retrieval feature selection: optimized with REINFORCE due to discrete sampling, using Relative Information Gain derived from normalized entropy as the reward signal.\n- Overall procedure: pretrain prediction with an initial feature selection, then alternate epochs of prediction training and retrieval training until convergence.\n\nExperiments on three large-scale Alibaba datasets (Tmall, Taobao, Alipay) show consistent gains over strong sequential and CTR baselines (including DIN, DIEN, HPMN, MIMN, GRU4Rec, Caser, SASRec), measured by AUC and log-loss. With the same number of behaviors as baselines (20, 20, and 12 retrieved), UBR4CTR reports AUC and log-loss improvements over the second-best results of 4.1% and 9.0% on Tmall, 10.9% and 12.0% on Taobao, and 22.3% and 32.3% on Alipay, and it remains best even when baselines consume much longer sequences while UBR4CTR keeps the same small retrieval size. The paper also argues for deployment feasibility: retrieval adds a search component but avoids keeping full behavior sequences in memory, and measured average inference time is under 1 ms, about 15% to 30% longer than DIEN in their comparison, with noted room for infrastructure optimization. *it does not require maintaining all the T behaviors in memory.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/ecommerce_evaluation/2005.14171_ubr4ctr.pdf"
}