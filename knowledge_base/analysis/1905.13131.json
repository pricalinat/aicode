{
  "paper_id": "1905.13131",
  "title": "Market Basket Prediction",
  "category": "product_matching",
  "status": "success",
  "summary": "The paper tackles personalized market basket prediction: forecasting the exact set of items a customer will buy next, given their sequence of past baskets. It argues prior work relies mostly on hand-crafted association-rule decision rules that (a) require exact item matching, (b) are autoregressive within one customer and miss cross-customer knowledge transfer, and (c) provide no practical confidence or predictability signal for when a prediction should be trusted. The authors propose a similarity-matching approach that compares a customer’s purchase history against all other customers’ histories to find the most similar subsequences, then uses those neighbors to predict the next basket, reporting substantial gains over rule baselines including up to a 4.0× increase in exact-basket correctness in one setting.\n\nThe core method combines item embeddings, Wasserstein distance, and subsequential dynamic time warping into a k-nearest-neighbor predictor called kNN-SDTW, with a fallback heuristic when similarity is insufficient. Key elements are:\n- Product embeddings learned in a word2vec-like fashion over co-occurrence within baskets, so substitutes become close in vector space (eg redwine near whitewine, farther from apples).\n- Basket-to-basket similarity via p-Wasserstein distance computed over sets of item embeddings, allowing comparison of baskets with different sizes and no overlapping items, and providing a numeric predictability score.\n- Fast computation via a tight lower bound on Wasserstein distance that filters many candidates before exact distance evaluation, plus subsequential DTW (via star-padding) to match shorter histories to subsequences of longer ones efficiently.\n- Prediction by taking the next basket from the closest matched neighbor sequence (or aggregating common items for k greater than 1), with a threshold τ that triggers fallback to personal top-n items when average neighbor distance is too high.\n\nEvaluation uses three public datasets and multiple granularities: Instacart aggregated to aisle level, Instacart at product level (restricted to the 500 most frequent products), and the Ta-Feng grocery dataset (also restricted to 500 frequent products). Metrics include F1-score and Jaccard for exact item overlap, plus Wasserstein distance to credit near-substitutes; baselines include global top items, personal top items, repurchase last basket, and association rules adapted to next-basket prediction. Results show the method is competitive on simpler aggregated data but becomes more advantageous on harder large-assortment settings, outperforming baselines by 2.54 percentage points in F1 on product-level Instacart and by nearly 400 percent in F1 on Ta-Feng, while an 80.14 percent lower-bound hit rate substantially reduces computation. The discussion highlights when the approach struggles, such as small assortments dominated by repeats or complex promotion-driven substitution effects, and frames practical use cases in recommender systems for complementary bundles, supply chain preemptive delivery of bundles, and assortment optimization via simulated basket behavior; the model is positioned as having few tunable hyperparameters, with p set to 1 and embeddings fixed to 50 dimensions in experiments.\n\n*The accuracy of identifying the exact market baskets based on state-of-the-art decision rules from the literature is outperformed by a factor of 4.0.*\n\n*For the previous dataset, we obtained a hitrate of 80.14%.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/1905.13131_market_basket_prediction.pdf"
}