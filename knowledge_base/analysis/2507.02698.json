{
  "paper_id": "2507.02698",
  "title": "Marl Pricing",
  "category": "product_matching",
  "year": 2025,
  "timestamp": "2026-03-01T14:32:21.583682",
  "summary": "This paper benchmarks multi agent reinforcement learning for dynamic pricing in supply chains, motivated by the limits of ERP style static, rule based pricing that ignores strategic interaction between market actors. It compares three MARL approaches MADDPG, MADQN, and QMIX against several rule based pricing heuristics inside a custom, data informed market simulator. The core claim is that MARL can produce emergent competitive or cooperative pricing behaviors that static rules cannot capture, but with meaningful trade offs in stability and fairness.\n\n*MADQN agents demonstrate the most aggressive pricing, exhibiting the highest adjustment magnitude (0.0358) and frequency (0.763).*\n\nMethodologically, the study builds realism by grounding the simulator in the Online Retail II transaction dataset from a UK retailer covering December 2009 to December 2011, then filtering to B2B purchases with valid customer IDs and removing invalid records such as negative quantities or non positive prices. It engineers temporal, country, and demand history features at a weekly level, and adds semantic product categories by embedding product descriptions with Sentence BERT MiniLM L6 v2 and clustering into 20 groups. Weekly demand is predicted via a LightGBM model trained on product week aggregates, reporting test R2 of 0.74, and the simulator advances in weekly steps where agents set prices, demand is predicted, profits are computed, and observations are shared for the next decision cycle; experiments typically use four competing agents, each with the same small product portfolio, over 104 week episodes repeated across 30 episodes with multiple runs.\n\n*Fully rule-based environments (4x Rule) exhibit near-perfect fairness (0.9896 Â± 0.0000), zero market share volatility, and low price instability.*\n\nResults emphasize a revenue versus stability and equity tension. Rule based agents produce the most stable prices and the highest fairness, but they show little competitive dynamics and earn far less revenue on average; the reported all rule configuration has mean agent return around 22,817 GBP, with high fairness near 0.99 and low volatility around 0.024. MADQN delivers the largest revenue gains, approaching about 1,000,000 GBP mean agent return in the all MADQN setup, but it is the most destabilizing with high price and market share volatility and the lowest fairness, including Jain fairness around 0.58 and near zero price convergence in the fully learning population. MADDPG is characterized as more conservative and stability oriented, offering a middle ground with higher fairness than MADQN and support for competition while keeping pricing relatively stable; QMIX tends to improve coordination in homogeneous settings but can degrade in mixed populations. The discussion highlights limits driven by the modeled market, especially near inelastic estimated demand that learning agents can exploit by raising prices, plus generalizability concerns from the single retailer and time period, the small fixed product portfolio, and compute cost and stochastic training variability; future work proposes using more elastic markets and more scalable MARL structures such as graph based or hierarchical methods.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}