{
  "paper_id": "2301.03147",
  "title": "Lookalike Customers",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper describes a production-oriented audience expansion system for Walmart e-commerce marketing that finds lookalike customers at very large scale, targeting hundreds of millions of users. The core idea is to represent every customer as a low-dimensional dense embedding, then use fast approximate nearest neighbor search to retrieve customers most similar to a seed list for a campaign. A key requirement throughout is interpretability: similarity is designed to correspond to business-relevant behavior so results can be explained and validated.\n\nThe system has an offline stage that computes embeddings for the full customer universe and builds FAISS indexes, and an online stage that embeds seed customers, searches the index for nearest neighbors, then filters and ranks candidates. Customer similarity is defined in terms of business metrics such as transactions, visits, and engagements, with a concrete example using cosine similarity over per-category purchase vectors. The embedding model uses a two-tower setup trained to make cosine distances between embedding pairs match the defined similarity scores, optimized with an L1 loss, and it ingests multimodal inputs including numerical features, categorical demographics, and a high-dimensional location feature encoded via transfer learning.\n\nExperiments compare model setups using MAE and inference time, including variants with no location embedding, word-embedding-based location encoding, and BERT-based location encoding. BERT yields the best quality but increases inference latency by more than 5x versus the baseline, making it impractical for large offline processing, while word embeddings provide a better quality-latency tradeoff. The paper concludes by positioning the system as scalable and adaptable across campaign goals, and suggests future work combining similarity-based retrieval with classification-based methods plus improved filtering and ranking.\n\n*We use a deep learning based embedding model to represent customers and an approximate nearest neighbor search method to quickly find lookalike customers of interest.*\n\n*Embedding model with word embedding strikes a good balance between quality and inference latency.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2301.03147_lookalike_customers.pdf"
}