{
  "paper_id": "2009.05794",
  "title": "Open Ctr",
  "category": "ecommerce_evaluation",
  "year": 2020,
  "timestamp": "2026-03-01T13:52:11.701637",
  "summary": "# Open Benchmarking for Click-Through Rate Prediction\n\nThis paper presents BARS-CTR, the first open benchmark for CTR prediction, addressing critical reproducibility issues in the field. The authors ran over 7,000 experiments spanning more than 12,000 GPU hours to evaluate 24 different models on two widely-used datasets: Criteo and Avazu. Their work reveals surprising findings: after exhaustive hyperparameter tuning, the performance differences between many state-of-the-art deep models become remarkably small, and some previously reported results show inconsistency due to non-standard data preprocessing and unknown random seeds.\n\n## Background and Motivation\n\nCTR prediction is fundamental to many applications including recommender systems, online advertising, and product search, where even minor accuracy improvements can translate to substantial revenue gains in large-scale platforms. The authors note that an improvement of 0.001 in AUC or logloss is considered practically significant in industrial settings at companies like Google, Microsoft, and Huawei. The field has evolved from simple logistic regression through factorization machines to increasingly complex deep neural networks, with models like Wide&Deep, DeepFM, DCN, xDeepFM, FiBiNET, and AutoInt+ achieving impressive results in published studies.\n\nHowever, the researchers identified a significant problem: despite the abundance of CTR prediction models in literature, there is no standardized benchmark for fair comparison. Different papers use varying data splits, preprocessing steps, and hyperparameter settings, making results incomparable and often non-reproducible. Some studies even report that simpler models like LR outperform complex deep models, which contradicts common understanding and suggests implementation issues or inadequate baseline tuning.\n\n## Benchmark Design and Methodology\n\nThe authors established comprehensive reproducibility requirements covering five critical areas: data preprocessing, model source code, model hyperparameters, baseline source code, and baseline hyperparameters. Their analysis of existing studies, summarized in Table 1, revealed that most papers fail to meet all these requirements. For instance, while some models like xDeepFM and FGCNN provide model source code, they lack baseline implementations and hyperparameter details needed for reproduction.\n\nThe benchmark uses two datasets: Criteo with 46 million instances, 39 fields, and 5.55 million features (26% positive); and Avazu with 40 million instances, 24 fields, and 8.37 million features (17% positive). They created multiple evaluation settings by varying preprocessing parameters like min_count thresholds for filtering infrequent categorical features and embedding dimensions (16 vs. 40). The authors notably fixed defects in prior preprocessing approaches, such as removing the id field in Avazu that had unique values in each sample and discretizing numeric features using log2 transformation for Criteo.\n\nTraining details were carefully controlled with Reduce-LR-on-Plateau schedulers, early stopping after 2-3 epochs without improvement, batch sizes starting at 10000, and exhaustive hyperparameter search averaging 73 experiments per model. They explicitly set random seeds and recorded all configurations to ensure reproducibility through their FuxiCTR benchmarking toolkit built in PyTorch.\n\n## Key Findings and Insights\n\nThe benchmarking results led to several unexpected observations. Many state-of-the-art models like IPNN, DeepFM, DCN, xDeepFM, and ONN all achieve approximately 81.4% AUC on Criteo after proper tuning, with negligible differences between them. The best-reported results from existing papers showed concerning inconsistencies, such as InterHAt performing worse than LR on both datasets and DeepCross underperforming LR on Avazu—outcomes largely due to different data splitting and preprocessing approaches.\n\nAfter retuning, the authors achieved substantial improvements over reported results, with some models showing up to 5‰ improvement in AUC. This demonstrates that proper hyperparameter optimization is essential and that baseline models in many papers may be underoptimized. The authors also documented significant efficiency differences: some models like CCPM, FGCNN, and FiGNN require hours per epoch due to convolution networks, fieldwise interactions, or graph neural networks, while simpler models like FM and LR run in minutes.\n\n## Practical Implications\n\nThe paper identifies several critical factors for performance tuning: appropriate min_count thresholds for filtering rare features, large batch sizes (10000) for faster training and better results, embedding dimensions larger than typically used (40 vs. 16), careful regularization and dropout tuning, and strategic use of batch normalization. The authors argue that data preprocessing often determines the upper bound of model performance and deserves more attention than it typically receives.\n\nFor practitioners, the benchmark provides validated baseline implementations and hyperparameters that can be directly applied. For researchers, it offers a standardized protocol for fair comparison and reveals which models genuinely advance the state of the art. For beginners, the open-sourced FuxiCTR toolkit serves as a comprehensive learning resource for understanding CTR model implementations and tuning strategies.\n\n## Conclusion and Future Directions\n\nThis work represents the first systematic attempt at open benchmarking for CTR prediction, releasing all code, evaluation protocols, hyperparameters, and experimental results through the BARS project. The authors acknowledge limitations including the focus on only two anonymized datasets lacking explicit user-item interaction information, random data splitting rather than temporal splits, and efficiency benchmarking limited to training time without inference latency analysis.\n\nFuture work plans include extending to more datasets from industrial applications, evaluating models with sequential data splitting over time, comprehensive efficiency benchmarking including inference time, and exploring AutoML techniques like Bayesian optimization for faster hyperparameter tuning. The authors emphasize that their benchmark raises the bar for baseline performance and calls for more openness and rigor in CTR prediction research.\n\n*We run DCN for 544 and 496 experiments on Criteo_x4_002 and Avazu_x4_002 respectively, yet only make indistinguishable differences compared to DeepFM.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}