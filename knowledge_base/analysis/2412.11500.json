{
  "paper_id": "2412.11500",
  "title": "Intention Kg",
  "category": "product_matching",
  "year": 2024,
  "timestamp": "2026-03-01T14:39:45.615173",
  "summary": "This paper proposes building an intention knowledge graph that explicitly links users intentions to each other, not just to observed behaviors, so models can represent deliberate, multi-step goal pursuit in sessions. The authors introduce the IGC-RC framework, which combines (1) intention generation from session item histories, (2) conceptualization that abstracts intentions into higher-level concepts for better generalization, and (3) relation classification that adds temporal, co-occurrence, and causal links between intentions. Using the Amazon M2 session dataset, they construct a large Relational Intention Graph, aiming to improve intention prediction and session-based recommendation by capturing how goals connect and evolve.\n\n*We construct an intention graph with 351 million edges, demonstrating high plausibility and acceptance.*  \n*We develop the IGC-RC framework, a novel methodology that integrates user behavior data with large language models to automatically construct rich, multi-faceted intention knowledge graphs.*\n\n**How the graph is built and what it contains**\n- **Intention Generation:** From about 1.2 million English sessions, the method extracts rich product attributes and uses GPT-3.5 to generate about 4.3 million concise verb-phrase intentions, emphasizing diversity and context fit.\n- **Intention Conceptualization:** Each intention is mapped to a small set of concise concepts using an instruction-tuned open model with prompts designed to enforce representativeness, unambiguity, and complementary coverage, enabling links between semantically related intentions.\n- **Intention Relation Classification:** For intention pairs, the system generates template-based natural-language assertions for five relation types (precedence, succession, simultaneity, cause, result), scores plausibility with a Vera-based classifier fine-tuned on expert annotations, and keeps only high-confidence edges (notably using a 0.90 threshold in the main build).\n- **Scale and schema:** The resulting RIG includes sessions, intentions, concepts, and edges spanning item to session, session to intention, intention to concept, and three intention to intention relation families (asynchronous temporal, synchronous co-occurrence, and causality). Reported overall statistics include about 1,176,296 sessions, 2,956,195 intentions, 110,741 concepts, about 4,243,232 nodes, and about 351,880,015 edges.\n\nThe paper evaluates both intrinsic graph quality and downstream utility. Human studies rate intention generation on plausibility and typicality (with high inter-annotator agreement) and assess relation edges with an acceptance definition that counts plausible and somewhat plausible links; the graph achieves a large-scale acceptance rate around the low 80 percent range while remaining orders of magnitude larger than several prior commonsense resources. Automatic evaluations cast tasks as inductive completion problems: ranking correct intentions for unseen sessions, predicting concepts for intentions, and recovering relevant products from intentions; across these, the embedding and graph-based approach is competitive on ranking quality and dramatically faster than direct large-model inference. In extrinsic session-based recommendation, an intention-enhanced model (RIGRec) integrates intention-derived connectivity through a meta-path filtering strategy and graph convolution on items, then uses a SASRec-style session encoder; it reports consistent gains over a broad set of classical, neural, and graph baselines, with ablations attributing improvements to both conceptualization links and commonsense relation paths.\n\nThe authors close with practical and responsible-use considerations: the framework relies on large-model generation (with compute overhead), is validated primarily in e-commerce using Amazon M2 and its English subset, and currently uses a limited set of relation types, leaving broader relation coverage and cross-domain or multilingual testing as future work. They also note privacy protections via anonymized data use and steps intended to avoid including personally identifiable information, and they argue that a precomputed graph can support transparency and fast inference in deployment settings.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}