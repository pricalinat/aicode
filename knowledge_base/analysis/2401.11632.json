{
  "paper_id": "2401.11632",
  "title": "Human Centric Eval",
  "category": "ecommerce_evaluation",
  "year": 2024,
  "timestamp": "2026-03-01T13:47:48.105828",
  "summary": "# Human-Centric Evaluation of Deep Learning Movie Recommenders\n\nThis research paper presents a comprehensive human-centric evaluation of four state-of-the-art deep learning-based recommender systems (DL-RecSys) compared to classic collaborative filtering methods in the movie domain. The study addresses a critical gap in recommender system research: while DL models have dominated benchmark accuracy metrics, their real-world user experience has been largely unexplored.\n\n## Study Design and Methodology\n\nThe researchers recruited 445 active users from MovieLens and generated personalized top-12 movie recommendations using four DL models (NCF, BERT4Rec, SSE-PT, GLocal-K) and two baseline collaborative filtering models (user-based CF and SVD). Participants completed surveys evaluating recommendations across seven human-centric dimensions: Novelty, Diversity, Serendipity, Accuracy, Trustworthiness, Transparency, and Satisfaction. The study also incorporated path analysis to understand how user contextual factors and model attributes influence overall satisfaction.\n\n## Key Findings on Model Performance\n\nThe results reveal a surprising disconnect between offline accuracy and user perception. While DL models excelled at recommending novel and serendipitous items, they significantly underperformed classic CF methods on diversity, trustworthiness, transparency, accuracy, and overall satisfaction. GLocal-K achieved the highest novelty scores, while SSE-PT won on serendipity, but neither translated to superior user satisfaction. NCF performed best among DL models, matching CF on trustworthiness, transparency, and accuracy, yet still fell short of CF on diversity and satisfaction. *GLocal-K is the top performer in Novelty, while SSE-PT wins in Serendipity.*\n\n## Path Analysis and User Perception\n\nThe path analysis uncovered the underlying mechanisms driving user satisfaction. Low diversity and excessive serendipity in DL models directly undermine perceived transparency, which subsequently erodes trust and accuracy perceptions. This chain ultimately leads to lower user satisfaction. Interestingly, user-native contextual factors also played significant roles: users who rated movies more critically (lower average ratings) perceived greater serendipity in DL recommendations, while those who valued novelty and diversity in their preferences reported higher perceived novelty and diversity in recommendations. Users' recognized importance of diversity showed positive correlations with both perceived novelty and diversity of recommendations.\n\n## Qualitative Insights and Design Implications\n\nThe qualitative analysis of 294 free-response submissions revealed three dominant themes. First, users overwhelmingly desired accuracy combined with at least one other desirable attributeâ€”particularly novelty, diversity, or serendipity. Second, trust building emerged as a critical concern, with users relying on accuracy, transparency, and familiarity with recommended items to establish confidence in the system. Third, transparency demands were prominent, with users requesting explanations for recommendations and control over recommendation parameters. One participant articulated this as: *I want a recommender that I can rely on to pick movies to watch. I want to see it recommend a variety of different genres and styles that introduce me to new movies.*\n\n## Limitations and Future Directions\n\nThe authors acknowledge several limitations: the study focused exclusively on the movie domain with active users, excluded cold-start scenarios, evaluated only four DL models, used single-item survey responses, and employed a static survey design rather than interactive interfaces. Future work should explore cross-domain studies, incorporate cold-start users, and develop more interactive evaluation frameworks.\n\n## Conclusion\n\nThis study provides compelling evidence that optimization for offline accuracy metrics does not automatically translate to positive user experience. The research demonstrates that classic collaborative filtering methods often outperform sophisticated DL models on dimensions that matter most to users, including satisfaction, trust, and transparency. The authors argue that future DL-RecSys development should incorporate human-centric evaluation alongside traditional accuracy metrics, potentially leveraging large language models for personalized explanations and implementing diversity-aware re-ranking mechanisms to improve user satisfaction.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}