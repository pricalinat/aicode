{
  "paper_id": "2410.15930",
  "title": "Centrality Aware",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper tackles a recurring failure mode in e-commerce search: top results can be semantically related to a query while still missing what the buyer most likely intends, especially for ambiguous, repetitive, or alphanumeric queries. The authors argue that relevance alone is not enough, and introduce centrality as a buyer-centric signal that captures whether a retrieved title is the typical match for the query intent rather than merely a related item. They curate an internal eBay dataset of queryâ€“title pairs with two human annotation layers: graded relevance on a 1 to 5 scale and a binary centrality score from multiple annotators.\n\nMethodologically, they propose User-intent Centrality Optimization, a supervised fine-tuning step applied to existing embedding-based retrieval models to separate central titles from hard negatives that sit close in embedding space. The approach is paired with a dual-loss optimization strategy combining Multiple Negative Ranking Loss, which helps with many negatives and dataset skew, and Online Contrastive Loss, which focuses learning on hard pairs within a margin; together they aim to pull central pairs closer and push non-central pairs away. Their experimental setup defines several evaluation splits derived from the internal graded dataset, including Common Queries, a balanced variant, a common-string split where the query substring appears in both positive and negative titles, and an alphanumeric split; evaluation uses Precision at k, Recall at k, NDCG at k, and MRR.\n\nResults show that UCO consistently improves retrieval and ranking quality across all curated splits and for multiple encoder backbones, with especially strong gains on the hardest settings such as common-string and alphanumeric queries. Public BERT baselines underperform compared to in-house eBERT variants trained on large-scale product title data, and adding UCO yields substantial increases in metrics like NDCG and MRR; an ablation indicates the combined dual-loss outperforms either loss alone. The paper closes by suggesting future work on making hard negatives less ambiguous and exploring GenAI-based prompt engineering and explainability to better align retrieved titles with user intent.\n\n*UCOleadstoasubstantialimprovementinprod- uctretrievalperformanceacrossallmetrics.*\n\n*Optimisingoneencoderbackboneusingtheaboveparameters takes 30 hours on a single NVIDIA V100 GPU.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2410.15930_centrality_aware.pdf"
}