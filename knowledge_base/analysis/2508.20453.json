{
  "paper_id": "2508.20453",
  "title": "Mcp Bench",
  "category": "mini_program_service",
  "status": "success",
  "summary": "MCP-Bench is a benchmark for tool-using LLM agents that targets realistic, multi-step work where success depends on picking the right tools under underspecified instructions, chaining outputs into later inputs, coordinating across multiple tool ecosystems, and grounding final answers in intermediate results. It is built on the Model Context Protocol and connects agents to 28 live MCP servers spanning 250 tools across domains such as finance, travel, scientific computing, and academic search, aiming to capture longer-horizon and cross-domain workflows than prior API collections or small MCP suites. It also introduces fuzzy task variants that deliberately omit tool names and step-by-step guidance, so agents must infer appropriate tools from context while avoiding distractor servers and tools.\n\nTask construction is automated: the authors analyze tool input output signatures to discover dependency chains, synthesize natural-language tasks from those chains, filter tasks by solvability and practical utility thresholds, then rewrite tasks into instruction-minimal fuzzy versions while preserving critical numeric parameters for mathematically constrained domains. Tasks are executed via multi-turn agent interactions (up to 20 rounds in the paper), with an explicit emphasis on intra-server complementary toolsets and cross-server coupling. Evaluation combines rule-based checks on execution traces with rubric-driven LLM judging, using prompt shuffling and score averaging to reduce ordering sensitivity and improve stability.\n\nKey capabilities and measurements emphasized include:\n- Tool schema understanding and compliance, including nested JSON and constrained arguments, measured via valid tool name rate, schema compliance rate, and execution success rate\n- Tool retrieval and selection under fuzzy instructions, stress-tested by attaching distractor servers to expand the tool space per task\n- Long-horizon planning and cross-server orchestration, assessed via dependency awareness and parallelism and efficiency\n- Information grounding and evidence-based reasoning, judged by how well claims are supported by tool outputs rather than inferred or hallucinated\n\nAcross 104 challenging tasks and 20 evaluated models, the results suggest low-level execution fidelity is often high for strong models, but higher-order planning, dependency handling, and robust cross-server coordination remain the main differentiators. The reported leaderboard shows top overall scores for gpt-5, o3, and gpt-oss-120b, while smaller models lag most in planning effectiveness dimensions even when basic tool invocation succeeds. The paper also reports that multi-server settings tend to degrade weaker models more noticeably, with performance drops concentrated in dependency awareness and parallelism rather than schema correctness.\n\n*MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search.*\n\n*Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2508.20453_MCP_Bench.pdf"
}