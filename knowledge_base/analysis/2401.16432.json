{
  "paper_id": "2401.16432",
  "title": "Ssl Cvr",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper presents a production-focused method to improve conversion rate prediction (CVR given a click) in online advertising when labeled training data is extremely sparse. The core problem is that click-attributed conversions are rare, but adding additional conversions that are not click-attributed can harm calibration, which is critical for bidding strategies like bidding as pCVR times target CPA. The authors propose self-supervised pre-training: train an auto-encoder on all conversion events to learn compact representations, then use the encoder output as an extra feature in the main CVR model that is still trained only on click and click-attributed conversion labels, preserving calibration.\n\nThe approach is built to fit strict real-time auction latency and an incrementally trained factorization-machine variant used at Yahoo called OFFSET. The encoder code is injected into the OFFSET scoring function as an additional linear term, and the auto-encoder itself is designed for tabular categorical inputs using embeddings plus an MLP encoder and a decoder that reconstructs each categorical column via multiclass cross-entropy rather than L2 on one-hot vectors. Because incremental training requires feature stability across daily intervals, the paper defines and tracks metrics for reconstruction quality, stability of codes between consecutive intervals, and interval generalization, and it reports architectural choices that improved these metrics, especially concatenating column embeddings and using a small code dimension. To boost the expressiveness of a linear function over the code without breaking latency constraints, they apply random Fourier features to the code, reporting meaningful offline gains with modest additional computation.\n\n*Training on view-attributed conversions is between 3% and 7% higher depending on the day than that of the model which does not include them.*  \n*On average, the online test bucket showed 0.6% CPM lift and 0.67% revenue lift overall.*\n\nEvaluation covers both offline and online outcomes. Offline, they first show that directly treating view-attributed conversions as positives makes the click-CVR model worse on its intended evaluation, supporting the calibration-related premise. They then show the auto-encoder can learn meaningful patterns (low reconstruction loss on real data, poor on random data, RandRatio approaching about 1e-4), and that generalization and stability behave well across days. In production A/B testing on large-scale Yahoo native traffic, the encoder-augmented CVR model improves revenue-related metrics without visible auction latency impact, leading to full deployment and claimed material annual revenue impact; the paper closes with extensions such as different projector networks, alternative self-supervised objectives, and potentially stronger model architectures while maintaining latency constraints.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2401.16432_ssl_cvr.pdf"
}