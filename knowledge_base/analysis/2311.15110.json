{
  "paper_id": "2311.15110",
  "title": "Relevance Feedback",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper studies how to reduce human review effort in high recall information retrieval tasks where missing relevant documents is worse than reviewing extra nonrelevant ones, such as patent search, due diligence, and legal or intelligence workflows. Instead of approaches that can automatically exclude documents and risk false negatives, it proposes a recall oriented loop that only re ranks results based on user feedback across iterations. The core method combines a text similarity retriever that produces a relevance ranking with relevance feedback that updates the next iteration query using the embeddings of accepted results, with the goal of reaching a fixed recall target using fewer review iterations.\n\nThe authors compare term based similarity using TF IDF with Solr MoreLikeThis against context based similarity using Sentence BERT dense vector search with HNSW approximate nearest neighbor search. They also test granularity choices by retrieving paragraphs rather than whole documents, then deriving a document ranking either from a documents highest ranked paragraph or from counting how many of its paragraphs appear in the ranking. Feedback strategies include no feedback, keyword expansion using high IDF terms, Rocchio style weighted averaging, and vector operations that average or sum embeddings, with cumulative variants that include the original query vector and non cumulative variants that do not; for paragraph retrieval, they also test amplifying positive feedback to sibling paragraphs from the same document.\n\nResults on a sampled subset of the Reuters RCV 1 v2 dataset show dense vector search with a highest paragraph based document ranking outperforms TF IDF configurations, including on an ambiguous topic set. For reaching 80 percent recall with 10 items reviewed per iteration, cumulatively summing vectors is the best feedback strategy and reduces iterations relative to no feedback by 17.85 percent on the test set and 59.04 percent on the ambiguous set; amplification mainly reduces variance rather than mean iterations. Vector based feedback adds little latency per iteration compared to no feedback, while keyword expansion is substantially slower, and the paper notes limitations from using only news data and 300 articles per topic, suggesting larger scale tests and faster sparse similarity methods as future work.\n\n*words that occur in the same contexts tend to have similar meanings*  \n\n*reduces review effort between 17.85% and 59.04%, given a target recall level of 80%*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2311.15110_relevance_feedback.pdf"
}