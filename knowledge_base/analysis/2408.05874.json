{
  "paper_id": "2408.05874",
  "title": "Llm Product Classification",
  "category": "product_matching",
  "year": 2024,
  "timestamp": "2026-03-01T14:51:01.157797",
  "summary": "## Summary\n\nThis paper studies product classification for international trade and e-commerce compliance, where wrong labels can cause incorrect duties and tax liabilities, penalties, and operational disruption. The authors argue that real product descriptions at inference time are often incomplete or heavily abbreviated, so evaluations on clean text overstate real-world performance. *We introduce a framework designed to simulate real-life data attacks on clean data.*\n\nThe core method is a data perturbation framework that generates realistic, label-preserving description degradations, using two main attack types: amputation (randomly removing tokens) and abbreviation (replacing some words with abbreviated forms), plus a combined attack that applies both. Perturbations are generated with GPT-4 (0613) via controlled prompts intended to keep meaning mostly intact and still classifiable by humans, and robustness is measured as the performance drop between clean and perturbed datasets. Experiments use two public datasets with hierarchical taxonomies: Icecat with 370 leaf classes and large train and test splits (with a 5,000-example stratified test sample for LLM cost control), and WDC-222 with 222 leaf classes used as a harder, heterogeneous gold-standard test set.\n\nEvaluation compares supervised baselines (DeBERTaV3-base fine-tuned in flat and two-stage hierarchical setups) against LLM classifiers (Llama-2 70B chat, GPT-3.5 0613, GPT-4 0613) across flat, hierarchical, and few-shot prompting configurations, including an explicit combined-reason instruction that alerts the model to possible abbreviation or missing content. Results show that LLMs, especially in few-shot settings, are more robust under the introduced attacks than the supervised baseline, which suffers large drops under abbreviation and combined perturbations, while GPT-4 few-shot delivers the strongest overall performance and robustness on both datasets. *Our findings show that LLM-based approaches are generally more robust against adversarial attacks.*\n\nThe paper also includes a human annotation study to validate perturbation realism and recoverability: annotators could expand abbreviated tokens correctly about 80 percent of the time on Icecat and 85 percent on WDC-222, and few-shot context substantially improved mapping attacked descriptions back to clean counterparts. The discussion notes potential dataset leakage cannot be ruled out for frontier LLMs but argues the observed gains, plus improved performance when adding few-shot examples and combined-reason prompting, suggest benefits beyond memorization. Limitations include modeling only a subset of real enterprise perturbations (notably excluding common typos), instability for some open models with small prompt variations, and the practical constraint of single-run LLM evaluation due to inference cost; the paper closes with privacy and governance practices and positions predictions as user-facing suggestions in deployment.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}