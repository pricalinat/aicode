{
  "paper_id": "2307.16789",
  "title": "Paper",
  "category": "mini_program_service",
  "year": 2023,
  "timestamp": "2026-03-01T13:49:36.278574",
  "summary": "# ToolLLM: Enabling Large Language Models to Master 16,000+ Real-World APIs\n\n## Overview\n\nThis paper introduces ToolLLM, a comprehensive framework for teaching open-source large language models to effectively use external tools (APIs). The researchers address a critical gap: while closed-source models like ChatGPT and GPT-4 demonstrate impressive tool-use capabilities, open-source models like LLaMA significantly lag behind due to the lack of suitable instruction-tuning data in this domain. ToolLLM encompasses three key components: a large-scale instruction-tuning dataset called ToolBench, a fine-tuned model called ToolLLaMA, and an automatic evaluator called ToolEval.\n\n## Dataset Construction: ToolBench\n\nThe researchers collected 16,464 REST APIs spanning 49 categories from RapidAPI Hub, filtering down from an initial 53,190 APIs to ensure quality and reliability. The dataset construction involves three stages: API collection, instruction generation, and solution path annotation. For instruction generation, they used ChatGPT to create diverse instructions covering both single-tool scenarios and complex multi-tool scenarios where multiple APIs must be interleaved. They developed a novel Depth-First Search-based Decision Tree (DFSDT) algorithm to annotate valid solution paths, which significantly outperforms traditional ReACT reasoning by allowing models to explore multiple reasoning paths and recover from errors. The final dataset contains 126,486 instruction-solution pairs with over 469,585 real API calls.\n\n## Key Innovations\n\nThe DFSDT reasoning strategy represents a major advancement over prior methods like CoT and ReACT. Unlike ReACT's linear exploration, DFSDT constructs a decision tree that enables LLMs to evaluate different reasoning traces, abandon failed paths, and explore alternatives. Experiments show DFSDT significantly improves pass rates across all instruction types, with particularly notable gains for complex multi-tool instructions. The researchers also developed ToolEval, an automatic evaluator using ChatGPT that achieves 87.1% agreement with human evaluators on pass rate and 80.3% on win rate, providing a scalable and reliable assessment method.\n\n## Experimental Results\n\nToolLLaMA, fine-tuned from LLaMA-2 7B on ToolBench, demonstrates remarkable capabilities. It outperforms Text-Davinci-003 and Claude-2, achieves performance nearly on par with ChatGPT (the teacher model), and is only slightly inferior to GPT-4. The model exhibits strong generalization to unseen APIs, requiring only API documentation to adapt to new tools. When combined with the neural API retriever trained on the dataset, ToolLLaMA achieves even better performance by finding more appropriate APIs than the original ground truth set. In out-of-distribution experiments on APIBench, ToolLLaMA outperforms Gorilla (a model specifically designed for that benchmark) on HuggingFace and TorchHub domains, despite having never trained on those APIs.\n\n## Significance\n\nThis work represents a substantial step toward democratizing AI tool-use capabilities. By demonstrating that open-source models can achieve comparable performance to closed-source alternatives through proper data construction and training, the researchers enable broader community innovation. The framework's ability to generalize to unseen APIs is particularly valuable, as it allows users to incorporate custom APIs seamlessly. The authors have open-sourced the codes, trained models, and demo, facilitating future research in instruction tuning and tool learning for LLMs. The DFSDT approach also provides a general strategy for enhancing reasoning capabilities in decision-making problems beyond tool use.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}