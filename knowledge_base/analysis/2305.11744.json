{
  "paper_id": "2305.11744",
  "title": "Refit",
  "category": "product_matching",
  "status": "success",
  "summary": "## Summary\n\nThis paper introduces ReFIT, a test-time method that uses a cross-encoder reranker to give relevance feedback to a dual-encoder retriever, with the explicit goal of improving recall in retrieve-and-rerank pipelines. The key idea is that rerankers usually only reorder the top K retrieved candidates and cannot improve Recall@K, so ReFIT distills the reranker’s score distribution into an updated query embedding during inference, then runs a second retrieval using that improved query vector. The approach is designed to be lightweight and plug-in: it updates only the query representation, not any model parameters, making it architecture-agnostic and applicable across domains, languages, and modalities.\n\nMechanically, ReFIT starts with standard retrieval of top K passages and reranking them, then forms two probability distributions over those K candidates: one from reranker scores and one from retriever dot-product scores. It minimizes a KL-divergence distillation loss between these distributions via gradient descent on the query vector for n update steps, and then retrieves again using the updated query embedding; the paper reports practical choices like min-max normalization of scores, a temperature for the reranker softmax, and tuned hyperparameters such as T=2, n=100, and learning rate 0.005. For latency fairness, it compares against a baseline that reranks more candidates, arguing that ReFIT’s added distillation plus second retrieval can be comparable to simply reranking a larger pool while yielding larger recall gains.\n\nAcross experiments, ReFIT improves Recall@100 on English BEIR (multiple domains) for different underlying retrievers, and it shows consistent gains in multilingual retrieval on Mr. TyDi and cross-lingual retrieval on MKQA (26 languages). It also extends beyond text by applying the same inference-time distillation idea to text-to-video retrieval on MSRVTT using BLIP, improving Recall@100 without noticeably harming short-list quality, and it outperforms a related test-time query optimization method TouR while avoiding TouR’s repeated rerank-retrieve iterations. Additional analysis visualizes how updated query vectors move closer to positive passages, reports that improvements often leverage highly scored reranker top results but can also arise from informative negatives, and shows benefits from multiple rounds of feedback with diminishing returns; it further demonstrates compatibility with multi-vector retrieval (ColBERTv2) and reports that ranking quality can remain comparable to reranking baselines.\n\n*We propose ReFIT, an inference-time mechanism to improve the recall of retrieval in IR using relevance feedback from a reranker.*\n\n*The proposed distillation step is fast, considerably increasing recall without any loss in ranking performance over a standard R&R pipeline with comparable latency.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2305.11744_refit.pdf"
}