{
  "paper_id": "2106.12657",
  "title": "Extreme Multilabel",
  "category": "product_matching",
  "year": 2021,
  "timestamp": "2026-03-01T13:45:07.435530",
  "summary": "# Extreme Multi-label Learning for Semantic Matching in Product Search\n\n## Summary\n\nThis paper from KDD 2021 presents a novel approach to semantic product search by formulating it as an eXtreme Multi-label Classification (XMC) problem. The authors address the challenge of retrieving semantically related products from a catalog of 100 million items while maintaining low latency for real-time inference. Traditional lexical matching methods like Okapi-BM25 can leverage inverted indices for fast retrieval but fail to capture semantic and behavioral signals, while embedding-based neural models like DSSM learn semantic representations but are limited by shallow encoders due to latency constraints. The proposed XR-Linear (PECOS) model combines the best of both worlds by using tree-based partitioning with hierarchical clustering to achieve logarithmic inference time complexity in the number of products.\n\nThe core innovation lies in building a hierarchical label tree where each node contains a linear one-versus-rest classifier, enabling efficient beam search to retrieve top-k relevant products. The model uses n-gram TF-IDF features including word unigrams, bigrams, and character trigrams, where character trigrams prove particularly effective for handling typos and compound words common in product searches. Label representations are constructed using PIFA (Query Aggregated Product Features) when product information is noisy, or direct featurization when rich textual data is available. Weight pruning is applied to reduce model size while maintaining performance, with experiments showing robust behavior across different pruning thresholds.\n\n## Key Results\n\nQuantitative experiments demonstrate significant improvements over competing methods. With a beam size of 15, XR-Linear achieves Recall@100 of 60.9% at 1.25 ms/q latency, compared to DSSM's 36.2% at 3.13 ms/q, representing a 65% relative improvement. Increasing beam size to 50 yields Recall@100 of 65.7% at 3.48 ms/q latency, still well within typical real-time constraints. The model proves robust to weight pruning, achieving a 3x model size reduction (from 295GB to 90GB) with minimal performance degradation (Recall@100 dropping from 60.3% to 57.6%). Training costs are dramatically lower at approximately $567 USD compared to $6,365 USD for DSSM, representing a 10x reduction.\n\nThe authors also conducted online A/B tests comparing traditional lexical matching augmented with DSSM versus XR-Linear. The treatment group using XR-Linear showed statistically significant improvements across multiple key performance indicators, validating the offline findings. A qualitative side-by-side comparison for the query \"rose of jericho plant\" shows XR-Linear retrieves more purchased products and provides greater diversity in the match set compared to both DSSM and Okapi-BM25.\n\n## Technical Architecture\n\nThe XR-Linear model architecture consists of D=5 layers with a branching factor B=32, resulting in a maximum of 100 labels per leaf cluster. Each layer contains K_t independent binary classifiers trained using LIBLINEAR with negative sampling for efficiency. The inference uses beam search with width b to traverse the tree top-down, computing relevance scores as the aggregation of ancestor node scores. The complexity is O(b × T_f × log L) where T_f is the score computation time and L is the total number of products, making it logarithmic in the output space.\n\nThe feature engineering combines word-level unigrams (1 million features), word bigrams (3 million features), and character trigrams (200K features) for a total vocabulary of 4.2 million dimensions. Character trigrams specifically help capture model numbers, brand names, and handle spelling errors common in e-commerce queries. The training dataset comprises over 1 billion positive query-product pairs from 12 months of search logs, with approximately 12% cold-start products in the test set not seen during training.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}