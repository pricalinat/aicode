{
  "paper_id": "2010.14013",
  "title": "User Cold Start",
  "category": "ecommerce_evaluation",
  "year": 2020,
  "timestamp": "2026-03-01T14:04:28.784036",
  "summary": "This paper studies pure user cold-start recommendation, where a new user has no interactions, no profile or side information, and is not asked to answer onboarding questions. Using latent factor embeddings from a trained recommender on warm users, it reframes cold-start as an item selection task: choose a small set of M items that is likely to include at least one future users favorite item, under the assumption that cold-start and warm users are drawn from the same distribution. The objective is defined via a fav loss that measures, for each warm user, the gap between that users best possible item in the full catalog and the best item available inside the selected subset, making the problem a multi-query extension of Maximum Inner Product Search.\n\n*we consider a pure cold-start scenario where neither interaction nor side information is available and no user effort is required.*\n\nThe paper proposes six solution strategies, ranging from heuristics to principled optimization. Max-Norm picks items with the largest embedding norms, motivated by norm bias in inner product retrieval and interpreted as selecting popular items; Max-In-Degree builds an inner product proximity graph over items and selects high in-degree nodes as good substitutes for many items, using an approximate graph method to avoid O(N^2) construction; User-Expectation averages warm user vectors into a single mean query and recommends top inner product items to that mean; IPGS estimates which items are most often top-1 favorites for warm users by querying the same proximity graph and counting nominations; Submodular Greedy directly optimizes a reformulated monotone submodular objective with a (1 - 1/e) approximation guarantee but high runtime; and a Convex Hull approach is discussed theoretically, including a result that an optimal set lies within the convex hull of item embeddings, but it is found impractical in high dimensions due to large hull size and computational cost.\n\nExperiments use MovieLens-20M, Epinions, and Amazons All Beauty, splitting users into warm and cold-start with a 4:1 ratio, learning item and warm-user vectors via probabilistic matrix factorization, and deriving cold-start user vectors by factorizing their ratings with item vectors fixed. On the core fav loss metric, IPGS and Submodular are generally best on both warm and cold-start users, while User-Expectation often performs poorly, suggesting warm user vectors are multi-modal and the mean loses critical structure; Max-In-Degree is close to IPGS despite not using user vectors. The paper also evaluates Precision, MAP, and NDCG on cold-start users and finds rankings can differ from fav loss; for example, on MovieLens-20M IPGS is strongest when M is small, while User-Expectation improves as M grows. A detailed analysis links Max-Norm success to stronger norm bias in All Beauty, supported by norm distribution statistics and the disproportionate presence of top-norm items in users top-10 favorites.\n\n*it is desirable to use the IPGS method because it achieves a very low loss value and also has a relatively low computational complexity.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}