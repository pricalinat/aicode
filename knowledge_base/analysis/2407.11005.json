{
  "paper_id": "2407.11005",
  "title": "Paper",
  "category": "mini_program_service",
  "year": 2024,
  "timestamp": "2026-03-01T13:50:54.058762",
  "summary": "# RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems\n\n## Summary\n\nRAGBench is a comprehensive benchmark dataset for evaluating Retrieval-Augmented Generation (RAG) systems, comprising 100,000 examples across five industry-specific domains: biomedical research, general knowledge, legal contracts, customer support, and finance. The benchmark addresses a critical gap in RAG evaluation by providing standardized evaluation criteria and annotated datasets sourced from real-world industry corpora such as user manuals, making it particularly relevant for practical applications.\n\nThe paper introduces the TRACe evaluation framework, which measures four key metrics: **Utilization** (fraction of retrieved context used by the generator), **Relevance** (fraction of context relevant to the query), **Adherence** (whether the response is grounded in the context, synonymous with hallucination detection), and **Completeness** (how well the response incorporates all relevant information). These metrics provide granular, actionable insights for RAG practitioners to identify specific weaknesses in their systems.\n\nThe authors demonstrate that automated labeling using GPT-4 achieves 93-95% alignment with human judgments on the DelucionQA benchmark. Through extensive experimentation, they find that a 400M-parameter DeBERTa-v3-Large model fine-tuned on RAGBench outperforms billion-parameter LLM judges and commercial RAG evaluation systems like RAGAS and TruLens across numerous domains and task types.\n\n## Key Technical Details\n\nThe benchmark was constructed by transforming 12 component datasets—including PubMedQA, CovidQA, HotpotQA, MS Marco, CUAD, DelucionQA, EManual, TechQA, FinQA, TAT-QA, HAGRID, and ExpertQA—into a standardized RAG format. Context token length ranges from 100 to 11,000 tokens, and responses were generated using GPT-3.5 and Claude 3 Haiku to introduce variability. The dataset totals 100k samples split across train, validation, and test sets with no query overlap across splits from the same data source.\n\nThe TRACe framework formalizes each metric mathematically. For example, context relevance is calculated as the ratio of relevant tokens to total document tokens, while adherence is a boolean indicating whether all parts of the response are grounded in the context. The authors note that predicting context relevance is more difficult than utilization because it requires determining whether specific information necessary to answer the question exists in the context—a task that inherently involves deriving the correct answer first.\n\n## Experimental Results\n\nBenchmark evaluation on test splits shows the fine-tuned DeBERTa model achieves hallucination detection AUROC scores ranging from 0.64 to 0.86 across domains, with RMSE for relevance and utilization ranging from 0.04 to 0.26. The model demonstrates statistically significant improvements over zero-shot GPT-3.5-judge, RAGAS, and TruLens on most datasets. The case study reveals that prompting LLMs with chain-of-thought leads to higher utilization and completeness, while GPT-4o combined with CoT prompts yields the lowest hallucination rates compared to GPT-3.5.\n\n## Dataset Availability\n\nThe labeled dataset is released at https://huggingface.co/datasets/rungalileo/ragbench, and inference code is available on GitHub. The benchmark enables development of more mature evaluation systems that can assess different components of RAG pipelines along multiple dimensions, facilitating iterative improvement of production applications.\n\n---\n\nThe work addresses the critical need for standardized benchmarks in RAG evaluation, enabling more precise and actionable insights into system performance. The TRACe framework's explainable metrics allow practitioners to diagnose specific weaknesses—whether in the retriever, generator, or overall system design—rather than receiving only a single quality score.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}