{
  "paper_id": "2505.15196",
  "title": "Ecomscriptbench",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T13:51:58.595422",
  "summary": "# ECOMSCRIPTBENCH: A Multi-task Benchmark for E-commerce Script Planning\n\n## Overview\n\nThis paper introduces **ECOMSCRIPTBENCH**, the first large-scale benchmark for evaluating large language models on e-commerce script planning—the ability to generate coherent step-by-step shopping plans while recommending relevant products at each step. The benchmark contains 605,229 product-enriched scripts derived from 2.4 million Amazon products, with human annotations providing gold labels for evaluation.\n\n## Core Contributions\n\nThe authors formally define **E-commerce Script Planning (ECOMSCRIPT)** as a three-step discriminative process: (1) **Script Verification** — determining whether a script is plausible and feasible for a given user objective; (2) **Step-Product Discrimination** — determining whether each step requires a product purchase and whether a specific product can help; and (3) **Script-Products Verification** — evaluating whether all products in a script can work together cohesively. This formulation enables automated e-commerce planning by filtering and verifying generated scripts.\n\nA key innovation is the **step-intention alignment strategy**, which addresses the semantic gap between how users describe planned actions and how products are indexed in e-commerce databases. Rather than using traditional keyword search, the framework leverages *purchase intentions*—customer motivations for buying products—as a semantic bridge to match products with actionable script steps. This approach generated 24 million purchase intentions from 2.4 million products and associated up to three products per step using Sentence-BERT similarity scoring.\n\n## Benchmark Construction\n\nThe dataset was constructed using GPT-4o-mini to generate user objectives from Amazon purchase reviews, then create goal-oriented scripts for each objective. Purchase intentions were distilled from product metadata using LLMs following prior work (FolkScope). Human annotations via Amazon Mechanical Turk provided gold labels for 5,000 sampled entries per subtask, achieving 78% inter-annotator agreement and 96.33% alignment with expert verification.\n\n## Experimental Findings\n\nExtensive evaluation across 20+ models—including RoBERTa, DeBERTa, LLaMA-3.1, GPT-4o, and others—revealed that *current LLMs face significant challenges with ECOMSCRIPT tasks*, even after fine-tuning. The best open-source model (LLaMA-3.1-405B) achieved only 75%, 68%, and 65% accuracy on the three subtasks respectively. Product-related tasks proved substantially harder than script verification, highlighting the complexity of e-commerce product understanding.\n\nNotably, experiments demonstrated that *injecting purchase intention knowledge significantly enhances performance*. Models sequentially fine-tuned on FolkScope and MIND intention datasets, then on ECOMSCRIPTBENCH, showed substantial improvements over those trained only on ECOMSCRIPTBENCH alone. This indicates that aligning LLMs with e-commerce purchase motivations helps them better identify useful products for desired actions.\n\nError analysis of GPT-4o identified three main failure categories: (1) wrong understanding of product features (68% of errors); (2) conflict in reasoning across script steps (27%); and (3) internal annotation conflicts (5%). Category-wise analysis showed performance varies significantly—Toys and Games and Grocery categories exceeded 80% accuracy, while Beauty and Personal Care dropped to 64%, indicating text-only differentiation remains challenging for ambiguous product domains.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}