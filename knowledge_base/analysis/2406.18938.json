{
  "paper_id": "2406.18938",
  "title": "Federated Multiscenario",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper tackles federated multi-scenario multi-task recommendation, where each business scenario is treated as a separate federated client and the model must predict multiple targets per scenario (for example CTR and CTCVR in e-commerce). It argues that simply combining multi-scenario multi-task learning with federated learning creates overlapping optimization conflicts: data-level conflict from differing scenario distributions, model-level conflict from parameter averaging under non-iid data, and task-level conflict where tasks compete and performance can become imbalanced. The proposed solution, PF-MSMTrec, is positioned as an initial attempt to address this specific combined setting while keeping each clients data local.\n\nPF-MSMTrec builds a bottom-up joint learning framework around an MMoE-style architecture on each client, then carefully chooses what is shared and how it is aggregated. Key components are:\n- **Expert parameter decoupling via a parameter template** that separates expert parameters into scenario-specific shared parameters (aggregated) and local personalized parameters (kept client-side), with task and scenario embeddings used to generate parts of the expert parameters.\n- **Federated batch normalization on the server** applied to uploaded shared expert parameters to reduce cross-scenario distribution mismatch during aggregation, with global normalization statistics computed across clients and experts.\n- **Conflict coordination for aggregation** that approximates gradient information using parameter differences between communication rounds, then applies a constraint-based adjustment inspired by conflict-averse optimization to reduce destructive updates when clients disagree.\n- **Personalized aggregation** that introduces learnable weights to scale the coordinated update for each expert (and separately for tower networks), aiming to balance global collaboration with local specialization.\nTraining uses binary cross-entropy across tasks plus a regularization term that penalizes divergence between each clients shared parameters and the aggregated shared parameters.\n\nExperiments use AUC on two public datasets: AliExpress (four country scenarios: NL, ES, FR, US; tasks CTR and CTCVR) and Tenrec (two platform scenarios, with click and like style tasks). The results tables report that PF-MSMTrec in federated mode generally exceeds both strong centralized multi-scenario multi-task baselines (such as MMoE, PLE, ESMM, STAR, AESM2, PEPNet) and federated baselines (FedAvg, FedProx, Ditto, FedAMP), with multiple statistically significant gains reported. Ablations attribute most of the benefit to expert parameter decoupling and the conflict coordination mechanism, while tower aggregation choices matter less; sensitivity studies suggest four experts per client works best, and convergence curves show behavior comparable to FedAvg while reaching better final performance. An appendix includes a derivation about the federated batch normalization aggregation outcome under a specific communication timing assumption.\n\n*We propose PF-MSMTrec, a novel framework for personalized federated multi-scenario multi-task recommendation.*\n\n*Extensive experiments on two public datasets demonstrate that our proposed method outperforms state-of-the-art approaches.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2406.18938_federated_multiscenario.pdf"
}