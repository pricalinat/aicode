{
  "paper_id": "2302.03525",
  "title": "Deep Recsys Survey",
  "category": "product_matching",
  "status": "success",
  "summary": "This survey reviews multi-task deep recommender systems MTDRS, motivated by real platforms needing multiple predictions at once, such as click, like, and conversion, while staying efficient. It formalizes MTDRS as learning shared parameters plus task specific parameters over K labels per user item record, typically optimized with a weighted sum of per task losses such as binary cross entropy, and contrasts multi-task recommendation with related directions like multi-objective recommendation, multi-scenario recommendation, and multi-behavior recommendation. It emphasizes two core benefits over training separate models: cross task knowledge transfer that can improve multiple tasks, and savings in computation and storage, while highlighting key practical difficulties in modeling task relevance, handling sparsity in downstream signals like conversion, and capturing sequential dependence across user actions. *Multi-task learning MTL aims at learning related tasks in a unified model to achieve mutual improvement among tasks considering their shared knowledge.*\n\nThe paper proposes a taxonomy along two axes, task relation and methodology, and describes trends across recent work. For task relations, it groups MTDRS into parallel tasks with no sequential dependency, cascaded tasks where later tasks depend on earlier ones such as impression → click → conversion and variants, and auxiliary with main where side tasks are designed to help a primary task. For methodology, it organizes approaches into parameter sharing, optimization, and training mechanisms: hard sharing via shared bottom and task towers, sparse sharing via masks and subnetworks, soft sharing via task specific models fused by relevance weights, and expert sharing via mixture of experts and gating modules such as MMoE and extensions like PLE that separate shared and task specific experts to reduce conflicts. It also discusses optimization issues like negative transfer driven by gradient magnitude imbalance and parameter conflict, plus emerging interest in multi-objective trade-offs inside MTDRS, and it categorizes training into joint training, reinforcement learning based training for long term objectives, and auxiliary task learning including manually designed auxiliaries and newer self auxiliary ideas. *the task relation is categorized into parallel, cascaded, and auxiliary with main, while the methodology is grouped into parameter sharing, optimization, and training mechanism.*\n\nOn practice and evaluation, it summarizes application areas beyond e-commerce, including online advertising and social media systems validated with online experiments, and notes that multi-task fusion is often applied after per task prediction to produce a final ranking score, with solutions ranging from grid search and Bayesian optimization to reinforcement learning. It lists commonly used public datasets and the recommendation stage they support, including Ali-CCP, Criteo, AliExpress, MovieLens, Yelp, Amazon, Kuairand, and Tenrec, spanning tasks like CTR, CVR, CTCVR, watch, rating, and multi-action engagement labels. Finally, it outlines open challenges and future directions: better understanding of what, where, and when to transfer to avoid negative transfer, unified modeling across both multi-task and multi-scenario settings, leveraging large pretrained models for multi-task recommendation without inefficient per task prompt tuning, broader AutoML that goes beyond routing, stronger explainability for complex task relevance, and handling task specific biases that differ across behaviors and tasks.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2302.03525_deep_recsys_survey.pdf"
}