{
  "paper_id": "2305.07633",
  "title": "Zeroshot Kg Recommendation",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper defines and targets Zero-shot Item-based Recommendation, where some items have no user interaction history during training, making standard collaborative filtering ineffective. It argues that text-based item embeddings from pre-trained language models capture generic semantics but miss recommendation-critical item-item relationships, and proposes using a Product Knowledge Graph to refine those embeddings while keeping inductive ability for unseen items.\n\nThe proposed framework, MPKG, builds a multi-relation PKG from item textual features (extracted with PLMs such as BERT variants) plus item-item relations like alsoBought, alsoViewed, and boughtTogether. It encodes each relation-specific graph with a simplified GCN-style encoder (SGC) and uses Task-oriented Adaptation layers to fuse relation embeddings differently per task, addressing three stated issues: multiple relation types, semantic divergence between text features and relations, and domain discrepancy between PKG pre-training and downstream recommendation. Pre-training is multi-task with four objectives: Knowledge Reconstruction for per-relation link prediction, High-order Neighbor Reconstruction over K-hop neighborhoods, Feature Reconstruction to align graph embeddings with PLM features via a decoder, and Meta Relation Adaptation that learns to reconstruct one relation using other relations through learned weighting, then fine-tunes mainly the MRA adaptation for the downstream recommendation loss.\n\nExperiments use an 18-market Amazon cross-market setup (Home and Kitchen), with a PKG totaling about 97.6K items and 3.2M edges, and evaluate both knowledge prediction and ZSIR via Recall@N and MRR under all-item and zero-shot-only candidate settings. MPKG consistently outperforms triplet-based KG embedding baselines and heterogeneous GNN baselines, shows especially strong gains on low-resource markets, and remains best on zero-shot knowledge prediction, supporting its inductive inference claims. Ablations indicate every pre-training task contributes, with the biggest drop when removing Knowledge Reconstruction, and sensitivity studies suggest moderate neighborhood order (K equal to 2) works best while higher order can add noise.\n\n*Existing recommender systems face difficulties with zero-shot items, i.e. items that have no historical interactions with users during the training stage.*\n\n*We propose a novel PKG pre-training and fine-tuning framework to tackle the ZSIR problem, which enhances the MPKG with inductive ability.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2305.07633_zeroshot_kg_recommendation.pdf"
}