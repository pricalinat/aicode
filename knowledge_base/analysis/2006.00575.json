{
  "paper_id": "2006.00575",
  "title": "Neural Entity Linking Survey",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper surveys neural entity linking research from the deep learning era, focusing on models published from 2015 onward and aiming to systematize design choices and performance relative to classic feature-based systems. It distills a generic neural EL pipeline, groups major architectural variations into recurring themes, and also reviews entity and context representation methods that many EL systems depend on. *This survey presents a comprehensive description of recent neural entity linking (EL) systems developed since 2015.*\n\nThe survey frames EL as two core subtasks, mention detection and entity disambiguation, and then decomposes disambiguation into candidate generation plus entity ranking, with an additional challenge of NIL or unlinkable mention prediction. It organizes candidate generation into three common strategies: surface-form matching, alias expansion via KB resources, and precomputed mentionâ€“entity priors, and it summarizes how mention and entity encoders evolved from CNN and RNN variants toward self-attention and Transformer-based encoders that reuse pretrained language models. For entity representations, it contrasts embeddings learned from unstructured text co-occurrence, graph-structure embeddings built from KG relations such as DeepWalk and TransE style objectives, and neural encoders that derive entity vectors from descriptions, types, categories, or verbalized triples.\n\nOn evaluation, the paper reports results across widely used benchmarks (notably AIDA and several TAC KBP, news, and web corpora) and shows that neural systems substantially outperform older non-neural baselines, with global disambiguation approaches often beating purely local ones while also introducing harder inference and error-propagation tradeoffs in end-to-end settings. It also summarizes entity-relatedness evaluation and highlights that reported best scores can depend heavily on hyperparameter search and candidate resources, complicating direct comparisons. Beyond classic uses in information extraction, biomedical text mining, retrieval, question answering, and KG population, it emphasizes a newer trend of integrating EL into pretrained language models and training with EL-related objectives, citing examples such as KnowBERT, ERNIE, KEPLER, LUKE, and Entities as Experts. *We identify five promising directions of future work in entity linking listed below:* end-to-end models that reduce reliance on candidate generation resources, stronger zero-shot transfer for emerging entities and domains, more standardized benchmarks for zero-shot settings, broader integration of EL losses into neural models, and growing interest in multimodal EL.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2006.00575_neural_entity_linking_survey.pdf"
}