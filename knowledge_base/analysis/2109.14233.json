{
  "paper_id": "2109.14233",
  "title": "Next Basket Reality Check",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper re-evaluates reported progress in next basket recommendation, where the task is to predict a users next set of purchased items from their prior baskets. It argues that prior results are hard to compare fairly due to inconsistent datasets, weak or missing baselines, and non-standard metrics, and it introduces a repeat versus explore lens: real next baskets mix previously consumed repeat items and new explore items, and models may succeed mainly by leaning toward the easier repeat side.\n\nUsing three public grocery style datasets (TaFeng, Dunnhumby, Instacart) and three method families (frequency baselines, nearest neighbor methods, deep learning methods), the authors run a unified comparison with common splits repeated multiple times. A key contribution is a stronger baseline, GP-TopFreq, that fills a basket first with a users personal top frequent items and then fills remaining slots with global top frequent items, addressing an overlooked issue where P-TopFreq can leave empty slots when a user has too few historical items. Under conventional metrics like Recall, NDCG, and PHR, no method wins consistently across datasets and basket sizes, and several deep learning models that beat the commonly used G-TopFreq baseline are often beaten by the simple personalized frequency baselines, with DNNTSP standing out as the most consistently strong deep model in their tests.\n\nTo explain these mixed results, the paper proposes NBR specific diagnostics: basket composition metrics RepR and ExplR, plus performance separated into repetition and exploration (Recallrep, Recallexpl, PHRrep, PHRexpl). The analysis shows most models are heavily skewed toward either repetition or exploration compared to the ground truth balance, and that repeat item prediction is systematically easier than explore item prediction, creating a large difficulty gap and tradeoff. Performance gains for many recent methods largely come from repeat recommendations, even when models include modules intended to improve exploration, and improvements over GP-TopFreq are often modest relative to added complexity and tuning cost.\n\nThe paper also examines distributional effects that average metrics can hide. It reports that method rankings can flip for user groups with low versus high repetition ratios, that some models perform well on average while under-serving large user segments, and that a small fraction of high repetition users can contribute disproportionately to average performance. From the item side, exposure in recommended baskets is long-tailed for all methods, with some models giving zero exposure to many items, and deep learning methods tending to align exposure more with training label frequency while repeat-biased baselines align more with test input history. The authors conclude with evaluation guidance: use multiple datasets with different repeat explore mixes, include GP-TopFreq as a baseline, report repeat explore metrics and basket composition, and break results down by user groups to better assess fairness and real progress.\n\n*No NBR method consistently outperforms all other methods across different datasets.*\n\n*All published methods are heavily skewed towards either repetition or exploration compared to the ground-truth.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2109.14233_next_basket_reality_check.pdf"
}