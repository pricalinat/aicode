{
  "paper_id": "2208.06150",
  "title": "Paper",
  "category": "mini_program_service",
  "year": 2022,
  "timestamp": "2026-03-01T14:01:51.443229",
  "summary": "This paper argues that standard BERT style pretraining on general corpora like Wikipedia can underperform in e commerce search because the text is often free form, repetitive, and grammatically irregular, and because long tail queries lack enough labeled training data. It proposes domain specific pretraining tasks tailored to two core modules in an industrial search pipeline: user intent detection during query processing and semantic embedding retrieval during candidate retrieval. The authors target production constraints by using smaller BERT like encoders suitable for cost efficient CPU serving and report gains both offline and in an online A B test.\n\nFor user intent detection, the task is framed as extreme multi label classification over thousands of leaf categories in a product hierarchy. Pretraining is done sequentially with Random Substring Classification, which samples short substrings from item titles as synthetic queries and predicts the item category, plus Masked Language Modeling, while omitting next sentence prediction as mismatched to the setting; fine tuning uses query click logs and applies a softmax temperature of 1 divided by 3 to widen margins. For embedding retrieval, the paper uses a two tower architecture trained with a triplet style objective and in batch negatives, and introduces Random Substring Retrieval, which again uses title substrings as synthetic queries to retrieve the originating item, optionally masking but finding little effect.\n\nExperiments use click log derived datasets including 566,161 pretraining examples, 180,008 intent fine tuning queries, 667,665 retrieval fine tuning examples, and two 10,000 query eval sets including a dedicated long tail set. In intent detection, the best 12 layer RSC plus MLM model improves over training without pretraining and over a Chinese BERT model fine tuned directly on the domain, with especially strong long tail gains; results also show RSC drives most of the benefit while MLM adds relatively little on top. In retrieval, customized pretraining improves precision and recall at k, while MLM again provides limited extra benefit beyond substring based pretraining, aligning with prior findings; an online test over 30 days on 20 percent traffic reports relative lifts in UCTR, UCVR, and GMV when swapping in the proposed models over internal baselines, and the paper releases a reproducibility dataset subset.\n\n*we propose customized and novel pre-training tasks for two critical modules: user intent detection and semantic embedding retrieval.*\n\n*our customized pre-trained models significantly improve no pre-trained models and outperform the official pre-trained BERT models.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}