{
  "paper_id": "1801.07030",
  "title": "Offline Ab Recsys",
  "category": "ecommerce_evaluation",
  "status": "success",
  "summary": "This paper studies how to predict the online A/B test uplift of a new recommender policy using only historical logs from the current production policy, aiming to iterate faster and avoid costly, weeks-long online experiments. It frames this as off-policy evaluation for ranking policies over top K lists, where basic importance sampling is unbiased but typically unusable in recommender settings because the action space is enormous and the resulting importance weights have extreme variance. The authors focus on counterfactual, business-metric-oriented offline A/B testing, and argue that common offline rank metrics and standard counterfactual estimators do not provide reliable decision support for personalized recommendation.\n\nIt analyzes classic estimators and why they fail in this setting: BIS and doubly robust can still have very high variance with sparse, noisy rewards like clicks, while capped importance sampling reduces variance but introduces bias that is hard to control in practice. *Inpractice,nocappingparameterforCISyieldsconfidenceinterval smallenoughtodecidewhetherπt isabetterpolicythanπp.* To improve the bias variance trade-off, the paper proposes modeling the capping bias rather than bounding it pessimistically, introducing a progression of estimators that adjust for the mass lost to capping:\n- NCIS: a global normalization that rescales the capped estimate, consistent when reward and capping are effectively independent, but potentially biased when groups differ.\n- PieceNCIS: a piecewise version using stratification over context groups to make the bias correction more local and reduce confounding from heterogeneous user segments.\n- PointNCIS: a pointwise, context-conditional normalization that estimates the needed inverse propensity term per context via sampling techniques, aiming to further reduce bias while keeping variance manageable.\n\nThe experimental section benchmarks these estimators against ground-truth online uplift using a proprietary set of 39 production A/B tests from a large-scale commercial recommender system, with a click-based metric where rewards are sparse and high-variance; the study fixes a capping value (c = 100) and excludes uncapped IS-style baselines as too noisy for decisions. Results show that locality in bias modeling improves alignment with online outcomes: CIS has negative uplift correlation and many missed improvements, while PointNCIS performs best (Table 3 reports correlation improving from −0.15 for CIS to 0.49 for PointNCIS, precision from 0.28 to 0.56, and false negative rate from 0.64 to 0.16, with PointNCIS also yielding narrower intervals than CIS on average). *falsenegativesaremuchworsethanfalse positives.* The paper concludes that these bias-aware capped estimators make offline A/B testing more predictive and operationally useful for recommender iteration, and suggests future variance reductions by leveraging mixtures of many recently deployed policies to increase overlap with the target policy.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/ecommerce_evaluation/1801.07030_offline_ab_recsys.pdf"
}