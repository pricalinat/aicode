{
  "paper_id": "2502.15990",
  "title": "Llm Query Product Labeling",
  "category": "ecommerce_evaluation",
  "status": "success",
  "summary": "This paper proposes an automated pipeline for labeling query product relevance for e commerce search using large language models, aiming to replace or reduce reliance on slow, expensive, and noisy third party human annotation. It frames relevance labeling as a multi class classification problem over query product pairs and shows that prompt engineered LLM labeling can approach human level quality while scaling to far more queries and products, including cold start and tail items where click signals are sparse or misleading. *Accurate query-product relevance labeling is indispensable to generate ground truth dataset for search ranking in e-commerce.*\n\nThe approach centers on prompt engineering with in context learning, Chain of Thought style reasoning, and Retrieval Augmented Generation that retrieves similar labeled query product examples to include as few shot demonstrations, with Maximum Marginal Relevance used to diversify retrieved examples and reduce redundancy. Experiments span three datasets: ESCI with four labels (Exact, Substitute, Complement, Irrelevant), WANDS with three labels (Exact, Partial, Irrelevant), and a proprietary Walmart Mexico search dataset (mostly Spanish) with five labels (Excellent, Good, Okay, Bad, Embarrassing) and heavy class imbalance, evaluated using accuracy plus macro and weighted F1 as appropriate. The paper details a practical stack for scalability, including GPU inference for open source models, vLLM serving, quantization for larger models, and a vector database (ChromaDB) for retrieval.\n\nResults show that stronger models perform best (LLM2 and LLM5), and that adding few shot examples usually improves over zero shot, with RAG sometimes improving further; the most consistent gains come from combining RAG with MMR diversification, tested across λ_MMR values from 0.75 down to 0.0, where lower λ increases diversity and often improves metrics. The authors report that some LLM disagreements with dataset ground truth appear arguably more correct, suggesting a secondary use case of finding labeling errors in existing benchmarks. On operational efficiency, they estimate human labeling turnaround of roughly 2 to 3 weeks and about 30 minutes per record, versus about 8.3 seconds per record for their best proprietary model configuration and roughly 0.3 seconds per record for many open source runs on GPU, alongside an estimate that LLM labeling is about 500 times cheaper per pair when including compute and API costs. *Overall, we observe that diversity helps in improving accuracy.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/ecommerce_evaluation/2502.15990_llm_query_product_labeling.pdf"
}