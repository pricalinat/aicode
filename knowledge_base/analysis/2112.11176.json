{
  "paper_id": "2112.11176",
  "title": "Tods Performance Quality",
  "category": "mini_program_service",
  "status": "success",
  "summary": "This paper reviews how task-oriented dialogue systems, or TODS, are commonly evaluated and argues that optimizing for task completion alone often misses conversational qualities that strongly affect user experience. It frames a central tension between performance metrics that are easy to count and quality attributes that are harder to measure but can determine whether users feel satisfied or frustrated. *TODS typically have a primary design focus on completing the task at hand, so the metric of task-resolution should take priority.*\n\nThe review surveys conversational quality attributes used alongside task-resolution and explains why their meaning is highly context dependent. Key attributes discussed include:\n- **Task-resolution and goal completion:** often treated as the core success signal, but complicated by shifting user goals, differing definitions of success, and the difficulty of inferring completion without explicit confirmation.\n- **Usability and efficiency:** user satisfaction and related usability notions are important but subjective, sometimes inconsistent across evaluators, and can even contradict other metrics.\n- **User sentiment:** can be assessed per utterance or across a whole conversation trajectory, with ideas like sentiment swing; the paper cautions that current sentiment tools struggle with sarcasm, nuance, dataset limitations, and social conventions that can skew scores.\n- **Dialogue cost, retention rate, response time, and conversation length:** commonly referenced, but ambiguous in definition and not reliably correlated with better experiences; for example, faster responses are not always better, and shorter conversations can be harmful when the domain needs richer context.\n\nIt then reviews evaluation approaches and frameworks, grouping methods into human evaluation, user-modelling or simulation, and automatic evaluation metrics such as BLEU and ROUGE, while emphasizing that automated scores often correlate poorly with human judgments. It summarizes established frameworks like PARADISE, which models user satisfaction from task success and dialogue costs, and discusses decision-analytic approaches like Analytic Hierarchy Process, plus tools like ChatEval that combine automatic metrics with human comparative judgments but can be rigid and better suited to delayed feedback than in-the-moment optimization. The paper concludes that despite extensive TODS research, there is still no widely accepted standard for evaluating new systems, reproducibility remains difficult, open datasets are a bottleneck, and modelling TODS performance as a multivariate function of multiple conversational quality attributes remains an open problem. *there is still no standard in place for a novel TODS to be measured against.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2112.11176_TODS_Performance_Quality.pdf"
}