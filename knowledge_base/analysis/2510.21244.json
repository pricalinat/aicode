{
  "paper_id": "2510.21244",
  "title": "Voiceagenteval",
  "category": "mini_program_service",
  "year": 2025,
  "timestamp": "2026-03-01T13:49:42.308801",
  "summary": "# VoiceAgentEval: A Dual-Dimensional Benchmark for Expert-Level Intelligent Voice-Agent Evaluation\n\n## Overview\n\nThis paper presents VoiceAgentEval, a comprehensive evaluation framework designed to assess Large Language Models (LLMs) in expert-level outbound calling scenarios. The framework addresses three critical limitations in existing evaluation methods: insufficient dataset diversity and category coverage, unrealistic user simulation, and inaccurate evaluation metrics. VoiceAgentEval is developed by researchers from Meituan, Xbench, Agora, and several Chinese universities, and represents a significant advancement in benchmarking AI voice agents for professional business applications.\n\n## Framework Components\n\n### Benchmark Development\n\nThe evaluation framework encompasses six major business domains and 30 representative sub-scenarios, each with scenario-specific process decomposition, weighted scoring, and domain-adaptive metrics. The six domains include Customer Service and Support, Sales and Marketing, Human Resources Management, Finance and Risk Control, Research and Information Collection, and Proactive Care and Notifications. For each sub-scenario, five different user types are defined, resulting in a total of 150 evaluation instances that provide comprehensive coverage of real-world outbound calling situations.\n\n### User Simulator Design\n\nThe framework introduces a large model-driven User Simulator that generates diverse, persona-rich virtual users with realistic behaviors, emotional variability, and communication styles. The simulator uses a structured persona document divided into two parts: the Calling Agent profile (defining role, core task, target profile, SOP, FAQ, opening, and termination conditions) and the User Simulator profile (containing role, background, core concerns, emotional shifts, style exemplars, and termination conditions). This design enables systematic evaluation of an AI agent's task completion, adaptability, and communication skills when interacting with different user personalities.\n\n### Construction Pipeline\n\nThe user simulator construction follows a rigorous seven-step pipeline: seed data curation from real online conversations, initial persona extraction from high-quality samples, scenario generalization to protect business-sensitive information, data de-identification using strict substitution rules, detail enrichment to improve completeness, humanization enhancement through personality depth addition, and persona scaling using a behavioral pattern matrix. The five behavioral patterns (Cooperative, Analytical, Hesitant, Resistant, and Efficient) ensure diversity and coverage across user types.\n\n## Evaluation Methodology\n\n### Text Evaluation\n\nThe text evaluation employs a dual-layer assessment system comprising Task Flow Compliance (TFC) and General Interaction Capability (GIC). TFC evaluates model understanding and execution of domain-specific business processes through scenario-specific evaluation criteria, measuring Coverage Score (70% weight) and Process Score (30% weight). GIC assesses fundamental conversational competencies using eight dimensions: naturalness, coherence, hallucination handling, redundancy, emotional richness, intent understanding, noise resistance, and safety issues, with the first three receiving 20% weight each.\n\n### Voice Evaluation\n\nFor speech evaluation, the framework establishes 15 metrics across three major scenarios, integrating expert scoring with objective data to provide multi-dimensional quantification of speech recognition accuracy, system robustness, audio quality, and interactive experience. Key metrics include Selective Attention Success Rate (SASR), 500ms Interruption Success Rate (ISR), End-to-End Responding Latency, Word Error Rate (WER), and Mean Opinion Score (MOS).\n\n## Experimental Results\n\nThe comprehensive evaluation of 12 state-of-the-art LLMs under the proposed framework reveals distinct trade-offs between expert-level task completion and interaction fluency. Doubao-1.5-32k (ByteDance) ranked first with an overall score of 0.8881, demonstrating strong capabilities in both TFC (0.8331) and GIC (0.9554). GPT-4.1 placed second (0.8818), followed by Claude-4-Sonnet (0.8748), Gemini-2.5-Flash (0.8719), and Qwen3-32b-base (0.8672). The top eight models achieved overall scores above 0.84, indicating that LLMs have reached a substantial baseline capability in outbound dialogue scenarios.\n\n### Key Insights\n\nThe evaluation reveals several important findings: model parameter count alone does not determine performance, as smaller models like Gemini-2.5-Flash slightly outperformed its larger counterpart Gemini-2.5-Pro in GIC. The trade-off between TFC and GIC provides crucial insights for model selection in practical applicationsâ€”models with high TFC but relatively lower GIC are suitable for standardized tasks with strict processes and low error tolerance, while models with balanced performance excel in customer service scenarios requiring high emotional intelligence.\n\n## Conclusion\n\nVoiceAgentEval establishes a practical, extensible, and domain-oriented standard for benchmarking LLMs in professional outbound calling applications. The framework's comprehensive design enables thorough examination of dialogue generation capabilities and voice interaction quality in outbound calling scenarios, offering an objective basis for performance comparison and optimization among different models. The research demonstrates that current large language models have established a solid foundation for outbound scenarios, though performance variations reflect trade-offs and different emphases among technical approaches, indicating that future work may explore integrating the strengths of different models and developing more advanced alignment algorithms.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}