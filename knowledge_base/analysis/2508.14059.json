{
  "paper_id": "2508.14059",
  "title": "Gnn Recommendation",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper benchmarks four graph neural network architectures for product recommendation on the Amazon Product Co-purchase Network, framing recommendation as an inductive link prediction problem on a productâ€“product graph with rich item metadata. The core contribution is a practical comparison of LightGCN, GraphSAGE, GAT, and PinSAGE across accuracy, runtime, scalability constraints, and how well they handle cold-start style generalization to unseen products under a strict inductive split. Overall, GraphSAGE delivers the strongest validation performance, GAT is by far the fastest per epoch with high accuracy, PinSAGE is much slower to train despite solid results, and LightGCN underperforms in this feature-rich setting.\n\n*We adopted a strict inductive split where nodes are partitioned before edge filtering, ensuring that no test node is seen during training.*  \n*GraphSAGE achieved the highest overall validation performance, with an AUC of 0.9977 and AP of 0.9974.*\n\nMethodologically, the work builds an item-level dataset from SNAP Amazon co-purchase metadata and constructs a labeled edge set with negative sampling for supervised link prediction. Key setup details include:\n- **Data**: the paper cites 548,552 items and 1,788,725 undirected co-purchase links, plus supporting files `Item.csv`, `Category.csv`, and `Review.csv`; experiments restrict to connected components and generate random non-edge negatives.\n- **Feature pipeline**: heterogeneous node features are concatenated, including SBERT title embeddings (384d), one-hot group labels (10d), standardized numeric review and popularity metadata (7d, with log transforms for skewed fields), and dense category-path embeddings (200d), yielding a 601d feature vector per product; an optional PCA reduction for SBERT is described but not used in primary runs.\n- **Inductive evaluation**: nodes are split into disjoint train and test subsets (80:20) before filtering edges so evaluation uses entirely unseen nodes, to avoid transductive leakage that can inflate metrics.\n- **Models and training**: LightGCN is run via RecBole; GraphSAGE uses PyTorch Geometric mini-batch neighbor sampling with an MLP edge decoder; GAT uses a two-layer GATConv setup; PinSAGE uses precomputed random walks and importance-weighted neighbor sampling, adapted for inductive link prediction.\n\nResults emphasize trade-offs between accuracy and system cost. A summarized comparison reports validation AUC and per-epoch time as roughly: LightGCN 0.8355 and 569.17s per epoch (H200), GraphSAGE 0.9976 and 19.60s (L4), GAT 0.9729 and 2.29s (L4), and PinSAGE 0.9562 and 1881s (A100). The discussion highlights that transductive edge splits can cause misleadingly high scores via shared node identities or neighborhood overlap, motivating the strict inductive protocol; it also notes negative sampling ratio sensitivity, where very aggressive ratios can be infeasible or harm generalization. The conclusion argues GraphSAGE is the best overall choice for this setup, GAT is attractive for latency-sensitive settings, PinSAGE needs major efficiency improvements to be practical here, and LightGCN appears mismatched to rich node-feature link prediction without feature transformation or attention mechanisms; proposed future work includes interpretability analyses, hierarchy-aware category embeddings, and runtime improvements via alternative sampling, coarsening, or mixed-precision methods.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2508.14059_gnn_recommendation.pdf"
}