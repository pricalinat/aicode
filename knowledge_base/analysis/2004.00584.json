{
  "paper_id": "2004.00584",
  "title": "Deep Entity Matching",
  "category": "product_matching",
  "year": 2020,
  "timestamp": "2026-03-01T14:38:22.618174",
  "summary": "## Summary\n\nThis paper presents Ditto, an entity matching system that fine-tunes pre-trained Transformer language models by casting entity matching as a sequence-pair binary classification task. The core idea is to serialize each record as a token sequence with lightweight structural markers and feed a pair of serialized entries to a model like BERT, RoBERTa, or DistilBERT, yielding a match or no-match decision. Across standard benchmarks, this straightforward use of pre-trained language models already improves matching quality substantially over prior deep learning entity matching systems.\n\n*We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models.*  \n*On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5%.*\n\nBeyond the baseline, the paper introduces three optimizations that further boost accuracy and reduce labeling needs:  \n- **Domain knowledge injection** via input pre-processing, including span typing (tagging important token spans like product IDs, phone suffixes, street numbers, persons, dates) and span normalization (rewriting equivalent forms such as numeric formats or known synonyms to the same surface form).  \n- **Summarization for long fields** using a TF-IDF style token selection approach to preserve informative tokens under Transformer sequence length limits, which is especially important when key matching evidence is not near the beginning of text.  \n- **Training data augmentation** with span-, attribute-, and entry-level perturbations (delete or shuffle spans, delete or shuffle attributes, swap entry order) combined with MixDA, which interpolates language model representations of original and augmented examples to reduce label corruption while encouraging robustness to noise and missing or misplaced values.\n\nThe evaluation spans ER-Magellan benchmark datasets (including dirty variants), the WDC large-scale product matching benchmark, and a real-world employer matching case study. Ditto sets new reported state of the art across these benchmarks, with gains up to 31 F1 on a dirty Walmart-Amazon setting, strong robustness to noise, and improved label efficiency that often matches prior best results using half or less labeled data. In the employer matching deployment, the paper combines blocking strategies to generate about 10 million candidate pairs, fine-tunes Ditto with simple domain tags and attribute deletion augmentation, reaches about 96.53 F1 on held-out data, and further speeds the pipeline with an optional advanced blocking step using Sentence-BERT embeddings and similarity search for a reported end-to-end acceleration of about 3.8x.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}