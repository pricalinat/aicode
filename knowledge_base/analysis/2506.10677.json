{
  "paper_id": "2506.10677",
  "title": "Offpolicy Ab",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T14:01:00.840384",
  "summary": "This paper argues that standard AB testing can be made meaningfully more statistically efficient without changing the experiment design, by replacing the usual difference in means improvement estimator with a new family of unbiased off policy estimators that exploit similarity between the two tested decision systems. The key idea is that the classical estimator ignores the policies propensities, so it fails to capitalize on overlap between what the two systems would do, even in an AA test where the policies are identical and the true improvement is zero. *We introduce a family of unbiased off-policy estimators that achieves lower variance than the standard approach.* *The resulting estimator is simple, and offers substantial variance reduction when the two tested systems exhibit similarities.*\n\nThe setting models each user as an independent trajectory in an extended MDP that allows long term, potentially non Markovian dependencies within a user, while assuming no cross user interference so SUTVA holds. The improvement target is the value difference I of policy πA over πB, and the baseline estimator uses the difference between empirical average cumulative rewards from the two randomized populations. To reduce variance, the authors construct an estimator parameterized by a bounded weight transform f that combines an f regularized importance weighted term on data from πB with an explicit bias correction term computed from data collected under πA, yielding an unbiased estimator when f satisfies a simple boundary condition f(0) = -1 and achieving zero variance in the identical policy case when also enforcing f(1) = 0.\n\nTo choose f, the paper introduces a variance surrogate based on summed per time step second moments that upper bounds variance when within trajectory covariances are neglected, and then derives practical design rules and an optimum under that surrogate:\n- Sufficient surrogate reduction condition: for all x, constrain f(x) between -1 and min(2x - 1, 1), which guarantees improvement over the difference in means surrogate and includes a clipped choice h1(x) = min(x - 1, 1)\n- Surrogate optimal transform for sample ratio nr = nA/nB: f*(x) = (x - 1)/(nr x + 1), producing a simple closed form estimator that can be rewritten as importance scoring over the union of both datasets under an implicit mixture policy weighted by the traffic split\n- Edge cases where gains vanish: if the policies supports are disjoint the estimator provably collapses to the classical difference estimator, and for long horizon rewards even small policy differences can make trajectory overlap decay exponentially, again forcing a return to the baseline absent more advanced sequential weighting techniques\n\nExperiments support the theory in two simulated regimes: a bandit like sparse reward environment where variance reduction is large when policies are close and can be amplified by unbalanced traffic splits, and a recommendation style environment with a user boredom dynamic where naive IPS becomes unstable as horizon grows while the proposed estimators h1 and f* consistently reduce variance versus the difference in means and outperform an alternative unbiased estimator based on difference in Q values. The paper closes by emphasizing that the method is easy to deploy when propensities for both systems are available, can be combined with classical regression style variance reduction and doubly robust ideas, and is limited primarily by the same support and horizon issues that constrain importance weighting plus the requirement that SUTVA holds.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}