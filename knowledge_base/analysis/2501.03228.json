{
  "paper_id": "2501.03228",
  "title": "Lightgnn",
  "category": "product_matching",
  "status": "success",
  "summary": "LightGNN proposes a lightweight, distillation guided pruning framework for GNN based collaborative filtering that targets two practical pain points in real world recommender graphs: poor scalability from large interaction graphs and embedding tables, and degraded robustness from noisy implicit feedback. The core idea is to explicitly identify and remove redundant or noisy graph edges and embedding entries while preserving the collaboration modeling capacity of a stronger teacher GNN. The paper reports that the pruned model can keep recommendation quality near state of the art while substantially shrinking graph structure and parameters.\n\n*LightGNN achieves an 80% reduction in edge count and 90% reduction in embedding entries*  \n*explicitly identifying and eliminating redundancy and noise in GNNs to enable efficient and denoised recommendations*\n\nMethodologically, LightGNN learns edge importance via a sparse, trainable weight matrix that is injected into message passing, then iteratively prunes the least important candidate edges; it also prunes embedding dimensions by magnitude and can reduce propagation depth to cut computation and mitigate over smoothing. To retain accuracy under high compression, it introduces hierarchical knowledge distillation: prediction level distillation that encourages student ranking behavior to match the teacher, embedding level distillation using a contrastive objective to align representations, and an intermediate distillation stage that augments the student graph with high order connections (up to a hop limit) so aggressive edge pruning does not erase critical multi hop signals. A further importance distillation step transfers edge weight and relation cues from the intermediate model into the final pruning decision, and a uniformity regularizer encourages well spread embeddings while using similarly pruned masks to form meaningful positive sets.\n\nExperiments evaluate on three datasets (Gowalla, Yelp, Amazon) with implicit feedback conversion, filtered users and items with fewer than three interactions, and a 70/5/25 train validation test split, using full rank evaluation with Recall and NDCG at 20 and 40, compared against 18 baselines spanning MF, neural CF, GNN recommenders, self supervised methods, and prior compression approaches. Key findings include: (1) LightGNN outperforms the compared baselines on accuracy while remaining compact, (2) efficiency improves materially, including large storage reduction, over 90 percent fewer forward pass FLOPs, and notably lower prediction time under stronger pruning ratios, and (3) ablations show both learnable pruning and the hierarchical distillation components are necessary, with random dropping and removing intermediate or importance distillation causing clear performance loss, especially at high pruning. Additional analyses argue LightGNN reduces over smoothing via sparser propagation plus uniformity constraints, and qualitative examples indicate low learned weights often align with negative feedback like overly salty or overly spicy reviews and with redundant interactions among similar venue categories; the paper closes with ethical discussion focused on privacy, security, and transparency considerations when pruning models trained on user interaction data.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2501.03228_lightgnn.pdf"
}