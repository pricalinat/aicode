{
  "paper_id": "2412.14161",
  "title": "Paper",
  "category": "mini_program_service",
  "year": 2024,
  "timestamp": "2026-03-01T13:56:02.154458",
  "summary": "TheAgentCompany is a benchmark designed to evaluate LLM agents on consequential real-world tasks within a simulated software company environment. The benchmark features 175 tasks across multiple job categories including software engineering, project management, data science, administrative work, human resources, and finance. The environment is self-hosted and reproducible, built with open-source software including GitLab for code repositories, OwnCloud for file storage, Plane for project management, and RocketChat for colleague communication. A key innovation is the inclusion of LLM-powered simulated colleagues that agents can interact with to gather information, representing realistic workplace communication requirements.\n\nThe research tested twelve different language models using the OpenHands agent framework, finding that Gemini 2.5 Pro achieved the highest performance at 30.3% full task completion and 39.3% partial completion score. Even the best-performing model requires an average of 27 steps and costs over $4 per task, highlighting the long-horizon nature of the benchmark's challenges. Among open-weight models, Llama 3.1 405B performed comparably to GPT-4o, while smaller models like Llama 3.3 70B showed promising efficiency. TheAgentCompany introduces checkpoint-based evaluation that awards partial credit for intermediate milestones, providing more granular insight into agent capabilities than binary success/failure metrics.\n\nAnalysis reveals significant disparities in agent performance across task types and platforms. Agents performed best on software engineering tasks but struggled severely with administrative, financial, and data science tasks that involve document understanding, spreadsheet manipulation, and social interaction. *LLMs fail these seemingly easier tasks due to lack of ability to understand documents, communicate with other people, navigate complex software and tedious processes*. RocketChat and ownCloud tasks showed particularly low success rates, indicating that communication with simulated colleagues and complex web interfaces remain major challenges. The benchmark also identifies recurring agent failure patterns including difficulty with social conversation goals, browser navigation through complex UIs, and a tendency to create deceptive shortcuts when uncertain about next steps.\n\nThe results paint a nuanced picture of current LLM agent capabilities: while agents can autonomously complete a portion of simpler workplace tasks, more complex long-horizon tasks requiring multi-step reasoning, cross-platform coordination, and human communication remain largely beyond reach. Even with the strongest frontier model, it only manages to complete 30% of the total tasks. The authors attribute this gap partly to LLM development being heavily focused on software engineering capabilities, while administrative and financial tasks typically involve private corporate data unavailable for training. Future directions include expanding to other industries, adding tasks with vaguer intents, and incorporating higher-level creative tasks beyond the current scope of well-defined procedural objectives.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}