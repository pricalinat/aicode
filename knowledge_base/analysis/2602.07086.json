{
  "paper_id": "2602.07086",
  "title": "Rag Sql Api",
  "category": "mini_program_service",
  "year": 2026,
  "timestamp": "2026-03-01T13:52:00.321245",
  "summary": "# Evaluating Retrieval-Augmented Generation Variants for SQL and API Generation\n\nThis paper presents a comprehensive evaluation of three retrieval-augmented generation (RAG) variants for generating SQL queries and REST API calls in enterprise contexts, using SAP Transactional Banking as a realistic use case.\n\n## Research Context and Motivation\n\nEnterprise software systems increasingly expose functionality through two complementary interfaces: databases for ad-hoc information retrieval via SQL, and REST APIs for transactional operations. While large language models show promise for code generation, their effectiveness in domain-specific enterprise contexts remains underexplored, particularly when both retrieval (SQL) and modification (API) operations must be handled jointly—a common requirement in real-world enterprise assistants.\n\nThe research addresses three critical gaps in existing literature: current benchmarks treat text-to-SQL and text-to-API tasks in isolation; the comparative effectiveness of advanced RAG variants like Self-RAG and CoRAG remains unexplored for structured code generation; and no existing dataset enables controlled evaluation across both modalities within a shared enterprise domain.\n\n## Methodology\n\nThe researchers constructed a novel test dataset of 631 verified cases (346 SQL, 285 API) grounded in SAP Transactional Banking documentation. The dataset was generated through a four-step pipeline: automated generation using GPT-5 with API specifications and database schemas, input humanization to convert technical inputs into conversational phrasing, execution validation against mock SQLite databases and Postman mock servers, and expert review by three domain specialists.\n\nThree RAG variants were evaluated alongside a no-RAG baseline across three documentation contexts: database-only, API-only, and hybrid (DB+API), yielding 18 experimental configurations. Standard RAG retrieves top-5 similar documentation chunks and concatenates them with the prompt. Self-RAG performs LLM-based relevance filtering after initial retrieval, assessing each chunk for relevance with a 0.2 threshold. CoRAG implements iterative retrieval via query decomposition, generating sub-queries to progressively retrieve relevant information.\n\nEvaluation metrics included exact match accuracy (binary correctness after structural normalization), component match accuracy (partial credit for correct sub-components), execution accuracy (percentage of outputs that execute successfully), endpoint retrieval accuracy (for API tasks), and classification accuracy (for the combined task).\n\n## Key Results\n\nThe baseline without RAG achieves 0% exact match accuracy across all tasks, demonstrating that LLMs cannot reliably generate domain-specific SQL or API calls without documentation grounding. Introducing RAG yields substantial improvements: execution accuracy rises to 71-79% in single-documentation contexts, confirming that retrieval-augmentation is essential rather than optional.\n\nFor REST API call generation in API-only contexts, CoRAG achieves the highest execution accuracy at 79.30%, outperforming standard RAG (71.58%) and Self-RAG (68.51%). However, in hybrid documentation contexts, performance degrades significantly: Self-RAG's component match drops by 26.97% and CoRAG's execution accuracy falls to 50.53%, suggesting that retrieval control mechanisms are sensitive to documentation heterogeneity.\n\nFor SQL query generation in database-only contexts, all RAG variants perform comparably, with exact match accuracy ranging from 14.45% to 15.61%. The critical divergence emerges in hybrid contexts: standard RAG's exact match drops to 11.56% (-20.00% relative) and Self-RAG declines to 10.98% (-29.66% relative), while CoRAG maintains 15.32% exact match accuracy. Paired t-tests confirm this hybrid-context advantage is statistically significant (CoRAG vs. Self-RAG: p = 0.0026; CoRAG vs. standard RAG: p = 0.0091).\n\nFor the combined task requiring dynamic task classification, CoRAG achieves 10.29% exact match, 70.77% component match, and 68.93% execution accuracy, with classification accuracy remaining high across all variants (95.89%-97.79%). The performance gap between exact match and execution accuracy reflects that many generated queries are semantically equivalent but structurally variant.\n\n## Implications and Conclusions\n\nThe findings establish three primary conclusions. First, RAG is functionally essential: without retrieval, state-of-the-art LLMs achieve 0% exact match despite possessing generic syntactic knowledge. Second, CoRAG provides statistically significant robustness in heterogeneous contexts, maintaining performance when documentation includes both database schemas and API specifications while standard RAG and Self-RAG degrade by 20-30%. Third, retrieval policy design is a primary determinant of production viability—how documentation is retrieved and filtered matters as much as what documentation is available.\n\nThe research provides actionable guidance for deployment: prioritize retrieval infrastructure investment, select retrieval policies based on documentation context, implement execution-based validation as the primary correctness signal, and design for graceful failure given that even the best configuration achieves only 15.61% exact match accuracy. Future directions include multi-turn interaction evaluation, cross-domain validation, execution-in-the-loop generation, and cost-quality trade-off analysis across RAG variants.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}