{
  "paper_id": "2105.02978",
  "title": "Multilingual Graph",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper describes an industry-scale, end-to-end multilingual product retrieval system for e-commerce search, motivated by the drawbacks of maintaining separate per-country retrieval models. The authors argue that country-specific training increases operational burden, underperforms in low-traffic countries due to limited data, and struggles with second-language queries within a country. A central challenge is vocabulary gap: user queries tend to be short and informal, while product text is longer and more formal, making direct semantic matching with pretrained multilingual transformers unreliable without task-specific fine-tuning.\n\n*Offline experiments on five countries data show that our algorithm outperforms the state-of-the-art baselines by 35% recall and 25% mAP on average.*\n\nThe proposed approach builds a bipartite graph from query to product interactions in click and purchase logs and uses this structure to share information between queries and products. Architecturally, it uses (1) a transformer-based query encoder and (2) a product encoder that combines a transformer representation of the product with a graph convolution style aggregation over neighbor queries, with the transformer parameters shared between query and product sides and CLS embeddings used as the base features. Training uses a pairwise ranking triplet loss and emphasizes hard negative sampling, comparing behavior-based negatives, offline model-based negatives, and online model-based negatives generated on the fly, with online model-based negatives presented as the most suitable tradeoff for retrieval quality and scalability.\n\nFor multilingual training, the paper highlights that naive mixing of languages within the same batch can destabilize gradients and create unhelpful negative sampling, so it proposes language-aware training via one-language-at-a-batch combined with exponentially smoothed reweighting to oversample low-resource languages. Key reported evaluation and system takeaways include:\n- Offline evaluation on five countries US, ES, FR, IT, DE using Recall@10 and mAP, where the full model outperforms DSSM and untuned multilingual BERT baselines, and ablations show both the transformer encoder and the graph component contribute meaningful gains.\n- Negative sampling results showing online model-based hard negatives match or slightly exceed offline model-based negatives while avoiding large extra compute, and behavior-based negatives perform worst for the retrieval setting described.\n- Deployment design split into an online query encoder plus periodically refreshed, precomputed product embeddings, using cosine KNN search over the product vectors to retrieve candidates for downstream ranking and match set augmentation.\n- Online A/B experiments in three countries and two languages reporting increases in clicks, revenue, and conversion, along with a decrease in reformulated searches, supporting the claim that graph-augmented product embeddings help bridge queryâ€“product mismatches in practice.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2105.02978_multilingual_graph.pdf"
}