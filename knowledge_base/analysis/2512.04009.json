{
  "paper_id": "2512.04009",
  "title": "Comparison Shopping",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper introduces Learning to Comparison-Shop, a search ranking architecture built to better match how users make decisions in online marketplaces such as Airbnb. The authors argue that mainstream learning-to-rank systems often score items in isolation, which misses the fact that users first screen results individually and then compare a short list of candidates across multiple dimensions before deciding. They frame comparison shopping as a two-stage process: pointwise evaluation followed by set-wise comparison, and propose integrating these stages rather than training them as disconnected ranker and re-ranker components. *We decompose users comparison shopping behavior into two distinct stages: a pointwise evaluation stage and a set-wise comparison stage.*\n\nThe proposed Learning-to-Comparison-Shop system co-trains a lightweight initial ranker with a transformer-based set-wise re-ranker so the two stages share representations and remain behavior-aligned. Key design elements include:\n- Initial ranker: an MLP that consumes query and item features and outputs both a booking-probability logit and an embedding taken from the last hidden layer.\n- Set-wise re-ranker: an encoder-only Transformer that ingests the top-K embeddings (selected by the initial ranker), produces a context embedding, and combines it with each item embedding via an MLP to compute re-ranking logits.\n- Co-training objective: a weighted sum of list-wise losses for both stages, motivated by a derivation that relates the re-ranker score to the initial ranker score and a contextual term, supporting joint optimization and reducing stage conflict.\nFor serving efficiency, the models are exported separately, deployed hierarchically with the initial ranker on distributed leaf nodes and the re-ranker on a master node, and the initial ranker representations are reused to avoid redundant computation. *This A/B test demonstrated a +0.6% increase in booking conversion rate (CVR) with a p-value of 0.0.*\n\nExperiments use a multi-stage Airbnb ranking setup, training on roughly 360 million examples over about a year with booking labels, and re-ranking only the top 40 items due to the transformers O(K^2) cost. Offline, LTCS achieves the best NDCG among compared baselines (attn-DIN, a two-stage attn-DIN+ with separately trained stages, and the production MO-LTR), reporting NDCG 0.6982 and an improvement of about 1.78% vs MO-LTR, plus the lowest training variance. Online, a 3-week A/B test against MO-LTR reports +0.6% booking conversion rate and a -0.88% decrease in unique listing detail pages viewed in the same location before booking request, interpreted as improved search efficiency; hyperparameter studies motivate top-40 re-ranking, about 30 transformer layers, and a midrange re-ranker loss weight (deployed at 0.5 after online validation). The paper closes by stating LTCS improves both ranking quality and business metrics in production, and includes a disclosure that no AI-assisted tools were used to write the manuscript.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2512.04009_comparison_shopping.pdf"
}