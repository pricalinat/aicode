{
  "paper_id": "2404.09091",
  "title": "Semantic Product Id",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper describes a semantic, low-latency query to product identification system for Adobe search and autocomplete, aimed at reliably triggering contextual product app cards even when product intent is implicit. It replaces brittle regular expressions and simple named entity recognition that missed minor phrasing variations and rarely handled implicit intent such as edit video mapping to Premiere Pro and Rush. The core idea is to learn product intent from user behavior and curated product content so users are routed to the most relevant Adobe product experiences across Adobe.com and Creative Cloud surfaces.\n\n*Our semantic model led to: >25% relative improvement in CTR (click through rate) across the deployed surfaces; a >50% decrease in null rate;*  \n*We pretrained a LM based on Microsoft DeBERTa v3 starting from publicly available pretrained weights on the HelpX document dataset*\n\nTraining data covers 46 products and merges four sources: noisy but large HelpX behavioral click logs from January 2021 to August 2022, high-quality HelpX document title and description pairs (weighted more heavily), an explicit product NER dataset from Creative Cloud queries, and an Adobe Express query dataset to compensate for limited historical clicks. The behavioral dataset assigns a relevance weight using a log click ratio so higher-clicked query document pairs influence training more while still retaining long-tail examples. Dataset sizes are reported as 177,500 HelpX behavioral rows, 11,757 HelpX document rows, 6,637 Express rows, and 5,208 explicit NER rows.\n\nThe model is a domain-adapted DeBERTa v3 backbone pretrained with masked language modeling on HelpX documents chunked into 128-token blocks, reaching perplexity 7.47 and yielding a reported 14% improvement in downstream classification accuracy versus using a general pretrained LM. On top, a multi-label classifier uses a two hidden-layer MLP with 0.5 dropout, learning rate 1e-5, weighted binary cross entropy, and a freeze then unfreeze training schedule, producing per-product probabilities because many queries map to multiple products. Offline evaluation on a held-out 10% test set reports precision 0.961, recall 0.941, accuracy 0.970, and F1 0.949 across 22,849 rows, while a manual expert evaluation on 2,700 unseen implicit-intent Creative Cloud queries reports accuracy 0.931 with strict all-predicted-products correctness. An A B test across Creative Cloud app and website locales reports a 2x increase in app cards surfaced, over 50% fewer queries with no app cards, and over 25% relative click-through rate improvement; future work targets multilingual implicit intent and better long prompt understanding for retrieval augmented generation use cases.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2404.09091_semantic_product_id.pdf"
}