{
  "paper_id": "2507.05639",
  "title": "Paper",
  "category": "mini_program_service",
  "year": 2025,
  "timestamp": "2026-03-01T13:59:43.743558",
  "summary": "## ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?\n\nThis paper introduces ECom-Bench, an open-source benchmark framework to evaluate LLM agents in realistic e-commerce customer support, with an emphasis on multi-turn, persona-driven interactions, multimodal inputs, and tool use via Model Context Protocol MCP. It argues that prior benchmarks miss key real-world requirements such as domain-specific tasks, dynamic user behavior, and multimodal evidence, creating an incomplete picture of whether agents can safely and reliably handle customer service workflows. *Even advanced models like GPT-4o achieve only a 10â€“20% pass^3 metric in our benchmark, highlighting the substantial difficulties posed by complex e-commerce scenarios.*\n\nECom-Bench is constructed from historical e-commerce dialogues and includes user simulation plus task instances designed to reflect practical business operations rather than toy problems. Key pieces include persona profiles derived from real interactions, a domain database with annotated data classes such as products, logistics, orders, and invoices, domain documentation that serves as partial world descriptions, and 21 categories of tools spanning product inquiry, order modification, returns and exchanges, and a multimodal tool for image understanding. Tasks are manually verified and cover 53 task types, with 18 involving multimodal interaction, and evaluation scores reflect (1) correctness of resulting database state, (2) whether required retrieval tools were invoked, and (3) whether outputs contain necessary keywords; robustness is measured with pass^k across repeated trials of the same task.\n\nExperiments compare multiple proprietary LLMs and multimodal models and highlight that tool-calling accuracy and consistent multi-step execution remain major failure points. Reported pass^1, pass^2, pass^3 results include GPT-4o at 44.03, 26.42, 16.98; Doubao-1.5-Pro-32k at 38.99, 23.90, 16.98; DeepSeek-V3 at 36.48, 21.38, 15.09, with multimodal end-to-end planners generally lagging. The paper analyzes common error modes including wrong argument filling, wrong tool or object selection, and partially resolved outcomes where an agent proposes actions without executing the needed tool steps, and it finds action-oriented database modifications to be especially difficult. An ablation study shows persona traits matter: anger, low patience, and vague communication can reduce completion by increasing hallucinations, shortening information gathering in multi-turn dialogue, and making parameter specification harder. Limitations include focus on the home appliances and furniture vertical, partial reliance on LLM-based data synthesis, a relatively small set of 53 task instances, and user simulation that depends on domain expertise and may not fully capture authentic conversational dynamics. *We analyze 159 trajectories 53 tasks executed 3 times and categorized the errors into three main types.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}