{
  "paper_id": "2601.08118",
  "title": "Mirrorbench",
  "category": "mini_program_service",
  "status": "success",
  "summary": "MirrorBench is a reproducible, extensible benchmark for evaluating how human-like LLM-based conversational user-proxy agents sound, explicitly separating user realism from downstream task success. It targets utterance-level realism conditioned on a specific dialogue regime rather than a single general notion of human behavior, motivated by the practical use of user proxies for testing, evaluation, stress tests, and synthetic data generation. *MirrorBench provides a reproducible benchmark for evaluating the human-likeness of LLM-based user proxies, assessing how closely a proxy’s utterances match real human users.*\n\nThe framework combines two complementary metric families and anchors them to human reference data: lexical or distributional realism via MATTR, Yule’s K, and HD-D, and behavioral realism via LLM-judge metrics GTEval, Pairwise Indistinguishability, and Rubric-and-Reason. MirrorBench runs goal-conditioned synthetic rollouts between a user-proxy model and an assistant model under clear role separation, then scores only the proxy user turns, using calibration controls Human–Human and Proxy–Proxy to contextualize judge scores and expose judge bias. It packages and preprocesses four public datasets spanning different regimes, ChatbotArena for open-domain chat, ClariQ for information-seeking with clarification, OASST1 for assistant-style chat, and QULAC for clarification-heavy interactions, with stratified sampling and per-dataset normalization such as human-anchored z-scores for lexical metrics.\n\nExperiments across five user-proxy LLMs show systematic gaps from real users and a recurring realism–diversity tension that varies by dataset, where strong judge-based realism does not guarantee human-aligned lexical diversity, especially in clarification-centric settings like QULAC. The paper reports robustness analyses for judge sensitivity, assistant sensitivity, and multi-seed variance, finding that judge choice can shift absolute scores and sometimes close rankings, while assistant swaps usually preserve overall ordering; it also validates judge reliability by correlating judge scores with blinded human annotations on ChatbotArena. Practical scaling details include telemetry for tokens, latency, and cost, with results indicating judge calls dominate token usage and costs, plus an open-source CLI-driven system that plans runs, persists artifacts in SQLite, supports caching, and produces replayable manifests and JSON reports. *Human-likeness is inherently scenario-dependent: whats sounds realistic in open-domain chat maybe unrealistic in information-seeking or clarification-heavy interactions.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2601.08118_MirrorBench.pdf"
}