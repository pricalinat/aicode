{
  "paper_id": "1302.2318",
  "title": "Search Metrics",
  "category": "ecommerce_evaluation",
  "year": 2013,
  "timestamp": "2026-03-01T13:49:58.941447",
  "summary": "This dissertation by Pavel Sirotkin addresses a fundamental problem in information retrieval research: how to evaluate the evaluation metrics themselves. Rather than simply applying various metrics to measure search engine quality, Sirotkin investigates whether these metrics actually correlate with real user preferences. The work introduces a novel meta-evaluation framework called the Preference Identification Ratio (PIR), which measures how well a given metric can predict which of two result lists a user would prefer. This represents a significant departure from traditional approaches that evaluate metrics against each other or against expert judgments without establishing their connection to actual user behavior.\n\nThe empirical study involved 31 undergraduate students who submitted 42 real queries, providing both individual result ratings and preference judgments between original Yahoo search results and randomized versions. Participants performed preference evaluations (comparing two result lists side-by-side), satisfaction evaluations, and individual result ratings on a six-point scale. This rich dataset enabled evaluation of multiple metric parameters including different discount functions, cut-off ranks, relevance scales, and threshold values. The methodology also allowed comparison between scenarios where the same users provided both result ratings and preferences versus different users providing each.\n\nThe findings challenge several widely-held assumptions about search engine evaluation. Perhaps most notably, traditional Mean Average Precision (MAP) with rank-based discounting consistently performed poorly compared to other metrics, while MAP without any discount performed significantly better. Normalized Discounted Cumulative Gain (NDCG) emerged as generally the best-performing metric, particularly with its standard log2 discount function. The study also revealed that evaluation quality often decreases when considering results beyond rank 6-8, suggesting that typical practices of evaluating top-10 results may actually reduce accuracy rather than improve it.\n\n*After the first five ranks, further results of a leading search engine (Yahoo) are on average no better than the average of the first 50 results.* This finding has important implications for evaluation methodology, as it suggests that beyond a certain point, additional results introduce noise rather than useful signal. The research further demonstrated that six-point relevance scales outperform binary or three-point scales, and that session duration and simple click counts are poor predictors of user preference. The dissertation concludes by recommending NDCG with log2 discount as a default metric while acknowledging that optimal parameters depend on specific evaluation contexts.\n\nThe theoretical contribution extends beyond metric comparisons to include the Web Evaluation Relevance (WER) model, which provides a multidimensional framework for understanding relevance judgments across different contexts. This model helps explain why certain metrics perform better under specific conditions by categorizing evaluation approaches along dimensions of representation, resource type, and context. The work also identifies important areas requiring further research, including the potential of snippet-based evaluation and the need for personalized relevance judgments that bridge the gap between same-user and different-user rating scenarios.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}