{
  "paper_id": "2410.11655",
  "title": "Spelling Correction Rag",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper tackles contextual spelling correction for e commerce search, where rapidly changing brand names and deliberately unconventional spellings make systems prone to over correction, especially for novel brands. The core idea is a Retrieval Augmented Generation approach: retrieve likely product or brand strings from a live catalog and place them into an LLM prompt so the model can decide whether a correction is needed using up to date context. The authors report broad experiments and analyses showing that adding retrieval improves accuracy over a standalone LLM, and that fine tuning the LLM to better use retrieved context yields the biggest gains, particularly on queries containing brand names.\n\nMethod and evaluation highlights:\n- Retrieval: BM25, Fuzzy BM25, and ColBERT over two catalog sizes, 60K documents and 572K documents\n- LLMs: primarily Mistral 7B and Claude 3 sonnet, with a zero shot baseline table that also includes larger models such as Mixtral\n- Data: 10K annotated input query and label query pairs sampled from search logs collected between 2021 and 2023, with a 2 plus 1 annotation process for label quality\n- Metric: F1 based on exact match after punctuation removal, reported as percentages\n- Fine tuning variants: Basic Fine Tuning on 50K input label pairs, and Contextual Fine Tuning on 50K triples that include retrieved context\n\nResults show consistent improvements from retrieval and further large jumps from contextual fine tuning. In zero shot without retrieval, Mistral 7B reaches 28.1 F1 and Claude 3 sonnet reaches 34.7, while Mixtral 47B is the strongest baseline in the table at 57.4. With RAG using a frozen LLM, the best reported frozen setup for Claude 3 sonnet is ColBERT with the 572K index at 39.3 F1, and for Mistral 7B it is ColBERT with the 572K index at 35.9. With fine tuning, the strongest reported configuration is Mistral 7B with ColBERT RAG plus Contextual Fine Tuning, reaching 71.0 F1 overall and 60.8 on brand queries, compared with 64.1 overall and 44.1 on brands for Basic Fine Tuning without RAG. A focused test on 525 brands present in evaluation but absent from the fine tuning data reports 78.4 F1 for Contextual Fine Tuning with ColBERT, supporting the claim that the approach helps with brands that are novel from the LLM perspective; latency costs are small, with retrieval adding about 2.42 percent for 60K and 2.79 percent for 572K to overall generation time.\n\n*Incorporating RAG leads to a slight latency increase; however, it remains within acceptable limits for real time applications.*\n\n*The overall message is very clear: if it is feasible to fine tune the LLM with context, that is likely to lead to very substantial performance improvements.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2410.11655_spelling_correction_rag.pdf"
}