{
  "paper_id": "2403.07714",
  "title": "Paper",
  "category": "mini_program_service",
  "status": "success",
  "summary": "StableToolBench proposes a more reproducible way to benchmark tool-using large language models by removing two major sources of instability seen in prior work like ToolBench: real online APIs that change or become unavailable over time, and automatic evaluation that is unreliable for complex tool-use tasks. The paper first documents concrete reproducibility gaps when rerunning ToolBench baselines months later, then introduces a replacement benchmark environment that aims to keep breadth while making results comparable across time.\n\nThe core contribution is a virtual API server plus a stable evaluation system. The virtual server combines (1) a caching system that stores validated API call responses keyed by category, tool, API name, and arguments, and (2) an LLM-based API simulator that generates responses when the cache misses and the real API is unavailable, using the original tool documentation and up to five cached few-shot examples to better match expected behavior. For evaluation, it shifts from ToolBench-style pass and win rates that depend on uncertain solvability judgments and weaker judges to a two-phase process: first filter tasks to a fixed set of solvable tasks via majority vote from strong models, then score answers with Solvable Pass Rate and Solvable Win Rate using GPT-4 class judgments to reduce randomness.\n\nExperiments and analyses emphasize stability under API failures, realism of simulations, and evaluator reliability. The authors show that many ToolBench APIs fail in practice and that performance can degrade sharply as more tools are made unavailable, while the virtual API approach keeps measured performance changes within variance even when substantial portions of tools are down. Human studies suggest simulated outputs follow documentation well and are hard to distinguish from real API outputs, and embedding-based analyses indicate response diversity remains similar between real and simulated APIs. The paper notes limitations: higher cost and reliance on closed-source GPT-4-class models for both simulation and evaluation, and the need to keep updating the cache as new methods and calls appear.  \n\n*only 44.4% of API calls are successful*  \n\n*human annotators cannot distinguish simulated and real APIs very well*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2403.07714_paper.pdf"
}