{
  "paper_id": "1704.04579",
  "title": "Chatbot Evaluation",
  "category": "mini_program_service",
  "year": 2017,
  "timestamp": "2026-03-01T13:59:15.977884",
  "summary": "This paper reviews how to evaluate the quality of chatbots and intelligent conversational agents, then proposes a practical assessment method. It argues that modern chatbot deployment is accelerating due to open-source code, widely available platforms, and SaaS offerings, and that quality matters both for beneficial uses like customer support and learning and for limiting social harm like misinformation and harassment. The authors synthesize prior research into a set of quality attributes and then recommend a structured way to compare systems or versions over time using the Analytic Hierarchy Process, or AHP.\n\n*The most profound technologies are those that disappear. They weave themselves into the fabric of everyday life until they are indistinguishable from it.* The literature review (systematic searches of Google Scholar, JSTOR, and EBSCO Host covering 1990 to 2017) narrows thousands of hits to a focused set of scholarly and industry sources, excluding highly technical programming and speech-recognition work. Extracted attributes show strong alignment with ISO 9241 usability framing, organized around effectiveness, efficiency, and satisfaction, with emphasis on both functional performance and human-centered outcomes:\n- **Efficiency and performance**: robustness to unexpected input, graceful degradation, avoiding inappropriate utterances, and providing escalation paths to humans.\n- **Effectiveness and functionality plus humanity**: accurate interpretation and outputs, task execution and follow-up, breadth of knowledge, maintaining themed discussion, answering specific questions, and transparency about chatbot identity, with debate over whether passing the Turing Test should be a goal.\n- **Satisfaction factors**: affect and personality cues, warmth and authenticity, ethics and behavior such as privacy, nondeception, safety sensitivity, and accessibility including neurodiverse needs like extra response time and text interfaces.\n\nFor quality assessment, the paper finds few works where quality assurance is the central focus, and highlights recurring gaps: subjectivity of response utility, domain-specific knowledge differences, and lack of standard guidance on which metrics to use when. Prior approaches include PARADISE-style performance functions based on task success and costs, information-retrieval metrics like precision, recall, and F1 (not sufficient alone), iterative multi-evaluator subjective ratings, ordinal scoring across multiple attributes, architectural and affective metric frameworks for emotional agents, and goal-oriented adaptations of Goal Question Metric. The authors then synthesize these into an AHP-based method: define a hierarchy of prioritized attributes, collect metrics for each attribute from user sessions, perform pairwise comparisons among categories and attributes, compute priority weights via eigenvectors, and check consistency scores (ideally under 10 percent, often acceptable under 20 percent). *Based on maximizing satisfaction and task success, and minimizing costs.* The conclusion positions the attribute list as a checklist for implementation teams and emphasizes AHP as a flexible comparison tool that can accommodate many metric types and supports ongoing evaluation as conversational agents adapt over time.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}