{
  "paper_id": "1901.04085",
  "title": "Passage Reranking Bert",
  "category": "product_matching",
  "status": "success",
  "summary": "The paper presents a straightforward way to repurpose BERT as the second stage of a query answering or retrieval pipeline: passage re-ranking. The authors report state of the art performance on TREC-CAR and a top leaderboard result on MS MARCO passage retrieval, including a 27 percent relative improvement in MRR@10 over the prior best MS MARCO entry at the time. *Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task.*\n\nMethodologically, the re-ranker scores each candidate passage independently by feeding the query as sentence A and the passage as sentence B into BERT, using the final [CLS] representation with a single-layer classifier to predict relevance. Queries are truncated to 64 tokens, and the combined query plus passage sequence is capped at 512 tokens; training fine-tunes a pretrained BERT model with a cross-entropy objective over relevant and non-relevant passages drawn from BM25 top results. *We use a BERTLARGE model as a binary classification model, that is, we use the [CLS] vector as input to a single layer neural network.* Key training details include TPU v3-81 fine-tuning on MS MARCO with batch size 128 for 100k iterations (about 30 hours, 12.8M pairs, under 2 percent of the available training tuples), Adam with learning rate 3 × 10−6, weight decay 0.01, 10k step warmup with linear decay, and dropout 0.1.\n\nExperiments cover MS MARCO and TREC-CAR. For TREC-CAR, the paper highlights a data leakage concern because standard BERT pretraining uses full Wikipedia, so they instead pretrain on only the Wikipedia subset aligned with TREC-CAR training to avoid exposure to test content; fine-tuning pairs are generated by retrieving the top 10 passages per query with BM25 using Anserini. Reported results show BERT Base and BERT Large substantially outperform BM25 and several neural baselines, with BERT Large reaching 36.5 MRR@10 on MS MARCO eval, 35.8 MRR@10 on MS MARCO dev, and 33.5 MAP on TREC-CAR test; the paper also emphasizes strong performance with relatively few fine-tuning examples, noting that BERT Large trained on 100k pairs (under 0.3 percent of MS MARCO training data) already exceeds the prior state of the art by 1.4 MRR@10 points. Code to reproduce the results is provided at `https://github.com/nyu-dl/dl4marco-bert`.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/1901.04085_passage_reranking_bert.pdf"
}