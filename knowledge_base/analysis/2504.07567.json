{
  "paper_id": "2504.07567",
  "title": "Image Embedding Benchmark",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T14:02:18.416258",
  "summary": "This paper benchmarks off the shelf image embedding foundation models for real world e-commerce classification and image to image retrieval, focusing on how pretraining paradigm and tuning strategy affect performance and cost. It compares supervised backbones, self supervised learning embeddings, and contrastive text image models across six open datasets spanning fashion, retail products, food, cars, and online products, and evaluates three adaptation modes: full fine-tuning, top-tuning (training a small MLP head on frozen embeddings), and cross-tuning (adapting on one dataset and evaluating on another). *Dataset size and granularity strongly influence adaptation strategies, with smaller datasets benefiting the most from top-tuning.*\n\nThe core results emphasize practical trade offs rather than a single universal winner:\n- Full fine-tuning is consistently strong, but can be computationally expensive and is not always necessary for top retrieval performance.\n- Contrastive text image embeddings often deliver state of the art retrieval without domain specific fine-tuning; among the tested text image models, SigLIP is reported as best on five of six datasets, with Apple CLIP leading on one.\n- Self supervised embeddings show higher variance than supervised ones, sometimes performing very well but less predictably; top-tuning generally stabilizes and boosts SSL results, though it can also hurt some SSL variants (notably MAE and DINO with ResNet50 in their reported averages).\n- For supervised backbones, ViT Base is highlighted as a strong balance of performance and embedding size for pre-trained retrieval, while ViT Large can underperform on smaller datasets despite higher training cost; in Cars196 fine-tuning for classification, ConvNeXt Base is the top model reported, reaching 0.93 accuracy.\n- Retrieval metrics used in the benchmark are shown to be strongly correlated, supporting the use of a primary retrieval metric like mMP@5 for comparisons. *All retrieval metrics used in this study are highly correlated.*\n\nMethodologically, classification is evaluated by training a small 2 to 3 layer MLP on frozen embeddings with Bayesian hyperparameter search, while retrieval is evaluated by indexing normalized embeddings in a vector database and using L2 nearest neighbor search with metrics including mMP@5, mR@1, MAP, MRR, and NDCG. The paper reports that fine-tuning on the target dataset reliably improves retrieval versus purely pre-trained supervised models, but also documents meaningful cases where pre-trained SSL or text image models outperform fine-tuned supervised baselines depending on dataset. Cross-tuning is mostly harmful (large drops in mMP@5) unless source and target datasets share similar visual characteristics, where modest gains are possible; the appendices turn these findings into a step by step deployment oriented guide recommending top-tuned text image models for visual search, full fine-tuned supervised models for stable categorization, and cautious cross-domain transfer only when dataset similarity is high.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}