{
  "paper_id": "2503.22458",
  "title": "Multiturn Agent Evaluation",
  "category": "mini_program_service",
  "year": 2025,
  "timestamp": "2026-03-01T13:56:33.783495",
  "summary": "# Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey\n\nThis comprehensive survey from Microsoft researchers examines evaluation methods for large language model-based agents in multi-turn conversational settings. The authors systematically reviewed nearly 250 scholarly sources using a PRISMA-inspired framework, covering literature from 2017 to 2025 across peer-reviewed articles, journals, preprints, and industry reports. The study establishes two interrelated taxonomy systems: one defining what to evaluate and another explaining how to evaluate, providing a structured blueprint for improving conversational agent assessment.\n\n## Core Evaluation Dimensions\n\nThe first taxonomy identifies key components of LLM-based agents and their evaluation dimensions. **Task completion in multi-turn conversations** measures how well agents fulfill user requests, extending beyond simple query resolution to encompass contextual coherence, error propagation mitigation, and evolving user intent adaptation. **Interaction patterns** analyze dialogue structures including recollection, follow-up, expansion, and refinement—demonstrating how LLMs progressively build context and guide conversations across multiple turns. **User experience and safety** addresses satisfaction, engagement, and safeguards against harmful content, including vulnerabilities to coreference-based attacks, prompt leakage, and backdoor insertions.\n\nThe **action and tool-use components** require evaluating a three-layered framework: basic API interaction proficiency, coherent chain-of-execution across turns, and reliability against action hallucination. The **memory system** evaluation spans turn memory (immediate dialogue context), conversational memory (extended dialogues spanning 40+ utterances), and permanent memory (cross-session personalization). The **planner** component involves task modeling, task decomposition into executable sub-components, adaptation to dynamic user intents, and reflection mechanisms for plan verification and selection.\n\n## Evaluation Methodologies\n\nThe second taxonomy categorizes evaluation approaches into four main methodologies. **Annotation-based evaluation** relies on expert-curated benchmarks using traditional metrics like BLEU, ROUGE, and METEOR, alongside advanced semantic methods such as BERTScore and DialogRPT. **Automated metrics** include point-wise scoring for individual responses and pair-wise or list-wise comparison for assessing accuracy, coherence, and helpfulness. **Hybrid strategies** combine human assessments with quantitative measures. **Self-judging methods** utilize LLMs as evaluators, providing direct numeric scores or explaining ranking decisions—a paradigm gaining traction due to reduced dependence on manual evaluation.\n\nThe paper discusses benchmark datasets including MT-Bench for multi-turn evaluation, AgentBench for agent-centric tasks, API-Bank for real-world tool usage assessment, and specialized benchmarks for memory retention (LoCoMo), factual verification (DialFact), and long-term conversations spanning months.\n\n## Challenges and Future Directions\n\nThe authors identify several critical challenges in current evaluation practices. **Unified and adaptive evaluation frameworks** are needed to assess conversation turns holistically rather than in isolation, capturing dynamic interplay among successive turns. **Memory and context retention** benchmarks must differentiate between short-term recall and long-term integration to address context leakage or drift over prolonged interactions. **Test-time evaluation for self-assessment** would enable agents to gauge coherence, factual accuracy, and context alignment in real-time during response generation.\n\nAdditional challenges include **dynamic self-correction mechanisms** for real-time error detection and correction, **tool-use and action planning evaluation** in extended interactions, **scalability through annotation-free pipelines** using self-supervised techniques, and **privacy preservation** using trusted execution environments and federated learning. The paper concludes by emphasizing that these contributions bridge historical insights with modern practices, paving the way for next-generation reliably evaluated conversational AI systems.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}