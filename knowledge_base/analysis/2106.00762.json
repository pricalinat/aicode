{
  "paper_id": "2106.00762",
  "title": "Ab Two Sided",
  "category": "ecommerce_evaluation",
  "year": 2021,
  "timestamp": "2026-03-01T13:48:14.152868",
  "summary": "# A/B Testing for Recommender Systems in a Two-sided Marketplace\n\nThis paper addresses a fundamental challenge in experimentation for two-sided marketplaces like LinkedIn, Amazon, and Facebook, where platforms connect consumers (viewers) with producers (sellers, content creators). The authors from LinkedIn propose a novel experiment design mechanism called UniCoRn (Unifying Counterfactual Rankings) to measure the producer-side impact of treatment variants in A/B tests.\n\n## The Core Problem\n\nTraditional A/B testing works well for consumer-side measurements because each consumer acts independently. However, measuring producer-side impact is fundamentally harder because a producer's experience depends on the treatment assignment of all their connected consumers. When some consumers see treatment and others see control, producers receive mixed exposure that complicates causal inference. Existing approaches include graph cluster-based randomization (which suffers in dense networks) and treatment propagation assumptions (which lack strict error control). This paper identifies that existing methods often require knowing the network structure in advance, making them unsuitable for dynamic graphs common in industrial settings.\n\n## The UniCoRn Solution\n\nThe authors propose a rank-based approach that unifies multiple counterfactual rankings based on producer treatment assignment. The key insight is that each producer-side treatment corresponds to a unique viewer-side ranking when all producers receive that treatment. When producers are allocated to different variants, conflicting counterfactual rankings emerge. UniCoRn resolves these conflicts through a mixing mechanism controlled by a parameter α ∈ [0, 1], which explicitly trades off experiment quality against computational cost.\n\n*Algorithmically, UniCoRn works by: (1) obtaining initial rankings using control model T0, (2) selecting a subset of control items to mix with treatment items based on α, (3) computing rank-based scores combining both treatment and control rankings, and (4) reranking the mixed items while preserving ordering within each producer group.*\n\n## Theoretical Contributions\n\nThe paper proves that UniCoRn with α = 1 is optimal with respect to a design inaccuracy measure that compares actual rankings to ideal counterfactual rankings. The proof relies on the rearrangement inequality, showing that no other design can achieve lower expected squared error. Additionally, the authors derive bias and variance bounds for the observed rankings, demonstrating that the bounds are tight when treatment and control rankings are reversed. The theoretical results extend to multiple treatment scenarios, not just the single treatment case.\n\n## Empirical Findings\n\nThrough simulations with 100-position recommendation environments, the authors show that UniCoRn(1) outperforms UniCoRn(0) on RMSE while performing similarly on MAE. Lower correlation between treatment and control scores creates more challenging design problems due to increased conflicts in counterfactual rankings. Compared to existing methods—HaThucEtAl (limited to small ramps) and OASIS (score-based)—UniCoRn variants demonstrate superior treatment effect estimation error, validating the advantage of rank-based methods over score-based approaches. The experiments also reveal that α sensitivity increases at smaller treatment proportions and with non-linear response functions.\n\n## Production Deployment\n\nThe authors deployed UniCoRn in LinkedIn's edge recommendation system serving tens of millions of members and billions of daily recommendations. Using α = 0 (minimum computational cost), they ran two experiments: a candidate generation model change and a ranking model change optimizing for viewee retention. Both experiments showed statistically significant positive impacts on Weekly Active Unique users and Sessions, with the candidate generation experiment showing +0.51% WAU improvement and the ranking model showing +0.13% WAU improvement. The implementation added no statistically significant serving latency.\n\n## Limitations and Future Directions\n\nThe framework captures differences in producer exposure distribution but cannot measure treatment effects on viewer attention patterns. The optimality results for design inaccuracy do not automatically translate to treatment effect estimation accuracy without additional assumptions about the rank-to-response function. Future work could explore alternative loss functions in the design framework and extend the approach to multi-partite graphs connecting more than two entity types, such as food delivery platforms with users, drivers, and restaurants.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}