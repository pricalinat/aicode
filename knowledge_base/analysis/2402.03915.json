{
  "paper_id": "2402.03915",
  "title": "Learning Metrics",
  "category": "ecommerce_evaluation",
  "year": 2024,
  "timestamp": "2026-03-01T13:59:43.817596",
  "summary": "This paper tackles a practical A/B-testing bottleneck: North Star metrics like long-term revenue or retention are often delayed and insensitive, so experiments must run longer and still suffer many false negatives. The authors propose learning new evaluation metrics from short-term signals that explicitly maximise statistical power with respect to the North Star, so teams can detect meaningful effects sooner and with fewer users. *We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness*.\n\nMethodologically, the work builds learned metrics as linear combinations of logged per-variant metric means plus their variance and covariance estimates, using standard significance testing (Welchs t-test) and emphasising correct handling of multiple comparisons and sequential peeking via corrections at the experiment level. The key critique is that maximising average z-scores can overfit and fail in the way practitioners care about, because it may not reduce type-II errors and can produce dangerous sign disagreements with the North Star, described as type-III or type-S errors. Instead, the authors optimise objectives based on minimising p-values, and a stronger variant that minimises a log-transformed p-value surrogate that more heavily penalises confident wrong-direction results, and they recommend evaluating learned metrics as complements to existing metrics rather than replacements.\n\nEmpirically, they label and use logs from two large short-video platforms, ShareChat and Moj, each with over 160 million monthly active users, covering 153 A/B experiments from 2023 plus over 25,000 A/A pairs, with roughly 100 available metrics and a deliberate restriction to 10 short-term input metrics to limit overfitting and improve interpretability. Using leave-one-out cross-validation, p-value and log p-value objectives improve median sensitivity and avoid the type-III issues seen with z-score maximisation; in downstream error-rate analyses, learned metrics reduce type-II error substantially without inflating type-I error, and provide the biggest gains when combined with the North Star and a top proxy under Bonferroni correction, yielding up to 210 percent relative power improvement over the North Star alone. They also show how this translates into experimentation cost reductions via relative z-scores, reporting that constant confidence can be achieved with far fewer samples, and they introduce spherical regularisation to speed optimisation for scale-free objectives, observing up to 40 percent fewer iterations to converge. *Alternatively, we can obtain constant statistical power at a sample size that is down to 12% of what the North Star requires*.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}