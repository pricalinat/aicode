{
  "paper_id": "2412.03581",
  "title": "Ecommerce Ltr Survey",
  "category": "ecommerce_evaluation",
  "year": 2024,
  "timestamp": "2026-03-01T13:49:08.577728",
  "summary": "# Summary: Survey on E-Commerce Learning to Rank\n\nThis academic survey paper, published in Procedia Computer Science in 2024, provides the first comprehensive review and experimental comparison of learning to rank (LTR) algorithms specifically designed for e-commerce product ranking. The authors address a significant gap in the literature: while LTR is well-studied in web search, e-commerce-specific approaches remain largely private due to business competition and data confidentiality.\n\n## Core Contributions\n\nThe paper makes two primary contributions. First, it provides detailed explanations of LTR methods tailored to e-commerce, describing the specific challenges and probable solutions. Second, it performs experiments with multiple LTR algorithms on a real eBay dataset, ranking them based on performance. The authors use NDCG@K and ERR@K (with K=10,20) as evaluation metrics, employing both BERT-based and spherical text embeddings for feature representation.\n\n## LTR Method Categories\n\nThe survey organizes LTR approaches into three groups. **Pointwise methods** (Linear Regression, Polynomial Regression, SVR, Random Forest, Gradient Boosting) treat each query-product pair independently, predicting a relevance score. **Pairwise methods** (RankNet, LambdaRANK, FRank, LambdaMART, SVM) compare pairs of products to determine relative ranking. **Listwise methods** (ListNet, ListMLE, p-ListMLE, DeepQRank) consider entire permutations of products, making them closest to actual ranking behavior.\n\n## E-Commerce Challenges\n\nThe paper identifies four key challenges that distinguish e-commerce LTR from web search. Unlike web queries where top-3 results typically satisfy users, e-commerce buyers browse extensively—averaging 20 products on eBay before purchasing. Same products appear multiple times from different sellers with varying prices and features. Rankings change rapidly due to demand shifts, user preferences, trends, and pricing. Finally, e-commerce queries are shorter and product-property dominant rather than natural language, requiring specialized embedding approaches.\n\n## Experimental Results\n\nThe authors tested 15 LTR methods using eBERT and spherical text embeddings. Among pointwise approaches, Gradient Boosting achieved the best NDCG@10 of 0.712 with spherical embedding. LambdaMART was the top pairwise method at 0.719. Listwise methods performed best overall, with ListMLE reaching 0.725 NDCG@10—the highest score in the experiments. Spherical text embedding consistently outperformed eBERT across all methods, demonstrating better suitability for similarity and ranking tasks.\n\nThe authors conclude that Gradient Boosting, LambdaMART, and ListMLE represent the strongest approaches for e-commerce LTR, though performance may vary across datasets. They note that their code will be shared publicly to facilitate future research in this domain.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}