{
  "paper_id": "2411.13547",
  "title": "Paper",
  "category": "mini_program_service",
  "status": "success",
  "summary": "The paper introduces TOOLSCAN, a diagnostic benchmark for evaluating tool-use large language models beyond simple task success rates. It is positioned as a response to prior tool-agent benchmarks that mainly report end success, making it hard to understand why models fail and how to improve them. TOOLSCAN targets realistic multi-turn tool interactions across 10 environment categories and over 30 tasks, backed by a 150-query human-annotated dataset with multiple valid ground truth trajectories and a built-in feedback mechanism to help pinpoint and correct tool-calling mistakes.\n\nTOOLSCAN characterizes seven recurring tool-use error patterns and evaluates them consistently across environments, emphasizing that tool calls are brittle and small mistakes can cascade downstream. *We observe that Insufficient API Calls (IAC) is the most common error pattern.* The seven error types are:\n- Insufficient API Calls, where the model does not call enough tools to fully satisfy the query\n- Incorrect Argument Value, including missing required arguments or choosing wrong values\n- Incorrect Argument Name, where the model hallucinates parameter names\n- Incorrect Argument Type, where the model uses the wrong data type\n- Repeated API Calls, where the model loops by calling the same tool repeatedly\n- Incorrect Function Name, where the model calls a nonexistent tool\n- Invalid Format Error, where the output does not match the required call format\n\nThe dataset is built by collecting seed queries from existing tool benchmarks, then using three augmentation strategies to increase complexity while preserving solvability: constraint-based augmentation that adds argument-specific constraints, sentence transformation that rephrases without changing intent, and relevance-based augmentation that adds distracting but related information. The authors generate roughly 600 candidates and filter down to about 150 via feasibility checks, constraint validation through execution, and manual review for diversity. The evaluation framework is deterministic and described in a POMDP-style setup, with a feedback loop that checks formatting, whether the action is in the allowed tool set, argument validity, and argument type before executing the call and returning observations; metrics score the proportion of tool calls without each error type, and for some errors use the best match across multiple ground truth trajectories. *The brittle nature of LLM-generated tool calls highlights the need to quickly discover errors in LLM-generated outputs.*\n\nIn experiments across several prominent models, results show meaningful differences in failure modes even when overall tool-benchmark performance looks similar. The analysis highlights that models fine-tuned for function calling tend to reduce hallucinations in function and argument names, while chat-oriented models show higher rates of those errors; smaller models often exhibit repeated-call loops that correlate with incomplete tool usage. The paper reports IAC as the most prevalent issue, attributing it to models answering only part of multi-part queries and to confusion among similar APIs that leads to choosing an insufficient tool variant. Additional findings include argument-value mistakes from ignoring optional controls like sorting and limits, type confusions that reduce correctness, and format-following weaknesses in some reasoning-focused models. Ablation studies indicate that adding irrelevant constraints increases incorrect function name errors, that the feedback mechanism improves success by enabling iterative self-correction, that structured output formats can increase format errors due to token overhead, and that environments with more similar tool names reduce accuracy, especially by driving insufficient or wrong tool selection; the conclusion argues that fine-tuning, feedback, and error-pattern visibility are key to improving tool agents and proposes expanding to environments with multiple families of actions.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2411.13547_paper.pdf"
}