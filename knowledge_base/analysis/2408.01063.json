{
  "paper_id": "2408.01063",
  "title": "Mobile App Review",
  "category": "mini_program_service",
  "year": 2024,
  "timestamp": "2026-03-01T14:03:25.139995",
  "summary": "This manuscript proposes T-FREX, a feature extraction method for mobile app reviews that reframes feature extraction as a named-entity recognition style token classification task using encoder-only Transformer models. It argues that app reviews are especially noisy and subjective, which hurts traditional syntactic pattern matching approaches, and tests the central hypothesis that encoder-only large language models can improve precision and recall in this setting. *We redefine feature extraction as a supervised token classification task.*\n\nThe system design combines two real-world data sources: Google Play reviews and crowdsourced feature lists from AlternativeTo, then transfers those feature annotations into review text by matching feature token sequences within reviews from the same app. It labels tokens with an IOB-like scheme using `B-feature`, `I-feature`, and `O`, producing a ground-truth dataset of 23,816 reviews from 468 apps (10 Google Play categories) with 29,383 feature mentions and 475,382 tokens overall, plus a larger separate corpus for continual domain adaptation. *This process resulted in 29,383 feature annotations over 23,816 app reviews.*\n\nEvaluation is organized around four research questions: baseline fine-tuning, extended pre-training, instance selection, and their combination, with a strong emphasis on recall due to the nature of requirements engineering hairy tasks. Key elements and findings include:\n- **Models and baselines:** Compares BERT, RoBERTa, and XLNet (base and large) for token classification, and also reports feature-level results to compare against SAFE, a syntactic baseline, showing T-FREX improves substantially in-domain at the feature level.\n- **Metrics and weighting:** Reports precision, recall, `f1`, and a recall-weighted `fβ`, where `β` is derived from human timing data for manual vs assisted assessment (reported as 28.29s vs 11.86s, yielding `β = 2.385`).\n- **Baseline results:** Out-of-domain, `XLNetbase` leads on `fβ = 0.499`; in-domain, `XLNetlarge` leads on `fβ = 0.599` and high precision, with `BERTlarge` close due to stronger recall.\n- **Extended pre-training:** Continual pre-training on 622,352 additional reviews (8,232,362 tokens after minimal cleaning) generally improves effectiveness, with the biggest gains often in early epochs and diminishing returns later. *On average, the best performance improvements are achieved after one or two epochs of extended pre-training.*\n- **Instance selection:** Introduces a central density-based instance selection variant for NER that uses BERT embeddings, feature-specific centroids, and distance-based selection to form reduced training partitions (12.5%, 25%, 50%, 75%). It often improves both quality and speed, with training time dominated by the training stage and roughly `×1.8` speedup at 50% data on average.\n- **Best overall configuration and new features:** The best `fβ` comes from `XLNetlarge` with extended pre-training at checkpoint `c = 1`, reaching recall `r = 0.700` and `fβ = 0.677`. Human evaluation of newly predicted features suggests the original ground truth is not exhaustive: baseline new-feature precision is 62.5% yes, while the extended model is 60.8% yes but yields far more new feature candidates in absolute terms (11,120 reviews with new features vs 1,956 in the baseline analysis).\n\nThe discussion emphasizes practical tradeoffs: extended pre-training tends to boost precision (and sometimes recall) but needs careful early stopping, while instance selection can reduce redundancy, improve cost-effectiveness, and sometimes improve correctness as well. It also details threats to validity, especially dependence on AlternativeTo feature coverage, potential dataset and domain bias, limits to generalization beyond Google Play and beyond mobile apps, and the fact that evaluation focuses on correctness and time behavior rather than end-to-end downstream decision-making impact. The paper concludes by positioning T-FREX as a reusable, publicly released set of datasets, code, and model checkpoints, and proposes future work on emerging app domains, other software types, and integration into real review mining pipelines.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}