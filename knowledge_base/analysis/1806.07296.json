{
  "paper_id": "1806.07296",
  "title": "Ecommerce Neural Ranking",
  "category": "ecommerce_evaluation",
  "status": "success",
  "summary": "The paper studies end-to-end neural ranking for eCommerce product search, focusing on ranking catalog items using only query text and SKU textual fields. It argues that product search differs from web search in ways that break common assumptions in neural IR and dataset construction, including stricter notions of relevance, more adversarial near-miss queries, and click behavior heavily influenced by non-relevance factors like price and imagery. To address this, the authors propose constructing training and benchmarking data from click-through logs using task modeling: grouping semantically related queries within a session to infer when a user refined an earlier query that did not match intent into a later query that did.\n\nA core contribution is a dataset-building method based on a task-centric click model perspective, designed to produce hard pairwise ranking triples where the irrelevant item is plausibly relevant to an earlier, similar query but not to the refined one. The construction filters for query refinements where the later query properly contains the earlier query’s tokens, the earlier result list has no clicks, the later list has a click, the unclicked earlier items are from top ranks to increase the chance they were examined, and the clicked SKU for the later query did not appear in the earlier list to reduce leakage. The resulting dataset (from Jet.com logs, April through November 2017, limited to electronics and furniture for iteration speed) is large and temporally split to reduce memorization by holding out queries seen in earlier months from validation and test.\n\n*Our final dataset consists of around 3.6 million examples, with 130k unique q, 131k unique drel, 275k unique dirrel.*\n\nExperiments compare classic baselines and several neural IR model families, emphasizing the distributed versus local-interaction distinction. Key findings reported are:\n- Local-interaction kernel pooling models substantially outperform a tf–idf baseline on this adversarial dataset, reducing pairwise ranking errors by over a third in the best setting.\n- Distributed representation models such as DSSM, CLSM, and a Siamese-style variant overfit and do not beat the baseline on validation and test.\n- Kernel pooling performance depends on truncating SKU text, with a middle truncation length performing best, and freezing embeddings causing only modest degradation versus fine-tuning.\n- Fine-tuning embeddings mainly separates certain confusable term pairs, which the authors argue could still be useful for other downstream search-related tasks even when overall accuracy gains are modest.\n\n*Note that, in all experiments to date on our dataset, none of the distributed models (DSSM, CLSM, Siamese) have outperformed the baseline.*\n\nThe conclusion is that task-aware, adversarial data derived from real eCommerce sessions can sharply differentiate model types: local-interaction models generalize better here, while distributed models need improved regularization and data augmentation to realize their operational advantages for retrieval-plus-ranking via approximate nearest neighbors. The authors outline ongoing work in two directions: exploring architectures that are easier to regularize and expanding or augmenting training data to reduce bias and increase effective diversity under concept drift.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/ecommerce_evaluation/1806.07296_ecommerce_neural_ranking.pdf"
}