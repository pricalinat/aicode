{
  "paper_id": "1911.12481",
  "title": "Product Kg Embedding",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper proposes a product knowledge graph embedding method tailored to e-commerce, where product relations are noisy, sparse, and semantically rich compared to ordinary knowledge graphs. It defines a PKG whose entities include products, words, and category labels, and whose key relations cover complement, co-view, substitute, describe, search, and IsA, aiming to support marketing, advertising, search ranking, recommendation, and knowledge completion. A central argument is that common KG embedding methods (for example translational models) rely on well-established factual triplets and relatively simple relation semantics, assumptions that often break for PKG data derived from customer behavior and text.\n\nThe method learns embeddings end to end from multiple modalities by treating relation discovery as discrete event-sequence prediction, using a self-attention module to focus on the most informative parts of purchase/view/search sequences and product descriptions. Substitute is modeled directly from accepted substitution records with a word2vec-like objective that makes substitutable products close in an embedding space, and this shared space helps connect other relations through a propagation rule that similar or substitutable products tend to share complements, co-views, descriptions, and search terms. For hierarchical category structure, it uses Poincar√© embeddings in hyperbolic space and then predicts IsA labels from product embeddings, and it trains all tasks with a simple weighted task-sampling multitask schedule.\n\n*In this paper, we propose a new product knowledge graph PKG embedding approach for learning the intrinsic product relations as product knowledge for e-commerce.*  \n*The proposed approach compares favourably to baselines in knowledge completion and downstream tasks.*\n\nExperiments use a real-world dataset from grocery.walmart.com with about 140,000 products, about 40 million session records, and accepted substitutions covering about 70,000 products, with preprocessing that leaves about 100,000 products for modeling. Across knowledge completion and classification, the proposed approach outperforms KG-embedding baselines even when those baselines are helped by a constructed product relation graph, reaching HIT@10 and NDCG@10 of 14.53 and 7.67 for complement, 20.84 and 10.26 for co-view, and 34.58 and 14.77 for substitute, plus strong category and department prediction scores. It also improves downstream tasks, reporting better search ranking for encountered and new queries (R@10 30.99 and 22.71, MAP@10 14.46 and 9.53) and better next-impression recommendation than several classic recommendation baselines.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/1911.12481_product_kg_embedding.pdf"
}