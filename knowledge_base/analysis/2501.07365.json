{
  "paper_id": "2501.07365",
  "title": "Multimodal Semantic Retrieval",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper studies how adding product images to dense semantic retrieval changes first-stage product search performance compared with text-only retrieval. It frames the gap in typical lexical matching and motivates dense two-tower retrieval, then argues that images are a central signal in e-commerce but underused in scalable semantic retrieval setups. The authors propose multimodal item representations and evaluate whether multimodality improves either purchase-oriented recall or relevance quality under large-scale approximate nearest neighbor retrieval with cosine similarity.\n\nThe core technical work compares a text-only BiBERT bi-encoder baseline with multimodal architectures built from BiBERT plus a pretrained CLIP model. Two multimodal families are tested: a 4-tower model that uses BiBERT and both CLIP text and CLIP image encoders, and a 3-tower model that drops the CLIP text tower and relies on an MLP fusion module to both fuse signals and reduce embedding-space misalignment between independently trained encoders. Fusion strategies include direct concatenation, alpha-weighted concatenation, and MLP-based fusion, with experiments that vary what components are fine-tuned and whether training batches are enriched with additional hard-ish negatives from non-purchased matched products.\n\nExperiments use 12 months of query-product interactions for training and a non-overlapping one-month slice for evaluation, with 38,268 evaluation queries and a 3,384,067 item index retrieved via FAISS top-100 nearest neighbors. Results show clear tradeoffs: CLIP alone yields strong relevance labeling precision but much lower purchase recall than BiBERT, while multimodal variants can improve recall slightly via simple fusion or improve relevance precision substantially via joint fine-tuning of BiBERT plus fusion, often at the cost of recall. The paper also emphasizes complementarity: multimodal models retrieve many items not retrieved by BiBERT, and those exclusive matches still have high proportions of exact and substitute relevance with relatively low irrelevant rates, suggesting multimodality contributes genuinely new high-quality candidates rather than only reshuffling the same matches.\n\n*Overall, our multimodal models show larger potential on improving relevance accuracy than purchase prediction.*  \n\n*the net predictions are pretty high 60 to 80 exclusive matches per query*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2501.07365_multimodal_semantic_retrieval.pdf"
}