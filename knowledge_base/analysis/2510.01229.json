{
  "paper_id": "2510.01229",
  "title": "Enhancing Rerankers",
  "category": "product_matching",
  "status": "success",
  "summary": "The paper presents a query-less pipeline for adapting transformer cross-encoder rerankers to specialized domains without any human-labeled query document pairs. It uses a large language model to generate synthetic queries from an in-domain corpus and to provide automated supervision that identifies positives and hard negatives, enabling efficient fine-tuning that aims to approach LLM-level reranking quality at much lower inference cost. *we propose a novel method for fine-tuning transformer-based reranking models with-out relying on manually labeled query-document pairs.*\n\nAt a high level, the method builds synthetic training triplets and optimizes a contrastive objective designed for top-ranked candidates:\n- Synthetic query generation: sample a passage from the domain corpus and prompt an LLM with few-shot examples to produce a realistic query for that passage.\n- Candidate mining: run the synthetic query through a bi-encoder retriever to get a top candidate set from the same corpus, then select one positive and multiple hard negatives.\n- LLM-based relevance classification: score each query candidate pair by the LLMâ€™s probability of a relevant label and threshold the score to label positives vs negatives, with the positive defined as the highest-scoring candidate rather than always the source passage.\n- Fine-tuning: train the cross-encoder with a contrastive loss using Localized Contrastive Estimation to emphasize distinguishing relevant documents from hard negatives among top retrieved results.\n\nExperiments use MedQuAD as the in-domain corpus and a subset of MS MARCO to check out-of-domain generalization, fine-tuning `BAAI/bge-reranker-v2-m3` on synthetic datasets from 100 to 1000 entries. Results show clear in-domain gains that mostly arrive early in training and then plateau with more synthetic data, while out-of-domain metrics stay roughly stable with occasional small improvements, suggesting limited catastrophic forgetting under this setup. *The highest nDCG@10 score of 0.952 is achieved using a dataset of 800 entries after the firstepoch oftraining.* The conclusion highlights the practical benefit of removing manual labeling for domain reranker adaptation, and suggests future extensions such as reinforcement learning to improve synthetic data generation, multilingual settings, and using knowledge graphs to guide query generation and reduce hallucinations.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2510.01229_enhancing_rerankers.pdf"
}