{
  "paper_id": "1803.06258",
  "title": "Ecommerce Experiments",
  "category": "ecommerce_evaluation",
  "year": 2018,
  "timestamp": "2026-03-01T14:00:04.436322",
  "summary": "Online controlled experiments like A/B tests are widely used to estimate causal effects of product changes, but the paper argues they become unreliable and underpowered for personalised e-commerce strategies because users must qualify based on behavior that correlates with outcomes. Key problems include broken statistical equivalence between strategy audiences, inability to assign users to strategies upfront due to dynamic qualification, heavy dilution when most users never qualify, and loss of power when restricting analysis to the small overlap of users who qualify for multiple strategies. *Online controlled experiments are the primary tool for measuring the causal impact of product changes in digital businesses.*\n\nTo address this, the authors propose the Stacked Incrementality Test framework. It first randomly splits the population into two top-level partitions, then within each partition randomises qualified users into exposed and control for that strategy, enabling separate incrementality estimates for each strategy and a statistical test on the difference between those incrementalities. The paper derives the test statistic using a Welch-style t framework, develops power and minimum sample-size calculations (including equal-sized groups and fixed exposed-to-control ratios), and provides guidance for a power calculator intended for both planning and post-hoc detectable-effect analysis. Compared to a Restricted Population Test that only includes users who qualify for either strategy, SIT can be less sensitive in pure statistical terms, but the paper derives conditions under which SIT is still better overall because it measures the right incrementality difference without the bias introduced by non-equivalent groups; for conversion-rate-style bounded metrics, it argues the required sample sizes to detect practical differences are often achievable at modern e-commerce scale.\n\nThe final section focuses on pitfalls learned from real ASOS experiments and similar setups, emphasizing that SIT adds new ways to break experiments if details are wrong:\n- Cross-contamination between strategies: exclusion rules or routing can cause one strategy to block or reshape the other strategy audience, breaking intended equivalence even within a strategy split. *We call this effect cross-contamination between strategies as a result of an incorrect test setup.*\n- External event contamination: large real-world events like Black Friday can swamp the experimental intervention, diluting incremental effects across both exposed and control groups even when randomisation is correct.\n- Bad control-group proxies: when exposure cannot be observed symmetrically, naive proxies can systematically include higher-performing users in control, yielding misleading negative incrementalities; the paper describes fixing this by using a measurable placebo exposure and only comparing users who were actually served an ad.\n\nOverall, the paper concludes that SIT is a practical experimental design for comparing personalised strategies when qualification breaks naive A/B assumptions, but it must be paired with careful power planning, monitoring for data quality issues like sample ratio mismatch and distribution drift, and explicit checks that each strategyâ€™s incrementality estimates are sensible before trusting the final strategy comparison.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}