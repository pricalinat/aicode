{
  "paper_id": "2002.11143",
  "title": "Entity Linking Kg",
  "category": "product_matching",
  "year": 2020,
  "timestamp": "2026-03-01T14:44:57.582313",
  "summary": "This paper proposes an end-to-end neural model for entity linking and disambiguation in simple natural-language questions over a knowledge graph, explicitly combining word embeddings with knowledge graph embeddings. The goal is to jointly identify the topic entity and the relation needed to answer a question, while reducing reliance on brittle string-matching post-processing when multiple entities share the same label. *we propose the first end-to-end neural network approach that employs KG as well as word embeddings to perform joint relation and entity classification of simple questions*.\n\nThe approach has three main parts: (1) entity span detection with a BiLSTM that predicts which question tokens belong to the entity mention using I-O style supervision, (2) relation prediction with a self-attention BiLSTM classifier over all relations, and (3) entity candidate selection that scores pre-generated entity candidates using both word-based similarity and KG-based similarity. Candidates are generated by matching tf-idf vectors of the question to entity labels, then re-ranked using heuristics such as label presence, relation connectivity, and Wikipedia mapping signals; relation candidates are generated per entity by collecting 1-hop relations. For entity selection, the model computes a word-based cosine similarity between a question embedding and averaged candidate-label embeddings, and a KG-based similarity by building a relation-specific question embedding from relation logits and relation KG embeddings, then max-pooling cosine similarity against each candidate’s set of 1-hop relation embeddings. A learned gating mechanism estimates whether ambiguity exists in the word-similarity candidate set and, if so, mixes the word- and KG-based similarity signals; during inference the model also enforces that the final predicted entity and one of the top relations form a 1-hop pair in the KG. *the proposed approach achieves a performance comparable to state-of-the-art entity linking while requiring less post-processing*.\n\nExperiments use SimpleQuestions with a Freebase subset and TransE KG embeddings plus GloVe word embeddings, training all submodules jointly with a multi-task objective (span loss, weighted relation loss, entity loss, and a gate loss, plus a soft parameter-sharing regularizer). With larger candidate sets, entity linking improves; the best reported entity-linking accuracy is 78.60% with 300 candidates and 95.49% candidate recall, and the model’s final question-answering accuracy reaches 73.20%, outperforming earlier end-to-end baselines but trailing stronger modular systems. Ablations attribute gains to the gating mechanism, relation-aware KG similarity, and candidate re-ranking, and error analysis highlights failures from span detection (including questions with multiple entities), missing 1-hop connectivity between gold entity and relation, and limited linguistic signals in hard disambiguation; future work suggests stronger span models (e.g., BiLSTM-CRF), newer NLP encoders such as BERT, and more expressive KG embedding methods.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}