{
  "paper_id": "2302.08092",
  "title": "Paper",
  "category": "mini_program_service",
  "year": 2023,
  "timestamp": "2026-03-01T13:48:16.217372",
  "summary": "# Product Question Answering in E-Commerce: A Survey\n\nThis paper presents the first comprehensive survey of Product Question Answering (PQA), an increasingly important research area in e-commerce that aims to automatically answer customer questions about products using user-generated content such as reviews, community QA pairs, and product information. The authors systematically categorize existing PQA research into four problem settings based on the form of provided answers: opinion-based, extraction-based, retrieval-based, and generation-based. Each setting has distinct methodologies, evaluation protocols, strengths, and limitations that the paper analyzes in detail.\n\nThe four problem settings represent increasingly sophisticated approaches to PQA. Opinion-based PQA addresses yes/no questions by aggregating crowd opinions from product reviews, using methods like Mixtures of Opinions (Moqa) and computing aspect-specific embeddings. Extraction-based PQA treats the problem as machine reading comprehension, extracting specific text spans from reviews that answer the question, typically evaluated using Exact Match and F1 scores. Retrieval-based PQA re-ranks documents to select the most appropriate answer, adopting standard ranking metrics like MAP, MRR, and NDCG. Generation-based PQA leverages sequence-to-sequence models to generate natural language answers, requiring both automatic evaluation (ROUGE, BLEU, BERTScore) and human evaluation for fluency and informativeness.\n\nThe paper identifies four key challenges that distinguish PQA from general QA systems. First, subjectivity is pervasive since most product questions seek opinions rather than factual answers, requiring aggregation of diverse user viewpoints. Second, reliability and answerability issues arise because user-generated content contains noise, spam, and unanswerable questions. Third, multi-type resources must be processed, including unstructured reviews, structured product attributes, and community QA pairs. Fourth, low-resource problems occur due to the need for domain-specific annotated data across different product categories.\n\nExisting datasets for PQA research include the Amazon Product Dataset (1.4M questions across 21 categories), AmazonQA (923K questions), SubjQA (10,098 questions across 6 domains), JD (469,955 questions in Chinese), and Taobao (1.15M questions). The evaluation protocols vary by problem setting, with Acc@k being the standard metric for opinion-based PQA, while extraction-based PQA uses Exact Match and F1 scores, and generation-based PQA relies on both automatic metrics and human evaluation.\n\nThe authors highlight several promising future directions: improving question understanding to identify subjectivity and user intent, incorporating personalization by considering user preferences and purchase history, leveraging multi-modal data including product images, and constructing larger high-quality benchmark datasets. They conclude that while PQA is a domain-specific application, it presents unique research challenges and significant commercial value as e-commerce continues to grow.\n\n*product-related questions typically involve consumers' opinion about the products or aspects of products*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}