{
  "paper_id": "2009.08950",
  "title": "Implicit Cf",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper builds an implicit-feedback product recommendation system for a large parts supply company using sparse customer purchase interactions, aiming to generate a ranked list of 12 products per customer. It evaluates four latent-variable collaborative filtering approaches and finds Neural Collaborative Filtering NCF achieves the best ranking quality on the company dataset, with Bayesian Optimization used to tune key hyperparameters. The work positions deep CF as a practical revenue and user-experience lever for the company, while noting extensions like click data and CTR modeling.\n\n*CF with Neural Collaborative Filtering NCF was shown to produce the highest Normalized Discounted Cumulative Gain NDCG performance on the real-world proprietary dataset.*\n\nThe dataset comes from two internal sources: an Invoiced Orders table with about 20 million transactions from August 2016 to June 2019 and an Item Current table with about 500,000 unique products and multilingual descriptions. Data is heavily filtered to the essentials for implicit CF: customer ID, product ID, English and French descriptions, and a purchase-count-based interaction signal used as implicit ratings. The methods compared are:\n- Matrix Factorization with Alternating Least Squares ALS for implicit feedback with confidence weighting\n- Bayesian Personalized Ranking BPR using pairwise triplets of observed vs unobserved items\n- Neural Collaborative Filtering NCF combining GMF and MLP into a NeuMF fusion trained with binary cross-entropy, including a pretraining variant\n- Autoencoder for Collaborative Filtering ACF trained with a logistic reconstruction loss plus dropout and weight decay  \nModels are implemented in TensorFlow and PyTorch, trained on a CPU server, and evaluated with leave-one-out plus negative sampling, using NDCG@12 as the main metric and a business-motivated One-product Hit Ratio.\n\n*The results in the Table 5 demonstrate that the NCF model achieved the best average performance for NDCG@12 over the leave-one-out evaluation test interval.*\n\nResults show average NDCG@12 of 0.577 for ALS, 0.636 for BPR, 0.724 for NCF, and 0.604 for ACF, with One-product Hit Ratio reported as 1 for all models. The discussion attributes BPR gains over ALS to pairwise training, and NCF gains to capturing both linear and non-linear user-item relationships, with pretraining GMF and MLP significantly reducing loss and improving NDCG compared to training NeuMF from scratch. ACF underperforms BPR and NCF here, with the paper arguing that extreme sparsity can cause autoencoders to capture less relevant signal; it also notes a key limitation that NCF latent factors are not interpretable. Future work highlights cold start handling, variational autoencoder CF variants, and adding click data to enable CTR-oriented models such as Deep Interest Network, along with possible hybrid user-based and item-based enhancements for sparse settings.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2009.08950_implicit_cf.pdf"
}