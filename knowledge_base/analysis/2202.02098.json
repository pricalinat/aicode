{
  "paper_id": "2202.02098",
  "title": "Supervised Contrastive",
  "category": "product_matching",
  "year": 2022,
  "timestamp": "2026-03-01T14:44:33.419543",
  "summary": "This poster paper applies supervised contrastive learning to e-commerce product matching, where the goal is to decide whether two product offers from different shops refer to the same real-world product. The authors pre-train a RoBERTa-base Transformer encoder with a supervised contrastive objective and then fine-tune it as a binary pair classifier, arguing that better representation learning before pairwise training improves matching quality. *We show that applying supervised contrastive pre-training in combination with source-aware sampling significantly improves the state-of-the-art performance on several widely used benchmarks.*\n\nA central challenge is that many product matching datasets lack entity-level identifiers for individual offers and only provide labeled matching or non-matching pairs, which can introduce label noise when constructing contrastive labels. The paper builds a correspondence graph from known matching pairs and assigns a label per connected component, then proposes a source-aware sampling strategy that samples batches from per-source datasets to eliminate inter-source label noise (assuming sources are deduplicated internally). Offers are serialized into a single string by concatenating attributes using special tokens like [COL] and [VAL], and the authors optionally add textual augmentations (typos, swaps, deletions, span deletions, synonym substitutions, word splits) with a 10 percent per-word augmentation chance, while also noting dropout acts as a softer augmentation. For fine-tuning, the model encodes both offers and classifies using the combined vector (u, v, |u − v|, u ∗ v) with binary cross-entropy, with experiments on freezing versus unfreezing the encoder during this stage.\n\nExperiments on Abt-Buy, Amazon-Google, and WDC LSPC Computers show consistent gains from supervised contrastive pre-training with the proposed sampling, setting new best F1 scores on the tested benchmarks. Reported test results include Abt-Buy F1 94.29 (about +3.24 over the strongest baseline), Amazon-Google F1 79.28 (about +3.7), and WDC Computers improvements ranging from about +0.84 to +8.84 depending on training size, with the biggest gains in smaller training regimes. The ablations emphasize that source-aware sampling is crucial on datasets without explicit identifiers, with large performance drops when sampling all sources together, and that self-supervised contrastive pre-training (SimCLR-style) hurts performance, attributed to treating true matches as negatives due to unavoidable label noise. *We have demonstrated that supervised contrastive pre-training followed by cross-entropy fine-tuning can generally improve the performance of product matchers compared to only performing cross-entropy fine-tuning.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}