{
  "paper_id": "2404.14415",
  "title": "Domain Adaptation Intent",
  "category": "mini_program_service",
  "year": 2024,
  "timestamp": "2026-03-01T13:54:15.337325",
  "summary": "# Domain Adaptation in Intent Classification Systems: A Survey\n\n## Overview\n\nThis paper presents a comprehensive survey of domain adaptation techniques for intent classification in dialogue systems. Intent classification is a fundamental component of spoken language understanding (SLU) systems, which consist of automatic speech recognition (ASR) to convert speech to text, followed by natural language understanding (NLU) to map textual data to user intent. The formal task definition involves classifying a user utterance into one of N pre-defined intent classes or categorizing it as out-of-scope (OOS) when the request falls outside the system's coverage.\n\n## Datasets Analysis\n\nThe survey systematically analyzes numerous NLU datasets categorized by data type, multilingualism, and domain coverage. Key datasets include ATIS (air travel, 5,871 utterances, 26 intents), BANKING77 (banking domain, 77 intents), CLINIC150 (10 domains, 150 intents), and MASSIVE (the most comprehensive at 51 languages and 18 domains). Two notable non-standard datasets are ORCAS-I (2 million query-URL pairs for informational/navigational/transactional intent) and Search4Code (1 million code search queries). The analysis reveals that most datasets are monolingual English, with MASSIVE being the only major multilingual dataset covering 51 languagesâ€”primarily from Europe and Asia with minimal African language representation.\n\n## Methods for Intent Classification\n\nContemporary approaches fall into three categories. First, fine-tuning pretrained language models (PLMs) such as BERT, JointBERT, ConveRT, and mT5 remains dominant, with techniques including joint intent-slot filling, knowledge distillation, and quantization for compression. Second, prompting methods using GPT-3 have shown effectiveness for data augmentation, though they struggle with semantically similar intents. Third, few-shot and zero-shot methods leverage techniques like dual sentence encoders (USE + ConveRT), contrastive learning, prototypical networks, meta-learning, and adapter-based approaches for adapting to new domains with limited examples.\n\n## Challenges and Limitations\n\nIntent classification faces significant difficulties stemming from multiple factors. Human communication is inherently multimodal (speech, facial expressions, gestures, tone), yet current systems only process text. The uniqueness of each domain necessitates highly customized classifiers that cannot easily transfer across domains. Reasoning ability in pretrained language models remains questionable, as they may simply output co-occurrence patterns rather than true understanding. Dataset limitations compound these issues: most NLU data is English-only, designed for single domains, and contains relatively small sample sizes. Additional challenges include semantic similarity between intents, imbalanced training data, and out-of-vocabulary words due to diverse user expressions.\n\n## Future Directions\n\nThe paper identifies several promising research avenues. Multimodal input processing should incorporate speech, gesture, and emotional cues. Language diversity requires datasets spanning more languages beyond European and Asian locales. Domain diversity needs coverage of broader topics like hobbies, personality, emotions, and occupations rather than simple home assistant tasks. Conversational pretraining objectives align better with dialogue tasks than general language modeling. Adapter-based fine-tuning offers efficient domain adaptation, and contrastive learning shows potential for distinguishing similar intents by pulling semantically related utterances closer while pushing unrelated ones apart.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}