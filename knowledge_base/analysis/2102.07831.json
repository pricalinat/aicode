{
  "paper_id": "2102.07831",
  "title": "Neuralndcg",
  "category": "ecommerce_evaluation",
  "status": "success",
  "summary": "The paper tackles a core mismatch in learning to rank: models are evaluated with rank-based IR metrics like NDCG, but those metrics depend on sorting predicted scores, making their gradients undefined or zero almost everywhere. To close the gap between training objectives and evaluation, the authors propose NeuralNDCG, a differentiable approximation to NDCG built by replacing the non-differentiable sorting operator with NeuralSort, so the metric itself becomes usable as a gradient-based loss. *We introduce NeuralNDCG, a novel smooth approximation of NDCG based on differentiable relaxation of the sorting operator.*\n\nMethod-wise, NeuralNDCG constructs an approximate permutation matrix from predicted scores using NeuralSort, with a temperature parameter τ that trades off approximation accuracy against gradient variance and converges to exact sorting as τ goes to 0. Because the relaxed permutation is row-stochastic but not necessarily column-stochastic, they apply Sinkhorn scaling to make the matrix closer to doubly stochastic, reducing artifacts where quasi-sorted gains can change total mass near NDCG discontinuities. They define two variants: a rank-summing form that quasi-sorts gains and then applies standard discounts, and a transposed form NeuralNDCGT that instead computes per-document expected discounts via the transposed matrix, enabling a document-summing view while still supporting truncation to a cutoff k by zeroing discounts beyond k. They argue this approach avoids ApproxNDCG’s compounded approximations and potential vanishing gradients, and is computationally O(n^2) versus SoftRank’s O(n^3). *By substituting the discontinuous sorting operator with NeuralSort, we obtain a robust, efficient and arbitrarily accurate approximation to NDCG.*\n\nEmpirically, they train a Transformer-based Context-Aware Ranker on Web30K and Istella (graded relevance 0 to 4), padding or subsampling lists to length 240 for training while evaluating on full list lengths, and treating empty queries as NDCG set to 1 to match common tooling behavior. They compare against RMSE, RankNet, LambdaRank, ListNet, ListMLE, and ApproxNDCG, using Adam with learning rate 0.001 for 100 epochs and tuning smoothness parameters, reporting τ = 1 and ApproxNDCG α = 1 as best in their sweep. Across both datasets and cutoffs, both NeuralNDCG variants consistently beat ApproxNDCG on NDCG@5 and NDCG@10; on Web30K they report the strongest results overall among the tested losses, and on Istella they report top performance on NDCG@5 while remaining competitive with state-of-the-art pairwise methods, and they note the approach can be extended to other rank-based metrics such as MAP.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/ecommerce_evaluation/2102.07831_neuralndcg.pdf"
}