{
  "paper_id": "2409.18568",
  "title": "Customer Service Chatbot",
  "category": "mini_program_service",
  "year": 2024,
  "timestamp": "2026-03-01T14:00:37.605833",
  "summary": "This paper proposes a tailored experimental evaluation method for a goal-oriented customer service chatbot built with a pipeline architecture, arguing that fair functional comparison requires evaluating each component separately. It focuses on three core components: Natural Language Understanding (NLU), Dialogue Management (DM), and Natural Language Generation (NLG), and uses automated hyperparameter optimization to identify both strong model choices and practical configurations. *The results show that for the NLU component, BERT excelled in intent detection whereas LSTM was superior for slot filling.*\n\nThe experiments use the MultiWOZ 2.2 dataset, filtered to the restaurant domain, with dialogue acts mapped into simplified categories and user turns prepared for intent detection and slot filling via tokenization and BIO tagging (including special-case handling for ambiguous or synonymous slot values). NLU compares BERT and an LSTM-based design with separate branches for intent and for inform and request slot filling; DM compares DQN versus DDQN agents trained in simulated conversations and scored by reward, turns, and success rate; NLG fine-tunes compact GPT-2-small and DialoGPT-small on serialized action-frame to system-utterance pairs and evaluates with BLEU, METEOR, and ROUGE. Optuna is chosen for hyperparameter optimization due to scalability and features like pruning that can stop weak trials early. *Optuna provides advanced features like pruning to stop unpromising trials early, which can significantly speed up the optimization process.*\n\nKey findings (after optimization) prioritize component-level outcomes and the hyperparameters that mattered most:\n- NLU: BERT achieves higher intent detection accuracy than LSTM, but LSTM produces stronger slot filling precision, recall, and F1 on the limited restaurant dataset; learning rate is the most influential hyperparameter for both, and smaller batch sizes tend to work best.\n- DM: DDQN is reported as the stronger dialogue manager overall, with higher rewards, slightly higher success rates, and fewer turns, consistent with DDQN reducing Q-value overestimation; DQN is most sensitive to learning rate, while DDQN is dominated by the initial epsilon setting.\n- NLG: GPT-2 outperforms DialoGPT on BLEU, METEOR, and ROUGE for translating action frames into responses, despite DialoGPT showing more flexibility across hyperparameter settings; learning rate is highly influential for both, and DialoGPT is also notably sensitive to batch size.\n\nThe discussion consolidates benchmark-style optimal values and practical tradeoffs: BERT is positioned for intent accuracy when compute allows, LSTM for stronger slot extraction (and potentially lighter deployment), DDQN for more efficient goal completion in simulated dialogue, and GPT-2 for higher-scoring response generation on this framing task. The conclusion recommends expanding future work to additional optimization factors (optimizer choice, dropout, data splitting and sampling), stronger modeling techniques (transfer learning, ensembles), and real-world deployment evaluations using customer service metrics such as NPS, CSAT, and CES.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}