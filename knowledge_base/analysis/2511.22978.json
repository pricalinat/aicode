{
  "paper_id": "2511.22978",
  "title": "Shopping Comp",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T13:53:11.756651",
  "summary": "# ShoppingComp: A Benchmark for Evaluating LLM-Powered Shopping Agents\n\n## Overview\n\nThis paper introduces ShoppingComp, a comprehensive benchmark designed to evaluate large language models as shopping assistants across three critical capabilities: precise product retrieval, expert-level report generation, and safety-critical decision making. The benchmark addresses a significant gap in existing e-commerce evaluation frameworks by focusing on open-world products, complex multi-constraint reasoning, and safety-aware recommendations.\n\nThe research comes from ByteDance and evaluates multiple state-of-the-art LLMs including GPT-5.2, Gemini-3-Pro, Deepseek-V3.2, and Grok-4. Results reveal substantial performance limitations: even the best models achieve only 17.76% Answer Match F1 (GPT-5.2) and 15.82% (Gemini-3-Pro), compared to human expert performance of 30.02%. *Results reveal stark limitations of current LLMs: even state-of-the-art models achieve low performance.*\n\n## Dataset Design\n\nShoppingComp comprises 145 instances organized into 558 scenarios, curated through a rigorous human-in-the-loop process involving 35 domain experts contributing over 1,000 person-hours and 15 annotators contributing over 3,000 person-hours. The dataset includes three distinct question types: synthesized user questions generated from real product attributes, expert-authored questions based on authentic consumer needs, and safety-critical questions designed to test hazard recognition.\n\nThe benchmark spans eleven Amazon product categories with diverse difficulty levels ranging from simple queries with 1-3 constraints to very high-complexity scenarios involving up to 15 constraints. Categories include electronics, home appliances, beauty and personal care, health-related products, sports equipment, and pet supplies, with natural bias toward high-value categories requiring complex multi-attribute decisions.\n\n## Evaluation Framework\n\nThe evaluation methodology combines AnswerMatch metrics with fine-grained rubric-based verification across four dimensions. **AnswerMatch F1** measures semantic correspondence between predicted and ground-truth products using standard precision, recall, and F1-score. **Score of Products (SoP)** evaluates what proportion of recommended products satisfy user requirements across N rubrics. **Rationale Validity** assesses whether product recommendation reports provide factually correct and logically sound justifications. **Safety Rubric Pass Rate** measures compliance with safety trap questions requiring hazard recognition.\n\nThe rubric-based verification employs Gemini-2.5-Pro as an LLM-as-a-Judge, achieving 81% agreement at the rubric level and 84% at the question level for product verification, and 94.5% rubric-level agreement for report verification. Cross-judge experiments confirm that while absolute scores vary, relative model rankings remain consistent across different evaluator models.\n\n## Key Findings\n\nThe research uncovers fundamental capability gaps in current shopping agents. **The Human-AI Gap** is substantial: even GPT-5.2 reaches only 17.76% Answer Match F1 versus 30.02% for human experts, and safety pass rates reach only 35.42% versus 77.08% for experts. *This divergence indicates that, rather than enforcing multiple requirements jointly, current models rely on semantic expansion and weak filtering.*\n\n**Retrieval Behavior Differences** reveal that humans exhibit higher precision than recall (38.52% vs 24.58%), employing a constraint-first strategy prioritizing accurate matches over exhaustive coverage. In contrast, all evaluated models show the opposite pattern with recall exceeding precision, suggesting they rely on semantic expansion producing many partially relevant but constraint-violating products.\n\n**Model-Specific Strategies** vary significantly. GPT-5 achieves the highest overall F1 through broad exploration issuing numerous structured queries, while Gemini-3-Pro demonstrates localized advantages in precision (15.33%) and SoP (57.12%) by committing early to narrower candidate sets. DeepResearch systems achieve superior Scenario Coverage through extensive web browsing but exhibit lower Rationale Validity due to evidence aggregation weakening constraint grounding.\n\n**Safety Performance** remains alarmingly low across all systems. Even the strongest models achieve safety pass rates below 40%, with DeepResearch providing only marginal improvement over standard LLMs. GPT-5 and GPT-5.2 show standard deviations exceeding 5%, indicating unstable and non-deterministic safety behavior across runs.\n\n## Error Analysis\n\nThe study identifies four dominant failure modes. **Information Grounding** proves challenging in open-world environments where products exist beyond model parametric knowledgeâ€”ablation shows recall drops to 1-3% without tools. **Multi-Constraint Requirements** expose weaknesses in joint requirement enforcement; models struggle to satisfy multiple interacting constraints simultaneously. **Noisy and Conflicting Evidence** challenges consistent reasoning, as web sources provide contradictory information that models fail to resolve reliably. **Implicit Requirement Inference** emerges as a dominant previously underexplored failure mode where models cannot infer latent product requirements from goal-oriented user queries.\n\n## Implications\n\nThe benchmark demonstrates that current LLMs remain far from human-level reliability in real-world shopping scenarios. The performance gaps across all evaluated dimensions highlight the need for advances in retrieval planning, evidence validation, constraint satisfaction, and safety awareness before shopping agents can be reliably deployed. The authors suggest future work should scale task complexity, extend to multilingual settings, and incorporate personalized evaluation adapting to user profiles and historical behaviors.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}