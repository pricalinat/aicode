{
  "paper_id": "2105.05881",
  "title": "Graphsage Product",
  "category": "product_matching",
  "year": 2021,
  "timestamp": "2026-03-01T14:52:34.659831",
  "summary": "This paper presents a graph neural network approach for product relationship prediction, framing products as nodes and relationships such as co-consideration or association as edges in a graph. It applies an inductive GraphSAGE representation learning pipeline that combines product attributes with network structure to predict whether a relationship link exists, including for unseen products and future-year networks. The motivating case study is the Chinese car market, where the goal is to forecast competitive co-consideration links and provide interpretable guidance for design and product strategy.\n\nMethodologically, the workflow has five parts: construct a binary co-consideration network from customer survey data, learn node embeddings with GraphSAGE via neighborhood sampling and aggregation, form edge embeddings from pairs of node embeddings, train a link classifier (an MLP) for binary link prediction, and interpret attribute importance using permutation-based feature importance. A key addition is an adjacency prediction model to address the practical issue that GraphSAGE needs at least partial neighborhood information: the paper generates an approximate adjacency matrix for a new-year network using similarity-based K-nearest neighbors over node features, then uses that partial structure for link prediction. Evaluation uses confusion matrices and ROC AUC as primary metrics for the link classification task.\n\n*We overcame this issue by developing a method to predict an approximate adjacency matrix using a separate machine learning model, which is referred to as the adjacency prediction model in Fig.1.*\n\nEmpirically, the study uses China customer survey data from 2012 to 2016 with over 40,000 respondents per year; for the main experiments it builds a 2013 network with 388 car models as nodes and uses 29 manually selected car attributes (categorical features one-hot encoded, yielding 210 features after encoding, and continuous features normalized). For held-out link prediction within 2013, it reports an average F1 score of 0.74 with train and test AUC both 0.84; for predicting entirely unseen future networks, it reports F1 scores of 0.65 for both 2014 and 2015 with AUC 0.80 for both years. In comparisons against Exponential Random Graph Models using a reduced six-attribute setting on 296 overlapping cars, the GNN approach achieves higher performance (F1 0.60 and AUC 0.78) than ERGM (F1 0.31 and AUC 0.68), and it notes ERGM parameter estimation may fail to converge as attribute counts grow. The permutation analysis identifies several categorical attributes as most influential for link prediction, especially make, body type and number of doors, and detailed segment, while many continuous attributes such as engine size, price, fuel consumption, and power show low or negative importance under this procedure; the paper also discusses limitations of survey sampling, class imbalance in a sparse network, and known caveats of permutation importance under correlated features, and it outlines future directions including weighted or directed links and more complex heterogeneous customerâ€“product networks.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}