{
  "paper_id": "2408.12779",
  "title": "Llm Ecommerce Applications",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper evaluates how large language models perform on core e-commerce NLP tasks, and whether common industrial strategies like fine-tuning smaller models can match or beat very large LLMs used with in-context learning. The authors instruction-tune Llama-2-7B using LoRA on four public e-commerce datasets, vary training set sizes, and compare against standard baselines (BERT-base for classification and token labeling, T5-base for generation) plus Mixtral 8x22B in zero-shot and few-shot settings. *Few-shot inference with very large LLMs often does not outperform fine-tuning smaller pre-trained models.*\n\nThe study covers four tasks and datasets: ESCI multi-class queryâ€“product classification (Exact, Substitute, Complement, Irrelevant), QueryNER query segmentation with 17 entity types using BIO tags, AMASUM review summarization, and a product description generation task constructed from ESCI product fields (title, brand, color, bullet points) with filtering to remove trivial or missing cases. Full dataset sizes are reported as 1,393,063 train pairs for ESCI (425,762 test), 7,841 train for QueryNER (871 dev, 933 test), 25,203 train for summarization (3,114 dev, 3,166 test), and 431,470 train for description generation (103,865 test). For efficiency, LoRA fine-tuning uses sampled subsets (for example, ESCI at 5k, 10k, 50k train; QueryNER at 1k, 5k, 8k; generation tasks at 1k to 25k), with consistent hyperparameters across runs (3 epochs, 8-bit loading, bf16 parameters, max input length 1500, LoRA alpha 16, dropout 0.05, learning rate 3e-5 with cosine schedule), trained on two NVIDIA A100 80GB GPUs.\n\nResults show that more training data generally improves performance, but the best approach depends on task type and output constraints. For classification and sequence labeling, Llama-2-7B SFT becomes competitive with BERT when given enough data, while Mixtral in-context learning is often weaker; the paper emphasizes that strict output formatting and exact-label matching heavily affect measured scores (for QueryNER, parsing failures can collapse predictions to all O labels). For generation, the fine-tuned LLM consistently outperforms T5 on both review summarization and description generation, and fine-tuned models often beat Mixtral zero-shot; description generation appears more aligned with general text generation, where a large model with in-context examples can be competitive. The authors also test multi-task adaptation strategies: mixed-dataset LoRA training and LoRA weight merging across tasks. Both tend to reduce performance versus training tasks independently, with LoRA merging especially damaging on tasks requiring rigid structured outputs, partly due to formatting mismatches rather than purely semantic errors. *LLMs required a certain volume of training data to reliably produce the correct formats in classification tasks.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2408.12779_llm_ecommerce_applications.pdf"
}