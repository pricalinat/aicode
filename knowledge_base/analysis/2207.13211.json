{
  "paper_id": "2207.13211",
  "title": "Intent Slot Datasets Survey",
  "category": "mini_program_service",
  "status": "success",
  "summary": "This paper surveys publicly available datasets used to train and evaluate two core natural language understanding tasks in task-oriented dialog systems: intent classification and slot-filling. It argues that research often relies on a small handful of benchmarks even though many more datasets now exist, and it aims to improve accessibility by cataloging 40 corpora, summarizing their key characteristics, and discussing each datasetâ€™s strengths, weaknesses, and suitability for different evaluation goals.\n\nIt first frames how intent classification and slot-filling fit into a typical dialog system pipeline (NLU feeding dialog management and downstream business logic), clarifies common utterance types (root requests, follow-ups, out-of-scope queries, multi-intent queries), and reviews dataset creation sources: crowdsourcing, expert authoring or translation, template or grammar generation, real user logs, and derived datasets created via translation or dataset merging. It also summarizes evaluation metrics: accuracy and precision/recall/F1 for intent classification (including micro vs macro averaging), BIO tagging and F1 for slot-filling, and exact match accuracy as a combined metric for joint models.\n\nMost of the paper is an organized tour of datasets grouped by task type. For joint intent + slot datasets, it highlights long-standing benchmarks like ATIS and Snips, notes that many modern models achieve near-ceiling results on them, and discusses critiques that ATIS is shallow, small, and error-prone or ambiguous in places. It then covers multilingual and derived variants (multiple ATIS translations such as Multilingual ATIS, MultiATIS++, PhoATIS; Snips-derived Almawave-SLU), plus datasets with special properties such as TOP and TOPv2 with hierarchical, nested intent-slot representations; spoken counterparts like STOP; broad-coverage crowdsourced sets like HWU-64 and its spoken extension SLURP; code-switching CSTOP; cross-lingual transfer benchmark xSID; the multilingual MTOP benchmark; grammar-generated Leyzer with many intents and extreme imbalance; and NLU++ as a multi-label, slot-rich dataset built from real user utterances in banking and hotels with many multi-intent examples. For intent-only datasets, it covers Chinese SMP-ECDT Task 1, the Outlier Collection designed to compare iterative paraphrase collection strategies, Clinc-150 (notable for many intents and a large out-of-scope set), fine-grained single-domain Banking-77, insurance-domain ACID from real customer interactions, HINT3 datasets with expert-like training data and real deployed test data including out-of-scope, ROSTD as an out-of-domain companion set, and Redwood as a large merged dataset created via automated intent collision detection and additional crowdsourcing. For slot-filling-only datasets, it describes the MIT Movie and MIT Restaurant corpora, the multi-task-motivated Jaech Collection (Airbnb, United, Greyhound, OpenTable), Restaurants-8k with requested-slot context annotations, and large or synthetic food-ordering corpora such as Pizza and the FoodOrdering collection (including evaluation-only sets intended for zero-shot testing).\n\nThe discussion section synthesizes gaps and future directions: non-English coverage is growing but often relies on translation that may miss realistic language phenomena, and non-English intent-only and slot-only resources are comparatively scarce; the landscape is dominated by crowdsourced and synthetic data with relatively fewer real-user corpora; and most datasets still lack dedicated out-of-scope or out-of-domain examples, even though real systems must handle them. It also emphasizes that truly realistic multi-intent data is rare and that artificially concatenated multi-intent sets may not reflect how users actually combine requests. Finally, it argues that near-perfect scores on legacy benchmarks do not imply the problem is solved, citing work on challenge and robustness datasets (paraphrases with lexical constraints, injected noise like ASR errors and disfluencies) that substantially degrade performance, and it calls for richer dataset analysis beyond simple counts, including tools to measure correctness, difficulty, and generalizability.\n\n*We catalog the important characteristics of each dataset, and offer discussion on the applicability, strengths, and weaknesses of each dataset.*\n\n*the goal of this survey is to catalog these intent classification and slot-filling datasets to help facilitate their use in building and evaluating dialog systems and beyond.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2207.13211_Intent_Slot_Datasets_Survey.pdf"
}