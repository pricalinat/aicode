{
  "paper_id": "2309.15646",
  "title": "Cold Warm Net",
  "category": "ecommerce_evaluation",
  "year": 2023,
  "timestamp": "2026-03-01T13:50:57.068639",
  "summary": "# Summary: Cold & Warm Net for Cold-Start Users in Recommender Systems\n\nThis research paper addresses one of the most challenging problems in recommender systems: the user cold-start issue in the matching stage. The authors, from Tencent and Tsinghua University, propose a novel architecture called Cold & Warm Net that dynamically handles both cold-start users (new users with minimal interaction history) and warm-up users as they accumulate behavioral data.\n\n## Problem Context and Approach\n\nThe matching stage in industrial recommender systems retrieves thousands of relevant items from massive candidate pools before ranking occurs. Traditional two-tower models like DSSM and YouTubeDNN rely heavily on user-item interaction history, making them ineffective for cold-start users who lack sufficient behavioral data. Existing solutions using side information or meta-learning face deployment challenges in large-scale industrial settings due to scalability constraints.\n\nThe proposed Cold & Warm Net solves this through a dual-expert architecture: a cold-start expert designed for users with sparse interactions and a warm-up expert for users with accumulated behavioral data. A gate network automatically determines the weighting between these two experts based on user state and login characteristics, enabling dynamic handling of different user types without强制性的策略 (compulsory strategies).\n\n## Technical Architecture\n\nThe model comprises three core components working in concert. First, the user cold & warm embedding layer utilizes attention mechanisms to retrieve prior user group information from pre-trained embeddings, allowing cold-start experts to access collaborative information from active users through shared group representations. Second, dynamic knowledge distillation addresses the underfitting problem of cold-start experts by comparing prediction accuracy between experts and selectively transferring knowledge from the warm-up expert when beneficial—this prevents assimilation of the two experts while ensuring cold-start users receive adequate learning signals.\n\nThird, the bias net explicitly models behavioral differences between cold-start and active users using mutual information to identify features most relevant to user behavior patterns, such as click rate disparities between new and established users. The final prediction combines similarity scores from the original network with bias scores through a sigmoid function.\n\n## Experimental Results\n\nOffline evaluations on MovieLens 1M and Little-World (a Tencent short video platform) demonstrate significant improvements. For cold-start users, Cold & Warm Net achieved a 29.39% improvement in HitRate@50 and 35.22% improvement in NDCG@10 on Little-World compared to the previous best model. Ablation studies confirmed that dynamic knowledge distillation contributes a 5.88-10.72% HitRate@100 improvement for cold-start users, while the bias net adds 3.24-3.68%.\n\nOnline A/B testing on the Little-World platform over one week showed a 3.27% increase in app dwell time and 1.01% improvement in user retention rate for cold-start users, validating the model's effectiveness in real-world deployment.\n\n*By applying dynamic knowledge distillation, it greatly improves the weight of cold-start expert from 0.0410 to 0.3140*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}