{
  "paper_id": "1908.08328",
  "title": "Business Value Recsys",
  "category": "ecommerce_evaluation",
  "status": "success",
  "summary": "This paper reviews how recommender systems create measurable business value in real deployments, arguing that the evidence is real but fragmented and often hard to interpret. It contrasts business-facing evaluation with the academic norm of offline experiments, highlighting two recurring risks: teams may measure the wrong outcome, and small offline accuracy gains may not justify complexity or translate to real impact. The authors survey field tests across domains and show that reported effects range from marginal revenue changes to very large lifts, depending on baseline, context, and what is measured.\n\nIt organizes business measurement into a small set of recurring metric families, illustrated with results from industry and published field studies:\n- **Click-based attention metrics** (CTR and variants) are common and can show large lifts (for example in news and video), but can be confounded by placement effects and by shifting activity from elsewhere on the site.\n- **Adoption and conversion proxies** (take-rate, long CTR, purchase-through, bid-through, add-to-wishlist, clickout) better reflect downstream value but are domain-specific and still may not equal incremental value if users would have acted anyway.\n- **Direct money outcomes** (sales, revenue, profit) are most informative when available; many studies report typical gains around 1 to 5 percent, with occasional larger effects in specific contexts or when replacing weak baselines.\n- **Distributional effects** (long-tail vs blockbuster concentration, catalog exploration, diversity metrics like Gini) show recommenders can reshape what gets consumed or bought, sometimes increasing exploration while reducing aggregate diversity.\n- **Engagement and retention proxies** (session length, repeat visits, activity) can matter most in subscription-style businesses, but the link to long-term retention must be validated.\n\n*Offline experiments were not found to be as highly predictive of A/B test outcomes as we would like.*  \n*This change, which was only at the presentation level, immediately led to an increase in CTR by 100%.*\n\nThe discussion emphasizes practical challenges: short A/B tests can miss longitudinal effects, statistical power and reporting are often insufficient, and optimizing easy metrics like CTR can be misleading or even harmful over time through feedback loops and reduced diversity. It also argues that many academic comparisons focus on fine-grained algorithm variants and small accuracy improvements, yet multiple studies find weak or inconsistent alignment between offline accuracy and online outcomes or user-perceived quality. The paper closes with implications: businesses should align metrics to strategy and validate proxies, while researchers should broaden evaluation beyond accuracy, study UI and explanations more seriously, improve offline evaluation methods for biased logs, and pursue more industry-academia field testing or live evaluation settings.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/ecommerce_evaluation/1908.08328_business_value_recsys.pdf"
}