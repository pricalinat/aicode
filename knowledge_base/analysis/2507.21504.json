{
  "paper_id": "2507.21504",
  "title": "Llm Agent Eval Survey",
  "category": "mini_program_service",
  "year": 2025,
  "timestamp": "2026-03-01T14:01:33.797010",
  "summary": "## Summary\n\nThis survey explains why evaluating LLM based agents is harder than evaluating standalone LLMs or traditional software, then organizes the emerging evaluation landscape into a two dimensional taxonomy: evaluation objectives that define what to evaluate, and evaluation process that defines how to evaluate. It argues that agents operate in dynamic, interactive environments with planning, tool use, memory, and real world side effects, so standard static benchmarks are often insufficient for predicting real deployment behavior. The paper also emphasizes enterprise needs that are underrepresented in research, including role based access control, reliability guarantees, long horizon interactions, and compliance.\n\nOn evaluation objectives, the survey groups work into agent behavior, agent capabilities, reliability, and safety and alignment. Agent behavior focuses on black box outcomes such as task completion and output quality, plus latency and cost, with task completion commonly measured by success rate, pass rate, pass@k, and related variants across benchmarks like SWE bench, BrowserGym, WebArena, AppWorld, and AssistantBench. Agent capabilities cover tool use, planning and reasoning, memory and context retention, and multi agent collaboration, highlighting fine grained metrics such as invocation accuracy, tool selection accuracy, retrieval ranking metrics like MRR and NDCG, parameter F1, execution based validation, sequence metrics like node F1 and edge F1, progress rate, step success rate, and memory consistency or recall scores in long dialogues.\n\nThe reliability section distinguishes consistency from robustness and stresses their importance for high stakes and enterprise settings, where occasional success is not enough. It discusses repeated trial metrics including pass@k and the stricter pass^k from tau bench for capturing consistency requirements, and robustness methods such as prompt perturbations, environmental changes like web page shifts, and induced tool failures to test recovery behaviors. Safety and alignment evaluation spans fairness and explainability, harm and toxicity and bias, and compliance and privacy, describing practices like red teaming with adversarial prompts, toxicity datasets and detectors, and domain specific policy tests where an agent must refuse or constrain behavior correctly under regulations and organizational rules.\n\nOn evaluation process, the paper contrasts static offline evaluation using fixed datasets with dynamic online evaluation using simulations, human interaction, and production monitoring, noting the tradeoffs between cost, realism, and coverage of nuanced behaviors. It surveys evaluation data resources ranging from tool and workflow benchmarks like ToolBench and API Bank to interactive environments like WebArena and AppWorld, plus safety and security benchmarks such as AgentHarm and AgentDojo and leaderboards like BFCL and holistic agent leaderboards. It then reviews metric computation approaches including code based checks, LLM as a judge and agent as a judge methods for scalable qualitative scoring, and human in the loop evaluation as the most reliable but expensive option, followed by tooling platforms that support continuous evaluation and an evaluation driven development mindset with AgentOps style feedback loops.\n\n*This survey provides an in depth overview of the emerging field of LLM agent evaluation.*  \n\n*evaluation is not a one time task but an ongoing process*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}