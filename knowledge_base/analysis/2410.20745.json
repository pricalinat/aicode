{
  "paper_id": "2410.20745",
  "title": "Shoppingmmlu",
  "category": "ecommerce_evaluation",
  "year": 2024,
  "timestamp": "2026-03-01T13:53:24.499019",
  "summary": "# Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for LLMs\n\n## Overview\n\nThis paper introduces Shopping MMLU, a comprehensive multi-task benchmark designed to evaluate large language models on online shopping tasks. The benchmark was developed by researchers at Amazon, HKUST, and the University of Notre Dame, and was presented at NeurIPS 2024. It addresses a critical gap in evaluating LLMs for e-commerce applications by providing a diverse set of tasks derived from real-world Amazon data.\n\n## Benchmark Design\n\nShopping MMLU consists of **57 tasks** and **20,799 questions** covering four major shopping skills:\n\n1. **Concept Understanding** - Tasks involving domain-specific concepts like brands, product models, and attribute values that appear in short texts such as queries\n2. **Knowledge Reasoning** - Tasks requiring implicit knowledge reasoning, including numeric calculations, commonsense understanding, and multi-hop reasoning over product relationships\n3. **User Behavior Alignment** - Tasks modeling heterogeneous user behaviors including queries, clicks, sessions, purchases, reviews, and Q&A interactions\n4. **Multi-lingual Abilities** - Tasks spanning six languages (English, German, Spanish, French, Italian, Japanese) reflecting the global nature of e-commerce\n\nThe benchmark reformulates all tasks as text-to-generation problems to accommodate LLM-based solutions. Evaluation metrics include accuracy for multiple choice, hit rate@3 for retrieval, NDCG for ranking, micro-F1 for named entity recognition, and sentence transformer similarity for generation tasks.\n\n## Key Experimental Findings\n\nThe researchers evaluated over 20 LLMs including proprietary models (ChatGPT, Claude-2, Claude-3 Sonnet) and open-source models (LLaMA2/3, QWen1.5, Mistral, Phi-2, and domain-specific eCeLLMs). Several important insights emerged:\n\n**Model Performance**: Claude-3 Sonnet achieved the best overall performance, though open-source models like LLaMA3-70B-Instruct and QWen1.5-72B performed on par with ChatGPT and Claude-2, demonstrating the potential for building powerful LLM shop assistants with public resources.\n\n**Task Correlations**: Analysis revealed highly positive correlations between pairwise tasks (average Pearson correlation of 0.557) and between skills (minimum correlation of 0.9), indicating substantial shared knowledge across shopping tasks and supporting the viability of unified multi-task solutions.\n\n**General Knowledge Transfer**: Strong correlations between Shopping MMLU skills and general LLM benchmarks suggest that general knowledge transfers effectively to the online shopping domain, and powerful shop assistants should be built upon strong base models.\n\n**Instruction Fine-tuning Effects**: General domain IFT improves performance in most cases, with LLaMA3 benefiting more than LLaMA2 due to better instruction data. However, stronger base models benefit less from IFT, and domain-specific IFT only works on sufficiently strong base models and observed tasksâ€”it fails to generalize to unseen skills.\n\n**Few-shot Learning Challenges**: In-context learning proved generally unhelpful on Shopping MMLU. Adding few-shot examples often decreased performance, and chain-of-thought prompting helped numeric reasoning but showed mixed results for multi-hop reasoning tasks.\n\n## Limitations and Future Work\n\nThe authors note that state-of-the-art proprietary LLMs still lag behind task-specific methods on certain tasks like aspect-based sentiment classification and query-product ranking. This motivates advanced training recipes and data curation for LLMs in online shopping. The benchmark is publicly available at GitHub and hosted a competition in KDD Cup 2024 with over 500 participating teams.\n\n---\n\n*This summary covers the benchmark design, experimental setup, and key findings. The work represents a significant step toward comprehensive evaluation of LLMs as general shop assistants in e-commerce contexts.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}