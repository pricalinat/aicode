{
  "paper_id": "2010.10442",
  "title": "Bert2Dnn",
  "category": "product_matching",
  "status": "success",
  "summary": "The paper introduces BERT2DNN, a relevance modeling framework for latency sensitive e-commerce search that distills a fine-tuned BERT style teacher into a much simpler feed-forward student network using massive unlabeled query and item title pairs from real search logs. It targets a core production gap: editorial relevance labels are costly and scarce, while behavior signals like clicks and purchases are biased and noisy, so direct CTR or CVR based relevance learning can plateau. *The distillation process produces a student model that recovers more than 97% test accuracy of teacher models on new queries, at a serving cost thatâ€™s several magnitude lower.*\n\nMethodologically, the teacher is a fine-tuned Chinese BERT-Base relevance classifier trained on roughly 380k human labeled query item pairs, with optional teacher ensembling to reduce variance and improve label quality via homogeneous stacking of multiple BERT teachers and heterogeneous stacking that adds ERNIE for diversity. The student is a 4 hidden layer DNN trained pointwise on soft labels generated by the teacher over very large transfer sets built from months of search logs, and the paper explores two student variants: a fully connected model that directly mixes query and title embeddings, and a deep-dot two tower model that learns separate query and item representations combined by a dot product. Key data and design choices include CWUB tokenization with a very large unigram and bigram vocabulary around 3 million entries, 64 dimensional token embeddings aggregated into sentence embeddings, temperature based softening of teacher logits, and scaling transfer data from tens of millions up to billions of examples to improve approximation of the teacher.\n\nEmpirically, larger transfer sets consistently improve offline relevance metrics, and with the largest distillation set the student comes close to the teacher while offering drastic speedups that make online deployment feasible. Online A/B tests over about 2 weeks on JD.com across sales sort, price sort, and default sort show positive movement in UV-value and modest gains in unique user conversion rate, with click-through changes small. The approach is also evaluated on sentiment classification using GLUE SST-2, where unlabeled data comes from Amazon movie reviews processed into tens of millions of sentences, and the distilled students retain most of teacher accuracy while running far faster than Transformer students; the analysis section further shows student score distributions track teacher scores closely, and embedding visualizations suggest the deep-dot variant better separates semantically unrelated queries. *We manage to reduce inference time by several orders of magnitude, from the previous best record of 20s held by Tiny-BERT down to 1.2s.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2010.10442_bert2dnn.pdf"
}