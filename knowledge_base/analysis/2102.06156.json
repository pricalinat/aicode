{
  "paper_id": "2102.06156",
  "title": "Personalized Embedding Ebay",
  "category": "product_matching",
  "year": 2021,
  "timestamp": "2026-03-01T14:38:53.241900",
  "summary": "This paper describes a personalized recommendation system for the eBay marketplace that learns user and item embeddings in a shared vector space using a two-tower deep learning architecture, with a strong emphasis on handling extreme sparsity and cold start at large scale. Items are embedded purely from content features such as title text, category, and structured aspects, avoiding behavior-derived item features that are unavailable or unstable for new and fast-changing listings. Users are embedded from implicit onsite behavior, extending beyond item views to include search queries modeled as pseudoitems, so recommendations can reflect both short-term shopping missions and broader intent signals.\n\nKey technical choices include: tokenizing title and aspect text and encoding them with a CBOW-style representation, concatenating feature embeddings and passing them through an MLP, then unit-normalizing item vectors. User events are encoded as concatenations of the associated item or pseudoitem embedding plus an event-type embedding; the paper compares a simple bag-of-events user representation to a GRU-based recurrent sequence model, reporting better offline Recall@20 with the recurrent approach and additional gains from adding search events. The retrieval score uses a dot product affinity scaled by a temperature parameter, and tuning this temperature is presented as a major lever for Recall improvements.\n\nThe authors build training data from logged interactions with recommendation modules on listing pages, treating clicked recommended items as positives and unclicked as negatives, with 30 days of user history features and safeguards against user overrepresentation. They find that negatives drawn only from impressed but unclicked items can lead to overfitting because candidates are too similar, and instead rely on in-batch random negative sampling. For production robustness, they analyze the impact of missing recent user history at prediction time and train with recent-history dropout to reduce performance cliffs. For serving, they use FAISS KNN over item embeddings and add a clustering-based retrieval scheme to improve diversity by allocating results across multiple nearest K-means clusters, then describe a daily batch pipeline using Spark ETL, GPU embedding generation, offline KNN, and a low-latency cache for JVM-based online serving, with optional downstream learning-to-rank.\n\n*Our approach was able to improve the Recall@k metric over the Recently-Viewed-Item RVI method.*  \n*Initial A/B test results show that compared to the current personalized recommendation module in production, the proposed method increases the surface rate by âˆ¼6%.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}