{
  "paper_id": "2601.09306",
  "title": "Paper",
  "category": "mini_program_service",
  "status": "success",
  "summary": "This paper targets a practical gap: large language models can improve sequential recommendation, but their memory and compute costs make fully on-device deployment difficult in settings that demand low latency, privacy, and robustness to poor connectivity. It introduces OD-LLM, described as the first task-adaptive compression framework specifically designed to deploy LLMs for sequential recommendation on resource-constrained devices, aiming to reduce model size without degrading recommendation quality. *no performance loss when the modelsize is halved.*\n\nOD-LLM combines three tightly coupled steps to make low-rank compression work reliably for recommendation signals that are sensitive to subtle temporal and behavioral patterns:\n- Token Covariance Normalization: a preprocessing step that uses a covariance transform (with Cholesky decomposition) to decorrelate and scale token embeddings, improving numerical stability and making SVD truncation decisions less distorted by embedding scale variance and correlations.\n- Low-rank structural compression via SVD: applies SVD to normalized weights, truncates smaller singular values to meet a chosen compression ratio, and reconstructs compressed weights with an explicit link between truncated singular values and compression loss.\n- Progressive alignment via layer-wise weight updates: iteratively refines each layer after compression to reduce activation mismatch between the original and compressed model, updating left singular components while keeping the low-rank structure fixed to limit accuracy loss at higher compression. *OD-LLM is fast in both compression and inference, offering competitive quality with substantially better runtime efficiency.*\n\nExperiments are built on an LLM-based recommendation setup using a LLaMA-7B model fine-tuned with LC-Rec, evaluated on three Amazon review sequential recommendation datasets: Instruments (24,773 users, 9,923 items, 206,153 interactions), Games (50,547 users, 16,860 items, 452,989 interactions), and Arts (45,142 users, 20,957 items, 390,832 interactions), all with very high sparsity and short average sequence lengths around 8 to 9 interactions. With a 0.5 compression ratio, OD-LLM matches or surpasses the uncompressed LC-Rec across Hit Ratio and NDCG at top-5 and top-10, and ablations show normalization is the most critical component while progressive updates add further gains. Against post-training baselines GPTQ (4-bit quantization) and SparseGPT (unstructured pruning), OD-LLM is reported to be much faster in inference; one batch timing is 5 seconds on GPU and 200 seconds on CPU versus 17 and 700 for GPTQ, and 12 and 620 for SparseGPT, while calibration set size shows diminishing returns beyond moderate budgets.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2601.09306_paper.pdf"
}