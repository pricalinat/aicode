{
  "paper_id": "2308.00721",
  "title": "Deep Active Deduplication",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper tackles semantic data deduplication in structured records, arguing that string similarity alone misses cases where literal similarity and true duplication diverge. It proposes PDDM-AL, a pre-trained Transformer based deduplication model trained with active learning so that humans label only the most informative record pairs, reducing labeling cost while improving duplicate identification performance. The approach frames deduplication as a sequence to classification task over serialized attribute text and reports substantial recall gains on benchmark entity resolution datasets.\n\nPDDM-AL combines several pieces into an end-to-end loop: a blocking strategy generates candidate record pairs, records are serialized with attribute markers like [COL] and [VAL], and domain knowledge is injected by marking important fragments (using an open source NER model plus regular expressions) with tags such as [LAST] and [/LAST]. To fit model input limits, it summarizes long serialized strings by keeping higher TF-IDF tokens, then fine-tunes BERT with a classification head while using R-Drop as a regularization style augmentation that aims to improve robustness on dirty data without altering semantics. The active learning selector scores unlabeled pairs by uncertainty (probability near 0.5) and iteratively sends the least confident samples for expert labeling.\n\n*PDDM-AL generally outperforms other methods in terms of Precision, Recall and F1 scores.*\n\nResults are reported on four datasets (Musicbrainz-20-A01, GeographicSettlements, Education, and an enterprise personnel dataset) against SentenceBERT, Dedupe, and field-based similarity, using a 6:2:2 train validation test split and metrics Precision, Recall, and F1. In the main comparison table, PDDM-AL achieves very high scores across datasets (for example F1 of 0.9747 on Musicbrainz-20-A01, 0.9610 on GeographicSettlements, 0.9392 on Education, and 0.9979 on enterprise personnel) and the abstract claims up to a 28 percent recall improvement on benchmarks. Active learning analysis shows uncertainty sampling beating random selection at the same labeling budget, with iterative gains especially early (for GeographicSettlements, adding 1000 pairs between rounds yields large jumps in F1 and recall), and the paper concludes by suggesting future work on alternative selection strategies and model combinations. *the selection strategy based on uncertainty is obviously better than random selection.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2308.00721_deep_active_deduplication.pdf"
}