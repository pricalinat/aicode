{
  "paper_id": "2302.04473",
  "title": "Multimodal Recommender Survey",
  "category": "ecommerce_evaluation",
  "year": 2023,
  "timestamp": "2026-03-01T13:50:54.178804",
  "summary": "This survey paper provides a comprehensive review of multimodal recommender systems (MMRec), which leverage multiple data modalities such as text, images, audio, and video to improve recommendation accuracy. The authors explain that traditional recommendation systems suffer from data sparsity and cold-start problems, which multimodal information can help alleviate by providing richer item representations and capturing hidden relations between different modalities that implicit interactions alone cannot reveal.\n\nThe paper presents a clear pipeline for multimodal recommendation consisting of feature extraction, modality fusion, and optimization stages. For feature extraction, visual content typically uses CNN-based models like ResNet, VGG, or Inception, while textual content employs pre-trained language models such as BERT, Sentence-transformers, or Word2Vec. The authors note that most state-of-the-art models use pre-extracted features rather than end-to-end learning due to computational efficiency.\n\nThe authors classify MMRec models into several categories based on their underlying techniques: Matrix Factorization-based models like VBPR that concatenate visual embeddings with ID embeddings; Multilayer Perceptron approaches like JRL; Convolutional Neural Network methods such as ConvMF and DeepCoNN; Attention-based networks including ACF, MAML, and DMRL; RNN models like VECF; Autoencoder techniques including CKE and MVGAE; and Graph Neural Networks, which represent the majority of recent research. GNN-based models are further divided into direct fusion, heterogeneous graph fusion, homogeneous graph fusion, sampled subgraph fusion, and graph structure refinement approaches.\n\nRegarding modality fusion, the paper identifies three main strategies: early fusion (concatenating embeddings before the model), intermediate fusion (fusing after high-dimensional embeddings), and late fusion (combining prediction scores from each modality). The authors provide mathematical formulations for element-wise sum, concatenation, and attention-based fusion methods, noting that attention mechanisms help capture the varying importance of different modalities.\n\nThe experimental section evaluates multiple models on Amazon datasets (Baby, Sports, Electronics, Food) using Recall@K and NDCG@K metrics. Key findings include that content-aware methods generally outperform pure collaborative filtering approaches, though some early models like MMGCN perform worse than LightGCN due to noise from improperly integrated multimodal features. FREEDOM achieves best performance on smaller datasets, while BM3 and LATTICE demonstrate stable performance across different dataset sizes. The authors emphasize that data splitting strategies significantly impact resultsâ€”random split, user time split, and global time split can produce different model rankings, making cross-study comparisons problematic.\n\nThe paper addresses a critical challenge: simply fusing modalities does not guarantee improvement. Experiments show that single modality (especially textual) often matches or exceeds multimodal performance, indicating that fusion methods frequently destroy modality-specific information. This suggests future research should focus on fusion techniques that preserve complementary information unique to each modality.\n\nSix future research directions are identified: developing effective modality fusion methods that capture complementary information while preserving modality-specific properties; standardizing data splitting strategies and datasets for reproducible research; expanding evaluation metrics beyond accuracy to include diversity, serendipity, and fairness; bridging the gap between academic research and industrial application regarding scalability; incorporating multimodal information into sequential recommendation systems; and leveraging multimodal content for cross-domain recommendation through universal representations.\n\nThe authors also provide an open-source framework called MMRec on GitHub containing implemented state-of-the-art models with pre-extracted multimodal features, enabling researchers to easily run baselines or develop new models. The survey covers approximately 80 papers from top conferences including WWW, SIGIR, KDD, AAAI, and RecSys.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}