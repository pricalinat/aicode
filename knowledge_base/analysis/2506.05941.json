{
  "paper_id": "2506.05941",
  "title": "Ml Retail Forecasting",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper evaluates modern forecasting approaches for long-horizon brick-and-mortar retail demand, using a high-resolution real-world dataset from a major South-East Europe retailer and targeting 365-day-ahead daily sales forecasts for a hygiene category. It argues that physical retail data is unusually difficult due to intermittent demand, heavy missingness, and frequent product turnover, and shows that model choice and preprocessing decisions strongly determine both accuracy and practicality.\n\n*Accurate forecasting presents a competitive advantage for companies.*  \n*Our results show that localized modeling strategies especially those using tree-based models on individual groups with nonimputed data, consistently deliver superior forecasting accuracy and computational efficiency.*\n\nKey setup and data facts:\n- Data is at daily product-store level with hierarchy from product groups to units-of-need, plus store zones that drive pricing structure.\n- Demand is predominantly irregular: about 70.06 percent of series are intermittent, with lumpy and erratic series making up most of the remainder; smooth series are rare.\n- Training vs validation statistics highlight the operational reality: 70,201 training series with about 50 percent average missingness and 0.63 average coverage ratio; many products appear or disappear across the split, reflecting assortment churn and censorship of series length.\n- Preprocessing includes competitive and macro variables such as competitor counts within 1 km, competitor pricing for comparable items, and national statistical indicators like CPI and salaries; prices are converted to real terms using CPI. Missing values are handled with basic fills and also a separate deep imputation experiment using SAITS via PyPOTS. Feature engineering adds lag and rolling-window statistics, and feature selection uses Boruta with LightGBM; categorical variables use CatBoostEncoder.\n- Models compared include tree ensembles LightGBM and XGBoost, neural architectures N-BEATS, NHiTS, and Temporal Fusion Transformer, plus a naive mean baseline. Neural models are implemented with Darts, trained under four regimes combining localized vs whole-category training and raw vs imputed data, with hyperparameters tuned via HEBO on sampled subsets.\n\nResults emphasize that boosted trees, especially in localized group-level training on nonimputed data, deliver the most consistent accuracy and the best compute footprint for this setting. On business-relevant metrics, the paper highlights group revenue and group profit WMAPE: for example, in the individual-group raw setting LightGBM reaches group revenue WMAPE 0.069, while XGBoost is competitive on group profit WMAPE 0.096; neural models improve notably when trained on imputed data, and one standout is N-BEATS achieving best group profit WMAPE 0.040 in the imputed individual-group setting, but overall ensembles remain the most reliable across metrics and settings. The study also reports bias behavior: many models show slight negative demand bias at the series level, yet aggregate forecasts can look mildly positively biased because errors differ by price tier, with underestimation on higher-priced items and overestimation on lower-priced items; it notes government price interventions as a possible additional source of systematic shifts. Training time comparisons reinforce deployment implications: XGBoost and LightGBM train in minutes (totals), while neural models are far more resource-intensive per epoch, especially with imputed data, and TFT is the most expensive. The discussion concludes that brick-and-mortar forecasting benefits most from localized modeling with robust tree ensembles, while neural approaches remain sensitive to extensive missingness and noise even with sophisticated imputation, and it proposes future work on hybrid methods and improved preprocessing to narrow the gap.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2506.05941_ml_retail_forecasting.pdf"
}