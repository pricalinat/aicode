{
  "paper_id": "2506.00210",
  "title": "Reic Intent Classification",
  "category": "product_matching",
  "status": "success",
  "summary": "REIC RAG-Enhanced Intent Classification presents a retrieval-augmented generation approach for large-scale customer service intent routing where intent sets grow over time and taxonomies differ across business verticals. The core idea is to avoid frequent retraining by retrieving the most similar labeled examples for a new query, then using a fine-tuned LLM to score only the retrieved intent candidates via constrained probability calculation, which both improves scalability and prevents generating intents outside the allowed candidate list. The paper motivates the need with heterogeneous, hierarchical intent structures across two verticals: third-party retail and consumer-account issues versus first-party digital and device troubleshooting, where a single flattened ontology would be extremely large.\n\nREIC is described as three components: dense index construction over historical (query, intent) pairs using a sentence-embedding encoder; approximate nearest-neighbor retrieval of top-k similar examples by cosine similarity; and intent probability computation that re-ranks candidates using a fine-tuned Mistral-7B-Instruct model with constrained decoding so only candidates are scored and selected by highest probability. Experiments use an anonymized real-world dataset of 52,499 training samples with 35,041 first-party queries and 17,458 third-party queries, plus test sets of 3,647 and 1,717 queries respectively, and compare against RoBERTa multi-head fine-tuning, a Mistral sequence-classification baseline, and Claude 3.5 Sonnet in zero-shot, few-shot, and RAG prompting variants. Key findings include:\n- Overall intent F1 in Table 1: REIC 0.572 versus RoBERTa 0.516 and Claude plus RAG 0.419, with REIC best in both verticals for the in-domain setting\n- Retriever impact in Table 2 within REIC: MPNet performs best overall (0.564), BM25 is competitive (0.532), and Contriever is lowest (0.457), emphasizing that retriever choice strongly affects final accuracy\n- Top-k trade-off: increasing k improves accuracy up to a point but raises latency sharply, and the paper selects k=10 as a practical balance for their setting\n\nFor robustness, the paper evaluates an out-of-domain scenario by training only on the third-party vertical and testing on the first-party vertical with many more unique intent combinations; in Table 3, RAG-based methods generalize better than pure prompting, and REIC remains competitive (though it drops to 0.283 on first-party in this setting while staying strong in-domain). The authors also report internal online deployment results: consolidating from two separate fine-tuned models to one REIC system reduced misclassifications and improved routing, yielding a 3.38 percent absolute lift in customer positive confirmation responses. Limitations center on dependency on retrieval quality since the model cannot recover if the correct intent is not retrieved, and on the fixed candidate-pool size that creates an accuracy-latency trade-off, with future work suggested around adaptive retrieval and confidence-based dynamic candidate sizing.\n\n*This approach enables dynamic updates to the intent space by simply adding new (query, intent) pairs to the index, leveraging the in-context learning capabilities of the LLM without requiring model retraining.*\n\n*Our REIC leads to a 3.38% absolute improvement for customer positive response rate.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2506.00210_reic_intent_classification.pdf"
}