{
  "paper_id": "2412.16516",
  "title": "Hammerbench",
  "category": "mini_program_service",
  "year": 2024,
  "timestamp": "2026-03-01T13:55:01.371393",
  "summary": "# HammerBench: Fine-Grained Function-Calling Evaluation in Real Mobile Assistant Scenarios\n\n## Overview\n\nHammerBench is a comprehensive benchmark framework designed to evaluate large language models' function-calling capabilities in real-world, multi-turn dialogue scenarios typical of mobile assistants. The framework addresses a critical gap in existing evaluation methods by focusing on the complexities of actual user interactions, where instructions may be imperfect, user intents may shift, and information may be conveyed indirectly through pronouns or external context.\n\nThe benchmark was constructed using a systematic four-stage process involving toolset collection, API generation, validation, and manual refinement. The underlying dataset derives from popular mobile app functionalities sourced from major app stores, complemented by queries generated from anonymized user logs. This approach ensures authenticity while capturing both common and rare user interaction patterns that would otherwise be difficult to obtain given the long-tail distribution of real-world queries.\n\n## Key Design Principles\n\nThe framework is built around three fundamental principles that address limitations in prior benchmarks. First, authenticity requires that test data reflect genuine user behavior rather than idealized scenarios, which the authors achieved by analyzing actual user logs and supplementing rare but impactful edge cases. Second, diversity ensures coverage across multiple domains—from ticket purchasing to daily scheduling—encompassing varied query types from simple requests to complex multi-turn interactions. Third, granularity enables detailed assessment through fine-grained metrics that can identify specific defects in model behavior, such as parameter hallucinations or failure to track evolving user intents.\n\nThe benchmark distinguishes itself by supporting evaluation across four critical scenario categories: imperfect instructions where users provide incomplete information; diverse question-answer trajectories reflecting varying conversational flows; intent and argument shifts where users change their goals or modify slot values mid-conversation; and external individual information where users reference context through pronouns rather than explicit values.\n\n## Methodology and Metrics\n\nHammerBench introduces a novel approach called Function Calling Snapshots, where models generate formatted function calls at each conversation turn regardless of whether context is complete. This contrasts with the traditional Learning to Ask paradigm and demonstrated a 16% improvement in task success rate during comparative experiments. The snapshot-based evaluation enables detailed analysis of missing parameters, hallucinations, and progress tracking throughout multi-turn interactions.\n\nThe evaluation metrics include Function Name Accuracy for assessing correct function identification; Parameter Hallucination Rate and Parameter Missing Rate for tracking incorrect or absent parameter predictions correct; Progress Rate measuring calls up to the point of error; and Success Rate evaluating overall accuracy across all conversation turns. These metrics collectively provide comprehensive insight into both individual function calls and the broader conversation trajectory.\n\n## Experimental Findings\n\nThe evaluation covered ten leading LLMs including GPT-4o, Claude 3.5 Sonnet, various Qwen and Llama variants, and specialized function-calling models like xLAM-7b-fc-r and ToolACE-8B. The results revealed several critical insights about LLM performance in mobile assistant contexts. Models demonstrated significant challenges in handling argument shifts, particularly when users modify slot values multiple times before or after function execution. The Slot Overriding task effectively exposed this weakness, showing that many open-source models struggle to track dynamically changing parameters.\n\nExternal information handling proved particularly problematic, with pronouns and contextual references causing substantial accuracy degradation. The benchmark found that parameter hallucination rates are significantly higher in early conversation snapshots where context remains incomplete compared to final snapshots with full information. This indicates that incomplete context creates fertile ground for hallucinations, representing a persistent bottleneck especially for open-source models.\n\nThe experiments also demonstrated that different parameter name error types constitute a major failure source across interaction scenarios, highlighting critical areas requiring improvement in LLM robustness for practical mobile assistant deployment. The fine-grained metrics proved effective at identifying specific model issues that overall accuracy scores would obscure, providing actionable insights for model optimization.\n\nThe benchmark comprises 60 functional categories, 1,063 tools, and over 6,500 queries distributed across single-turn and multi-turn scenarios. Quality assurance procedures achieved a data availability rate exceeding 95% through LLM validation modules and manual review processes. The code and datasets are publicly available to support continued research in this domain.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}