{
  "paper_id": "2308.15272",
  "title": "Autodroid",
  "category": "mini_program_service",
  "year": 2023,
  "timestamp": "2026-03-01T14:00:30.438582",
  "summary": "AutoDroid presents an end to end Android task automation system that uses large language models to plan and execute multi step GUI interactions for arbitrary tasks on arbitrary apps, without requiring developer integration, user demonstrations, or hand built workflows. The core idea is to combine an LLMs general commonsense reasoning with app specific, automatically discovered knowledge from dynamic analysis, addressing three recurring problems for mobile agents: representing GUI state in a model friendly way, integrating domain knowledge about each app, and reducing the high cost and latency of repeated LLM queries. *AutoDroid is able to precisely generate actions with an accuracy of 90.9%, and complete tasks with a success rate of 71.3%.*\n\nThe system has an offline stage that explores apps via random interaction to build a UI Transition Graph, then synthesizes app memory by asking an LLM to summarize UI element and screen functionality, producing simulated task entries that link a functionality description to the UI states and action traces that reach it. In the online stage, AutoDroid converts the current GUI into a simplified HTML style representation using a small set of tags for interactive elements, constrains the LLM output to a strict action schema, and augments the prompt with the most relevant app memory by retrieving similar simulated tasks using an embedding model and cosine similarity, including hints such as an onclick style description of what a UI element leads to. It also adds practical safety layers: a privacy filter that masks PII in prompts sent to cloud models, and a risky action detector that can require user confirmation for potentially irreversible actions.\n\nEvaluation centers on DroidTask, a new benchmark with 158 manually labeled tasks across 13 common open source apps, including reproducible environments and exploration memory to support methods that rely on offline app interaction. Reported results show sizable gains over multiple baselines and analyze where errors come from, including alternative valid task paths not captured by a single human trace and occasional mistakes judging task completion from UI cues. Key reported findings include:\n- GPT 4 with AutoDroid: 90.9% action accuracy and 71.3% task completion rate on unseen tasks, outperforming GPT 4 prompt only baselines by roughly 39.7% in completion\n- Query efficiency: average prompt length reduced from about 625 tokens to 339 tokens via pruning and merging functionally equivalent UI elements, plus UI merging via automatic scrolling\n- Cost and latency: average query cost reduced and Vicuna 7B inference latency reduced by about 21.3% on sampled prompts; overall LLM calls reduced via shortcut execution when a simulated task matches strongly\n- Security and privacy: risky action detection reaches 75.0% precision and 80.5% recall on selected apps, with moderate accuracy and completion drops when privacy replacement and confirmation prompts are enabled\n\nThe paper concludes that pairing LLM guidance with automatically mined app specific memory enables more reliable multi step automation than prompt engineering alone, while multi granularity optimizations help make inference costs more practical. It discusses remaining constraints such as randomness versus determinism in model decoding, persistent latency as a usability bottleneck, and future directions like calling a large model once to produce a guideline that smaller models ground to UI elements, plus caching common instructions. *We present an LLM-powered mobile task automation system that can support arbitrary tasks without manual efforts.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}