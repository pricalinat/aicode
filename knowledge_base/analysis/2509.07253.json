{
  "paper_id": "2509.07253",
  "title": "Ir Benchmark",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T13:53:40.064070",
  "summary": "# Benchmarking Information Retrieval Models on Complex Retrieval Tasks\n\n## Summary\n\nThis paper presents CRUMB (Complex Retrieval Unified Multi-task Benchmark), a comprehensive evaluation framework for assessing how modern neural retrieval models perform on complex information retrieval tasks. The authors address a critical gap in the field: while large language models have achieved remarkable capabilities, retrieval models have not seen similar advances, particularly when dealing with queries that contain multiple parts, constraints, or requirements expressed in natural language.\n\nThe benchmark comprises eight diverse retrieval tasks: tip-of-the-tongue queries for movie retrieval, multi-aspect queries for scientific paper retrieval, set-based logical queries for entity retrieval, state-specific legal questions, multi-constraint math problems, varied StackExchange questions, clinical trial search using patient histories, and code retrieval. Each task features unique characteristics including different vocabularies between queries and documents, highly technical terminology, and numerical comparisons that challenge existing retrieval models.\n\nThe authors evaluated twelve retrieval models including BM25, Snowflake Arctic Embed, GTE Qwen models, Lion models, and Promptriever. The results reveal significant challenges: even the best-performing model achieved only 0.346 average nDCG@10 and 0.587 R@100 across all tasks. This performance is substantially lower than what these same models achieve on traditional benchmarks like TREC DL, indicating that current retrieval systems struggle significantly with complex queries.\n\nKey findings include that model performance is most impacted by the ability to follow instructions, model size, diversity of training data, and the base model architecture. Interestingly, experiments with LLM-based query rewriting showed counterintuitive results: rewriting techniques tended to harm stronger models while bringing notable improvements to weaker models. The best model, GTE Qwen 7B, experienced decreased performance across all metrics with all rewriting techniques, suggesting that strong retrievers already capture semantic intent effectively and additional augmentation introduces noise.\n\nThe analysis also revealed that sparse retrieval models (Lion SB) generally outperformed their dense counterparts (Lion DS) across most datasets, though dense models showed advantages in domains with highly specialized terminology like clinical trials and paper retrieval. Tasks with set-based operations (SetOps) and tip-of-the-tongue queries proved particularly challenging, with even the best models achieving below-average performance.\n\nThe authors conclude that significant work remains to improve retrieval quality on complex search tasks. They hope CRUMB will set a new standard for complex retrieval evaluation and spur innovation in next-generation retrieval models. All data and artifacts are available at https://github.com/jfkback/crumb.\n\n*Even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}