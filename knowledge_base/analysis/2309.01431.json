{
  "paper_id": "2309.01431",
  "title": "Paper",
  "category": "mini_program_service",
  "year": 2023,
  "timestamp": "2026-03-01T13:51:28.880077",
  "summary": "# Benchmarking Large Language Models in Retrieval-Augmented Generation\n\nThis paper presents a comprehensive evaluation of Retrieval-Augmented Generation (RAG) capabilities across six major large language models. The researchers created a new benchmark called RGB (Retrieval-Augmented Generation Benchmark) in both English and Chinese to systematically assess four fundamental abilities required for effective RAG: noise robustness, negative rejection, information integration, and counterfactual robustness. The benchmark uses the latest news articles and external documents retrieved from search engines to ensure that the internal knowledge of LLMs does not introduce bias into the evaluation results. By evaluating models including ChatGPT, ChatGLM-6B, ChatGLM2-6B, Vicuna-7B, Qwen-7B-Chat, and BELLE-7B, the study reveals significant limitations in current LLMs when applying RAG in practical scenarios.\n\n## The Four Core Abilities Evaluated\n\nThe benchmark addresses four critical capabilities that determine how well LLMs can utilize retrieved information. Noise robustness measures the ability to extract useful information from documents that are relevant to a question but do not contain the answer—for example, when external documents about the 2021 Nobel Prize in Literature are retrieved for a question about the 2022 prize. Negative rejection evaluates whether LLMs can decline to answer when no relevant information exists in the retrieved documents, a crucial safety feature for preventing hallucinations. Information integration assesses the capacity to combine information from multiple documents to answer complex questions requiring multi-aspect responses. Counterfactual robustness tests whether models can identify and correct factual errors in retrieved documents when explicitly warned about potential risks through instructions.\n\n## Key Findings and Limitations\n\nThe experimental results expose substantial challenges in current RAG implementations. For noise robustness, while LLMs demonstrate reasonable performance at lower noise ratios, accuracy drops significantly when noise exceeds 80%—ChatGPT's performance decreases from 96.33% to 76.00%, while ChatGLM2-6B drops from 91.33% to 57.33%. The most alarming finding is that negative rejection poses a severe challenge, with the highest rejection rates being only 45% for English and 43.33% for Chinese, meaning models frequently generate incorrect answers when they should refuse to respond. Information integration shows similarly weak results, with the highest accuracy reaching only 60% for English and 67% for Chinese even without noise, dropping to 43% and 55% respectively when noise is introduced. Error analysis identifies three main problems: long-distance information difficulty (where answer-relevant content is far from question mentions), evidence uncertainty (speculative information misleading the model), and concept confusion (similar but incorrect concepts being mistaken for correct ones).\n\n## Implications and Future Directions\n\nThe study demonstrates that current LLMs heavily prioritize retrieved information over their own internal knowledge, even when warned about potential errors—a significant concern for real-world applications where misinformation abounds on the internet. The models struggle particularly with complex questions that require reasoning across multiple sub-problems, often merging answers from different sub-questions, ignoring one sub-question entirely, or misaligning documents with the wrong sub-questions. The counterfactual robustness experiments reveal that LLMs can be easily misled by documents containing incorrect facts, and they lack safeguards to handle inaccurate responses caused by misinformation. These findings indicate that substantial work remains before RAG can be reliably applied to LLMs in production environments, requiring careful design of retrieval systems and potentially new training approaches to address these fundamental limitations in information integration and error detection capabilities.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}