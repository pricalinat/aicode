{
  "paper_id": "2306.16425",
  "title": "Cctl",
  "category": "ecommerce_evaluation",
  "year": 2023,
  "timestamp": "2026-03-01T13:59:11.285999",
  "summary": "The paper proposes CCTL, a collaborative cross-domain transfer learning framework for CTR prediction across multiple business domains where click-through rates, data volumes, and feature schemas differ. It targets two common industry approaches and their failure modes: multi-task union training that can be dominated by data-rich domains and cause a seesaw effect for sparse domains, and pretrain then finetune that can get stuck in a source-domain local optimum and suffer negative transfer under domain shift. *CCTL achieved SOTA score on offline metrics.*\n\nCCTL combines three modules that work together to transfer only helpful information while preserving domain differences:\n- Symmetric Companion Network (SCN): a dual-tower design with a mixed tower trained on source plus target samples and a pure tower trained only on target samples, using the loss gap on the same target samples to estimate information gain and detect negative transfer; it periodically synchronizes parameters from mixed to pure to reduce drift.\n- Information Flow Network (IFN): assigns a weight to each source sample to control how much it influences training, and includes a Semantic Align Network that maps source semantic tokens (user, item, context) into target-shaped representations when feature schemas differ; it trains the selector via REINFORCE using SCN-derived reward signals, with discounted reward accumulation and delayed updates for stability.\n- Representation Enhancement Network (REN): an auxiliary contrastive objective that encourages cross-domain consistency for user sequence embeddings while keeping ID embeddings domain-distinct, aiming to retain domain-specific knowledge when domains differ substantially.\n\nExperiments span public datasets (Amazon Books to Movies and Taobao category splits) and a large Meituan industrial dataset with substantial user and item overlap between source and target domains. Baselines include single-domain models (LR, DNN, DeepFM, DIN, AFM) and cross-domain methods (Finetune, DANN, CoNet, MiNet, STAR), with evaluation via AUC and log loss offline and CTR plus GMV online. Results show cross-domain methods generally outperform single-domain models, but finetuning can underperform strong single-domain models on the industrial data, aligning with the non-optimal finetune problem; CCTL is reported as best overall, and ablations indicate IFN contributes the larger gain, supporting the claim that selective weighting of source samples is critical. The system is deployed in Meituan in 2022, exporting only the pure tower for online serving to avoid extra inference cost, with an A/B test over Sep. 2022 to Dec. 2022 reporting lifts. *bringing 4.37% CTR and 5.43% GMV lift, which is significant to the business.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}