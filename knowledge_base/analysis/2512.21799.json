{
  "paper_id": "2512.21799",
  "title": "Kg20C",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T13:52:23.458316",
  "summary": "# KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks\n\n## Overview\n\nThis paper introduces KG20C and KG20C-QA, two curated benchmark datasets for advancing question answering research on scholarly data. KG20C is a high-quality scholarly knowledge graph constructed from the Microsoft Academic Graph (MAG), while KG20C-QA supports QA tasks by converting graph triples into natural language question-answer pairs.\n\n## Motivation and Background\n\nExisting knowledge graph benchmarks like WN18RR and FB15k-237 are lexical or encyclopedic in nature and do not reflect the structure or challenges of scholarly metadata. Large scholarly graphs such as MAG are massive and noisy, lacking standardized train/validation/test splits necessary for controlled evaluation. The authors note that datasets like DBLP-QuAD exist but do not align with the rigorous standards of KG completion benchmarks.\n\n## Dataset Construction\n\n**KG20C** was constructed in three stages:\n1. **Data extraction**: Papers published between 1990-2010 at 20 top-tier computer science conferences (AAAI, AAMAS, ACL, CHI, COLT, EC, FOCS, ICCV, ICDE, ICDM, ICML, ICSE, IJCAI, NIPS, SIGGRAPH, SIGIR, SIGMOD, UAI, WWW, and DCC) were selected from MAG using CORE 2020 A* ranking criteria\n2. **Graph construction**: Five entity types (Paper, Author, Affiliation, Venue, Domain) with five intrinsic relation types (author in affiliation, author write paper, paper in domain, paper cite paper, paper in venue)\n3. **Standardized splitting**: Random splits into training (48,213 triples), validation (3,670), and test (3,724) sets, ensuring all entities and relations appearing in validation/test also appear in training\n\n**KG20C-QA** converts every KG20C triple into one or more question-answer pairs using template-based questions. Each relation generates both forward queries (given head entity and relation, predict tail) and reverse queries. The dataset contains 96,426 training, 7,340 validation, and 7,448 test QA pairs.\n\n## Baseline Experiments\n\nThe authors evaluated multiple models:\n- **Random baseline**: Near-zero performance\n- **Word2vec skip-gram**: MRR of 0.068\n- **CP (Canonical Polyadic)**: MRR of 0.215\n- **MEI (Multi-Partition Embedding Interaction)**: MRR of 0.230, best overall performance\n\nFor QA, MEI achieved varying results across relation typesâ€”strongest on author-organization and paper-conference queries (MRR up to 0.693), weakest on domain-related queries (MRR as low as 0.052).\n\n## Key Findings\n\nMulti-relational embeddings significantly outperform single-relational baselines like word2vec. The MEI model demonstrates superior expressiveness and parameter efficiency. The benchmarks remain challenging and unsaturated, presenting opportunities for future modeling improvements. Relation-level analysis reveals that current graph-based methods succeed with author-organization and paper-Conference relationships but struggle with domain and citation relations.\n\n## Contribution Summary\n\nThe datasets provide reproducible, standardized resources comparable in rigor to WN18RR and FB15k-237 but grounded in scholarly metadata. They support both graph-based methods (using entity-relation form) and text-based models (using natural language questions), enabling comparative study across the knowledge graph and NLP communities.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}