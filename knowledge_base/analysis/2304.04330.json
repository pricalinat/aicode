{
  "paper_id": "2304.04330",
  "title": "Pretrained Embeddings Ecommerce",
  "category": "product_matching",
  "year": 2023,
  "timestamp": "2026-03-01T14:33:43.925467",
  "summary": "This paper studies why pretrained embeddings that look good in offline pretraining can fail in real e-commerce ML systems, focusing on two practical pain points: downstream instability across repeated pretraining runs with the same setup, and the lack of scalable ways to predict whether an embedding will help a given downstream task. It argues that both issues often stem from how information is encoded during pretraining and decoded downstream, and it proposes a kernel-based perspective that yields simple, stable predictability metrics backed by theory, benchmark experiments, and online production testing.\n\nThe first major finding is that model-structure mismatch between pretraining and downstream can materially degrade both performance and stability even when pretraining and downstream data distributions are aligned. The paper contrasts homogeneous configurations where encoding and decoding match (for example contrastive learning with inner-product decoding, or BERT-style text pretraining with linear decoding) against heterogeneous ones (for example contrastive learning embeddings used with logistic regression, or BERT embeddings used with an inner-product kernel SVM). Using Item2vec and BERT or ALBERT embeddings on Instacart, Amazon Electronics, and MovieLens-1M, the authors show large coordinate-level variability in embeddings across runs and high variance in downstream classification when decoding is mismatched, while matched configurations improve both accuracy and variance. Theoretical analysis via generalization bounds highlights an extra mismatch-related term for heterogeneous setups, supporting the recommendation to design downstream fusion and heads to be approximately aligned with the pretraining encoding.\n\nThe second major contribution is a set of kernel-based metrics that aim to predict downstream usefulness without retraining full downstream pipelines. Key ideas include defining an embedding kernel via inner products and relating downstream neural models to functions of that kernel through neural tangent kernel results, then using task-dependent alignment quantities as predictors. For entity classification, the metric is based on the alignment between an embedding kernel and a label kernel; for sequential recommendation, the paper proposes a likelihood-based sequential interaction model that incorporates exposure bias and yields a sequence scoring function expressed as a weighted sum of embedding-kernel similarities, discounting frequently exposed items. Across varied Item2vec hyperparameters, these kernel metrics correlate strongly with downstream results for multiple classifiers and sequential recommenders. In a production deployment on a large US e-commerce platform, the kernel metric differentiated three candidate item embeddings and better matched online outcomes than standard offline evaluation, which the authors argue is often confounded and costly because it requires retraining and tuning per embedding version.\n\n*pretrained embeddings can cause instability issue, that is, the downstream performance is unstable even if the pretraining data and configurations are kept the same*\n\n*we establish a principled perspective of pre-trained embeddings via the lens of kernel analysis, which can be used to evaluate their predictability, interactively and scalably*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}