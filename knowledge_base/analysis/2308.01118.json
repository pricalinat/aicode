{
  "paper_id": "2308.01118",
  "title": "Popularity Bias Survey",
  "category": "ecommerce_evaluation",
  "year": 2023,
  "timestamp": "2026-03-01T14:03:03.342105",
  "summary": "This survey reviews popularity bias in recommender systems: the tendency for algorithms to over-recommend already popular items, limiting discovery of long-tail content and potentially creating reinforcing feedback loops over time. It argues popularity bias is not inherently harmful in every context, but can reduce value for users and providers or cause harms in specific domains, especially when popularity reflects exposure effects, engagement-optimizing objectives, or manipulated interactions. It also highlights that the literature often treats popularity bias as problematic by default, with limited application-specific justification and limited direct evidence about real-world stakeholder impact.\n\n*popular items are recommended even more frequently than their popularity would warrant.*  \n*the recommendations provided by the system focus on popular items to the extent that they limit the value of the system or create harm for some of the involved stakeholders.*\n\nThe paper synthesizes 123 works (published up to January 31, 2024) and proposes an impact-oriented definition of popularity bias centered on value limitation or harm, rather than popularity itself. It maps causes and amplification paths, including naturally skewed interaction distributions, data collection and representation bias, model inductive biases, optimization choices that reward popularity, position bias and exposure effects, and retraining dynamics that feed popularity back into the data. It categorizes mitigation techniques by where they intervene in the pipeline: pre-processing (sampling, filtering, augmentation), in-processing (regularization, constraints, re-weighting including IPS-style methods, graph similarity adjustments, side-information and NLP enrichment, causal and counterfactual approaches), and post-processing (score re-scaling, re-ranking, rank aggregation, filtering).\n\nA major conclusion is methodological: evaluation is dominated by offline experiments with many abstract, inconsistent metrics and thresholds, making results hard to compare and risking an abstraction trap where metrics stand in for real impacts. User studies and field tests are rare, and the paper notes that computational popularity metrics often fail to align with user perceptions, while offline datasets (especially media datasets like MovieLens) may not reflect the skew and stakes of real deployments. It calls for more application-grounded problem framing, clearer success criteria tied to stakeholder outcomes, richer study designs (including longitudinal analyses and human-in-the-loop work), and better dataset availability in high-impact domains where fairness and harm concerns are most acute.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}