{
  "paper_id": "2503.09223",
  "title": "Lref Ecommerce",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T13:59:38.440444",
  "summary": "This paper presents LREF, an LLM-based framework for predicting query–product relevance in e-commerce search, arguing that BERT-like discriminative models have limited world knowledge and less controllable reasoning for complex relevance rules. It highlights three main obstacles when using LLMs directly for relevance classification: data quality matters more than data volume, the model needs task-specific intermediate reasoning aligned to business rules, and LLMs show an optimistic bias that can cause over-recall and mis-expose irrelevant products. *To overcome the above problems, this paper proposes a novel framework called the LLM-based RElevance Framework (LREF) aimed at enhancing e-commerce search relevance.*\n\nLREF combines three stages, evaluated on a five-level relevance schema named ESMTR: Exact, Significant, Marginal, Trivial, Irrelevant. The method is:\n- SFT with Data Selection: selects challenging but clean training pairs using three auxiliary models (Initial Model, Challenge Identifier, Mislabeled Supervisor) to filter easy examples and potential annotation noise, then fine-tunes on the selected subset.\n- Multi-CoT tuning: trains the LLM to produce structured reasoning via three Chain-of-Thought types, Expert Explaining, Rule Adherence (using product relevance plus modifier relevance rules), and Decision Reflection (learning from prior incorrect decisions).\n- DPO de-biasing: frames optimistic over-recall as a preference alignment problem, constructing preference pairs from cases where the model’s top prediction is wrong but the correct label appears in beam search top-k, then applying Direct Preference Optimization to favor the cautious correct output.\n\nExperiments include offline comparisons against BERT, DeBERTa, and an LLaMA-2 7B fine-tune baseline, plus online A/B testing on JD traffic. Offline tables show LREF improves over the LLM baseline and ultimately exceeds the reported BERT/DeBERTa results after adding Multi-CoT and DPO, with analysis emphasizing better handling of borderline Marginal vs Significant cases. The paper reports training on about 5,000,000 labeled pairs, while the test set size is inconsistent between text (33,000) and Table 2 (330,000) though the class distribution is detailed in Table 2. For deployment, it distills LLM judgments by having the LLM annotate about 2 billion pairs and training a BERT-based model for low-latency serving; an online 7+ day A/B test (20% traffic test vs 20% base) reports improvements of +0.209% UCVR, +0.120% UCTR, +1.016% relevance satisfaction, and +0.023% UV value. *The experiments show that our framework achieves significant improvements in both offline and online metrics.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}