{
  "paper_id": "2601.06426",
  "title": "Nc-Bench",
  "category": "mini_program_service",
  "year": 2026,
  "timestamp": "2026-03-01T13:48:54.592788",
  "summary": "NC-Bench is a benchmark designed to evaluate the conversational competence of large language models by focusing on the form and structure of natural conversation rather than the content of model behavior. Developed by researchers at IBM, the benchmark is grounded in the IBM Natural Conversation Framework, which captures over 120 generic conversation patterns derived from conversation analysis research. The key insight driving this work is that natural conversation involves more than information exchange—it requires repairing answers, closing sequences, and coordinating mutual understanding between participants.\n\nThe benchmark comprises three distinct test sets. The Basic Conversation Competence set evaluates fundamental sequence management practices including answering inquiries, repairing responses (definition, paraphrase, repeat, and example requests), and closing conversational pairs. The RAG set applies the same sequence management patterns but incorporates retrieval-augmented generation with information passages to test whether models maintain conversation patterns when grounded in document context. The Complex Request set extends evaluation to intricate business process scenarios requiring slot-filling, preliminary requests, recommendations, and multi-turn detail elicitation.\n\nInitial evaluations across six open-source models—Granite-2B/8B, Llama-3B/8B, and Qwen2.5-3B/7B—revealed clear performance patterns. All models performed well on basic answering tasks, which aligns with their training on question-answering data. Repair tasks proved more challenging, with all models struggling significantly on repeat requests—Qwen-3B achieved only 75% on basic repeat tasks while Granite models performed worst at 5-30%. The models tended to paraphrase instead of repeating, possibly due to fine-tuning practices that discourage repetition. Closing sequences showed mixed results, with some models elaborating unnecessarily rather than recognizing closure signals.\n\nModel-specific strengths emerged across different benchmark sets. Qwen-3B achieved the highest score on the Basic set at 82.22%, while granite-8B led the RAG set at 77.77%. Interestingly, granite-2B performed best on the Complex Request set at 80.15%. The RAG ungrounded task revealed a significant issue: all models frequently provided answers even when the passage did not contain relevant information, failing to appropriately respond with \"I don't know.\" Complex multi-turn requests proved most challenging overall, with models showing mixed performance on preliminary, recommendation, detail request, and expansion patterns.\n\nThe research makes several contributions to LLM evaluation methodology. NC-Bench provides a lightweight, extensible framework that tests generic conversation practices applicable across domains rather than domain-specific skills. The benchmark enables targeted identification of specific gaps in model ability, such as the widespread inability to repeat prior turns—a critical capability for voice interfaces where there is no chat history. The authors argue that natural conversation patterns appear to exist within trained models, but optimizing their performance through system prompting or fine-tuning remains an open area for investigation.\n\nFuture work will expand the benchmark to include storytelling sequences for long answers and narratives, as well as embodied conversation patterns involving deictic references to physical artifacts. By open-sourcing both the code and dataset, the researchers aim to enable community contributions toward a more comprehensive set of conversation patterns for evaluating conversational AI systems.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}