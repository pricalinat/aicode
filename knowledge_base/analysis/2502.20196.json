{
  "paper_id": "2502.20196",
  "title": "Chinese Ecomqa",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T13:50:31.902619",
  "summary": "# ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for Large Language Models\n\n## Summary\n\nThis paper introduces ChineseEcomQA, a comprehensive question-answering benchmark designed to evaluate large language models on fundamental e-commerce concepts in the Chinese language context. The benchmark addresses a critical gap in evaluating LLMs' domain-specific capabilities, as existing models often generate factually incorrect information (hallucinations) when applied to complex e-commerce scenarios.\n\nThe authors identified two primary challenges in building scalable e-commerce benchmarks: the heterogeneous and diverse nature of e-commerce tasks, and the difficulty of distinguishing between general world knowledge and domain-specific e-commerce expertise. To address these, ChineseEcomQA is built on three core characteristics: Focus on Fundamental Concept, E-commerce Generality, and E-commerce Expertise.\n\nThe benchmark encompasses 1,800 carefully curated question-answer pairs across 20 major industries and 10 core concept dimensions. These sub-concepts range from basic concepts like Industry Categorization, Category Concept, and Brand Concept to advanced concepts such as Intent Concept, Relevance Concept, and Personalized Concept. The average reference answer length is 18.26 characters, maintaining a concise format consistent with the SimpleQA series for cost-effective evaluation.\n\nThe data construction process combines three validation methods: LLM verification using GPT-4o to filter low-quality pairs, Retrieval-Augmented Generation (RAG) validation to ensure e-commerce generality through web and e-commerce search engines, and rigorous manual annotation for final quality assurance. Questions must meet strict criteria including objective and unique answers that do not change over time, with knowledge cutoff at the end of 2023.\n\nThe authors conducted extensive evaluations on 27 mainstream LLMs, including both closed-source models (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-pro, o1-preview) and open-source models (Qwen2.5 series, LLaMA3.1 series, DeepSeek series). Key findings reveal that Deepseek-R1 and Deepseek-V3 currently outperform all other models, demonstrating the promising potential of reasoning LLMs in the e-commerce domain. The benchmark confirms scaling laws apply to e-commerce concepts, with larger models showing superior performance on advanced concepts.\n\nInterestingly, the study found that Deepseek-R1-Distill-Qwen models perform worse than the original Qwen series, suggesting that distilled reasoning capabilities do not always transfer effectively to open-domain knowledge tasks. Chinese-developed models generally showed better performance on e-commerce-specific concepts compared to Western models.\n\nThe research also analyzed model calibration and found that larger models exhibit better confidence calibration, though most models tend toward overconfidence. The introduction of RAG significantly improved performance across all model sizes, with small models like Qwen2.5-14B achieving a 27.9% absolute improvement, and large models like Deepseek-V3 showing 10.44% relative improvement. RAG also narrowed the performance gap between models while still maintaining scaling law benefits.\n\nThe benchmark is publicly available at GitHub and represents a significant contribution to domain-specific LLM evaluation, providing valuable insights for future research in e-commerce applications.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}