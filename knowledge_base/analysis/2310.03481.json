{
  "paper_id": "2310.03481",
  "title": "Paper",
  "category": "mini_program_service",
  "year": 2023,
  "timestamp": "2026-03-01T14:01:23.317903",
  "summary": "This paper argues that embedding-based retrieval models produce similarity features that are not optimal for downstream ranking in e-commerce, and proposes a ranking-oriented transformer-based two-tower model plus a two-stage training recipe to close that gap. The approach is built for personalization on surfaces like a homepage where intent is implicit, and it is reported as deployed in Yandex Market with strong offline and online gains.\n\n*We demonstrate that features produced by retrieval-focused deep learning models are sub-optimal for ranking stage in e-commerce recommendations.* The model encodes items using content only (product titles), avoiding ID embeddings to reduce memorization and improve cold-start robustness; titles are tokenized with a large wordpiece vocabulary and pooled as a CBOW-style embedding, then passed through a simplified transformer-like stack with residual connections and layer norms, followed by l2 normalization. Users are represented by a bidirectional transformer over a chronological sequence of events (click, add-to-cart, add-to-favorites, purchase, plus optionally web-search queries), where each event embedding sums content, reversed absolute position, and event-type embeddings; the final user vector is the post-transformer [CLS] embedding, also l2 normalized, and user item similarity is an inner product equivalent to cosine similarity.\n\nTraining is explicitly split into retrieval pre-training and ranking fine-tuning to handle sparse high-value signals: pre-training uses sampled softmax with in-batch negatives, while fine-tuning uses impressed negatives and combines calibrated pairwise losses for multiple positive signals with a pointwise click loss to keep scores well-behaved during continuous training. The paper also introduces context debiasing for offline, batch-served embeddings: a lightweight context tower learns additive context scalars during training (using features like recommendation surface and device), then is detached at deployment so the produced similarity feature is less entangled with context that downstream rankers can already model. Offline evaluation treats the learned similarity as an added feature to gradient-boosted tree rankers and reports incremental relative nDCG improvements from the two-stage scheme, calibration choices, longer histories up to 1024 events, context debiasing, and adding web-search queries, with larger gains on discovery than retargeting; early fusion of web and e-commerce events performs best among compared query-integration strategies. *Table 4 shows that our model increased the number of orders attributed to recommendations by 6% on the homepage and by 5% on the cart page.* Implementation details include a 4-layer user transformer (hidden size 256, 4 heads), a 4-layer item tower (hidden size 1024), distributed training with PyTorch and Deepspeed on A100 GPUs, and weekly continuous fine-tuning on new data chunks.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}