{
  "paper_id": "2102.07098",
  "title": "Product Relevance",
  "category": "product_matching",
  "year": 2021,
  "timestamp": "2026-03-01T14:53:19.602100",
  "summary": "The paper tackles product relevance in e-commerce search, focusing on the gap between what users type and how sellers describe items, and on the lack of direct, clean relevance labels at scale. It argues that click-through data is abundant but unreliable because clicks reflect many non-relevance factors, and that common pair-wise approaches help with bias but produce poorly calibrated absolute scores that are hard to use for online filtering. The core contribution is a framework that learns a relevance model from weak click supervision while shaping a usable score distribution, then optionally improves it with limited human labels.\n\n*click behavior is noisy and misleading, which is affected by not only relevance but also factors including price, image and attractive titles*  \n*The proposed model has already been deployed online, serving the search traffic of Taobao for over a year.*\n\nMethod-wise, it introduces a Level-Wise Relevance dataset built from Taobao search logs by creating finer-grained training types instead of binary relevant or irrelevant labels. Key pieces:\n- Position-bias handling: it estimates exposure-position bias via an online random re-ranking experiment, then calibrates CTR by dividing by the estimated bias so CTR better reflects relevance.\n- Five-instance taxonomy from logs: three positive levels based on calibrated CTR percentiles on the first results page (strong relevant, relevant, weak relevant) plus two negative types (strong irrelevant via random product sampling, and weak irrelevant designed to be hard).\n- Hard negative generation: weak irrelevant examples come from query rewriting; low-confidence rewritten queries are intended to be semantically close but meaning-shifted, so products clicked under the rewrite are treated as likely irrelevant to the original query.\n- Model architecture: MASM is a representation-based neural matcher for query and product title text, enhanced with multi-aspect attention that produces multiple latent vectors per side, then combines them with lightweight interaction features and an MLP to output a relevance score in 0 to 1.\n- Training objective: instead of pair-wise loss, it uses a point-wise thresholded loss with per-type target thresholds (higher for stronger positives, lower for stronger negatives). The loss only penalizes when a score falls on the wrong side of its threshold, aiming for robustness to noisy labels and a more interpretable score scale. A separate fine-tuning stage uses MSE on human Good or Bad labels when available.\n\nExperiments use an extremely large constructed click dataset (about 6.2 billion query-product samples) plus a human-annotated dataset (over 1.3 million total across train, validation, and test) and an external test set from prior work. Offline results show that the proposed dataset construction plus thresholded point-wise training substantially outperforms training on raw clicks, beats a DSSM baseline, and is competitive with or better than stronger baselines, including a BERT-embedding approach fine-tuned on labels; adding fine-tuning on annotations yields the best scores reported in their comparisons. Ablations attribute especially large gains to the weak irrelevant hard negatives, and score-distribution analysis shows their method produces higher scores for most positives instead of the near-normal score spread typical of pair-wise click-trained models. Online, the model is deployed with precomputation to reduce latency from about 50 ms to about 8 ms, and a 7-day A/B test against a strong prior system reports a roughly 0.55% lift in Gross Merchandise Volume and a 0.76 percentage-point increase in human-judged relevance rate.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}