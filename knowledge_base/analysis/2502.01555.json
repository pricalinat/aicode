{
  "paper_id": "2502.01555",
  "title": "Brand Entity Linking",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper tackles brand entity linking for e-commerce search queries: detecting brand intent in very short, noisy queries and linking the mention to a unique global brand entity despite huge, constantly growing brand space and many surface-form variants (languages, scripts, abbreviations, parent and sub-brand relationships). It compares a standard two-stage pipeline (brand mention detection, then disambiguation) with an end-to-end approach that directly predicts the brand entity from the query, and reports both offline benchmark gains and online A/B test lifts in recall and engagement.\n\nThe two-stage system uses a multilingual DistilBERT sequence tagger (MetaTS-NER) to extract brand mentions, then maps them to brand entities via either exact dictionary matching or semantic matching framed as extreme multi-class classification using PECOS; a product-type classifier (Q2PT) plus precomputed brand entity to product type associations provides a filtering step to resolve one-to-many mappings and protect precision. The end-to-end model (Q2E-PECOS) predicts brand entities (plus a NIL class for non-branded queries) directly from query text, with complexity benefits from PECOS (beam search over a hierarchical structure) and optional filtering using product type and relevance scores. *The task presents unique challenges because queries are extremely short (averaging 2.4 words), lack natural language structure, and must handle a massive space of unique brands.*\n\nExperiments use three training sources (a brand2entity dictionary used both for training augmentation and mapping, a strongly-labeled multilingual human-annotated query set across 13 languages, and a weakly-labeled set derived from historical queryâ€“product interactions) and evaluate precision, recall, coverage, and F1 across two store groups and multiple languages. Results show exact lexical matching is very high precision but lower coverage, semantic and end-to-end PECOS increase coverage and recall with some precision trade-offs, and a fusion strategy that prioritizes lexical matches while falling back to Q2E-PECOS yields the best overall balance; a bi-encoder baseline lags behind PECOS variants in this setting. The paper also measures false alarm rates on 85K non-branded queries (lowest for NER + exact, higher for PECOS-based models) and reports online A/B test improvements for the fusion treatment over the lexical baseline. *The fusion solution improves brand entity recall by +11.0% in Group-1 cluster stores and +5.44% in Group-2 cluster stores.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2502.01555_brand_entity_linking.pdf"
}