{
  "paper_id": "2506.09902",
  "title": "Personalens",
  "category": "mini_program_service",
  "status": "success",
  "summary": "PersonaLens is a benchmark to measure how well task-oriented conversational assistants personalize their help, meaning they adapt to individual user preferences while still completing goals. It targets gaps in prior benchmarks that focus on chit-chat, non-conversational personalization, or narrow domains by combining rich user context, multi-turn task interactions, and automated evaluation at scale. *We introduce PersonaLens, a comprehensive benchmark for evaluating personalization in task-oriented AI assistants.* It includes 1,500 user profiles and 111 tasks across 20 domains, producing 122,133 user-task scenarios, with evaluation done through an LLM-based user simulator and an LLM-based judge that scores task success, personalization, and response quality.\n\nThe benchmark builds each user profile from demographic attributes sourced from PRISM Alignment, then generates domain preferences and past interaction summaries using an LLM while applying a per-user domain mask to remove domains the user is not interested in. Tasks are generated as single-domain and multi-domain (typically spanning 3 to 5 domains) and paired with situational context such as location, device, time of day, day of week, and environment, also LLM-generated to reflect realistic conditions. Conversations are produced by a user agent that initiates and continues dialogue until completion or a turn limit, while a judge agent assigns metrics including task completion as a boolean, personalization on a 1 to 4 scale, and naturalness and coherence on 1 to 5 scales. Validation checks cover demographic representation, internal consistency of generated profiles, preference distribution balance using Shannon evenness, lexical diversity comparisons to other dialogue datasets, and alignment with human ratings on a 100-dialogue sample using inter-annotator agreement and Cohen kappa.\n\nExperiments evaluate multiple assistant model families including Claude, Llama 3.1 Instruct, Mistral 7B, and Mixtral 8x7B, using a sampled subset of 50 profiles for feasibility with 3,283 single-domain and 813 multi-domain dialogues. Results show task completion rates are often high, but personalization scores cluster around roughly 2 out of 4, indicating substantial headroom, and performance generally drops on multi-domain tasks due to cross-domain complexity and preference conflicts. Ablations on Claude 3 Sonnet find past interaction summaries drive the largest personalization gains, with demographics and situational context adding smaller improvements that become more helpful when combined with interaction history; domain analysis suggests recommendation domains personalize more easily than procedural domains. *We also demonstrate that interaction history is the most valuable contextual factor for improving personalization,* and the paper notes limitations such as text-only scope, simulated rather than executed actions, and potential biases inherited from LLM-generated profiles and dialogues despite mitigation steps.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2506.09902_PersonaLens.pdf"
}