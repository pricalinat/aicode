{
  "paper_id": "1210.7056",
  "title": "Selective Transfer",
  "category": "ecommerce_evaluation",
  "status": "success",
  "summary": "This paper tackles cross-domain collaborative filtering when the target domain has sparse ratings: transferring data from other domains can reduce overfitting, but naive transfer can cause negative transfer because some source data are inconsistent with the target. The authors propose Selective Transfer Learning for Collaborative Filtering (STLCF), which explicitly estimates cross-domain consistency and selectively transfers knowledge instead of assuming all sources help equally. The core idea is to judge consistency using both empirical prediction error and the variance of that error, since low error on a sparse target can happen by chance and is not reliably informative without a confidence signal.\n\nSTLCF embeds this criterion into a boosting framework that reweights instances and whole source domains over multiple iterations. A weighted transfer CF base learner (illustrated with TGPLSA, an extension of Gaussian probabilistic latent semantic analysis) is trained each round; then the method computes target and source errors, assigns a model weight, increases weights on mispredicted target items, and decreases weights for less helpful source domains via a domain-level factor. The loss combines an exponential form of item-level error with a variance term weighted by a parameter gamma, and prediction tolerance is controlled by a threshold tau; empirically, tau on the order of 1e-2 and gamma around 0.4 to 0.5 work well. The framework is presented as general-purpose: other generative CF models could replace TGPLSA as the weak learner.\n\nExperiments cover multiple real datasets and transfer settings: Douban Music, Douban Book, Douban Movie, a sampled Netflix subset, Wikipedia editing logs (binary), and an IMDB movie-similarity hyperlink graph. Across several tasks (including book to movie, Netflix to Douban Movie with shared items, multiple-source transfer to Douban Movie, and heterogeneous sources to Netflix), selective transfer improves RMSE over non-transfer baselines (GPLSA, PMF) and non-selective transfer baselines (CMF, TGPLSA), with the largest gains when the target is extremely sparse and domain inconsistency is higher. The variance-aware version STLCF(EV) is especially beneficial on real cross-domain cases and helps long-tail users with few ratings; the method also shows good convergence (about 40 boosting iterations) and is more robust to overfitting than GPLSA and TGPLSA as the number of latent topics increases.\n\n*we propose a novel criterion based on empirical prediction error and its variance to better capture the consistency across domains in CF settings.*\n\n*The experimental results on real-world data sets showed that our selective transfer learning solution performs signiÔ¨Åcantly better than several state-of-the-art methods at various sparsity levels.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/ecommerce_evaluation/1210.7056_selective_transfer.pdf"
}