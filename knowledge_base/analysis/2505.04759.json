{
  "paper_id": "2505.04759",
  "title": "Zeroshot App Review",
  "category": "mini_program_service",
  "year": 2025,
  "timestamp": "2026-03-01T13:55:30.076074",
  "summary": "## Summary\n\nThis paper investigates the effectiveness of zero-shot learning with ChatGPT (GPT-4o mini) for classifying mobile app reviews into four categories: Functional Requirements (FRs), Non-Functional Requirements (NFRs), Both, or Neither. The researchers compiled a benchmark dataset of 1,880 manually annotated reviews from 10 diverse applications spanning domains such as communication (WhatsApp), travel (Uber), music (Spotify), social media (Twitter), entertainment (Netflix), gaming (Candy Crush Saga), shopping (Amazon), education (Duolingo), and health (Google Fit). Five software engineers with over five years of experience performed the annotation, achieving a Fleiss's kappa score of 0.76 indicating substantial inter-annotator agreement. The study addresses three research questions examining ChatGPT's classification accuracy, the impact of review length and complexity on performance, and which requirement types are more prone to misclassification.\n\nThe findings demonstrate that ChatGPT achieves a robust micro F1 score of 0.842 using an optimized configuration that combines RolePrompting, EmotionPrompting, and Chain-of-Thought prompting techniques at a temperature setting of 0.2. This performance significantly outperforms traditional machine learning models including Random Forest (F1=0.45), Decision Tree (F1=0.35), Support Vector Classifier (F1=0.49), XGBoost (F1=0.46), and Logistic Regression (F1=0.48). The analysis revealed that review complexity, measured by Flesch-Kincaid Grade Level (FKGL), substantially impacts classification accuracy, with correctly classified reviews having an average FKGL score of 6.34 compared to 9.24 for misclassified reviews. In contrast, review length has minimal effect on performance. The model performs particularly well on FRs (F1=0.91), NFRs (F1=0.87), and Neither categories (F1=0.94), but struggles with the \"Both\" category (F1=0.54) due to the inherent complexity of reviews addressing both functional and non-functional aspects simultaneously.\n\nManual error analysis identified five main misclassification patterns: Negative Sentiment Bias (19 instances), where the model prioritizes strongly negative language regardless of underlying intent; Overlapping Characteristics (33 instances), where requirements overlap and the model overemphasizes one aspect while neglecting another; Ambiguity in Language (22 instances), where phrases can be interpreted multiple ways; Emotionally Charged Reviews (17 instances), where intense emotional reactions lead to incorrect classifications; and Others (9 instances), including informal language and technical jargon. *The model tends to prioritize strongly negative reviews, classifying them as indicative of functional issues, regardless of the underlying intent.* The paper concludes that zero-shot ChatGPT offers a cost-effective, time-efficient solution that eliminates the need for large domain-specific training datasets, though future work could explore improved prompt engineering and review simplification preprocessing to address identified limitations.\n\nThe study makes several notable contributions to the field of requirements engineering and natural language processing. It demonstrates that large language models can effectively classify app reviews without extensive fine-tuning, which addresses a major limitation of traditional machine learning approaches that require costly and time-consuming domain-specific labeled datasets. The researchers also provide the annotated dataset and source code publicly available through Zenodo to ensure reproducibility. The paper identifies specific challenges when using ChatGPT for this classification task, including sensitivity to prompt structure, difficulty with reviews containing both functional and non-functional requirements, and bias toward negative sentiment. These findings contribute valuable insights for practitioners seeking to automate the analysis of user feedback in mobile app development contexts, while also highlighting areas where further research could improve classification accuracy.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}