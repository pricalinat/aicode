{
  "paper_id": "1803.06966",
  "title": "Polyglot Semantic Parsing",
  "category": "mini_program_service",
  "status": "success",
  "summary": "This paper proposes polyglot semantic parsing for API documentation: learning a single model that translates natural-language descriptions into well-formed function or signature representations across many natural languages and programming languages, rather than training separate models per dataset or language pair. The motivation is that APIs often share overlapping functionality and redundant descriptions across languages and libraries, so jointly training can improve robustness and enable flexible decoding into multiple target languages or even unobserved language pairs. *To solve the decoding problem, we introduce a new graph-based decoding and representation framework that reduces to solving shortest path problems in directed graphs.*  \n\nThe core technical contribution is a decoding framework that represents the entire finite search space of valid API components as a minimized directed acyclic finite-state automaton, then performs shortest-path search where edge weights are produced by a translation model during decoding. The paper instantiates this framework with (1) a lexical word-translation model adapted to incremental scoring during shortest-path search, and (2) neural encoder-decoder models with attention, plus variants that add lexical biasing and copying, with monolingual decoding achieved by restricting the graph and polyglot decoding achieved by taking the union of graphs with artificial language or library identifier tokens. Experiments cover 45 technical API documentation datasets plus multilingual semantic parsing benchmarks, using measures like Acc@1, Acc@10, and MRR, and include mixed-language inputs and zero-shot settings. Key reported facts include:\n- Technical API docs: training polyglot models on multiple datasets can substantially improve over monolingual training on Py27, with an average Acc@1 gain of about 9 percent, but the best lexical models still tend to be trained on the same distribution as the evaluation set rather than on all available datasets.\n- Semantic parsing benchmarks: neural shortest-path models are competitive and often strong on GeoQuery and Sportscaster, while lexical unigram models struggle particularly when correct argument ordering matters.\n- Generalization behaviors: polyglot decoding supports zero-shot translation across unobserved language pairs and can be run without specifying the output language, and mixed-language evaluation highlights robustness differences not captured by simple accuracy comparisons. *Beyond increases in accuracy, our polyglot models support zero-shot translation as shown in Figure 4, which can be used for translat- ing between unobserved language pairs.*  \n\nThe dataset and implementation details emphasize scale and constraints: the technical resources include Stdlib (short descriptions and signatures across 10 programming languages in 7 natural languages) and Py27 (English documentation mined from 27 popular Python projects), plus newly built datasets such as a Japanese translation of the Python 2.7 standard library and a Lua stdlib set with mixed Russian, Portuguese, German, Spanish, and English; in total they report 79,885 training pairs. Graph construction for the global polyglot setting yields a large minimized automaton with 218,505 nodes, 313,288 edges, and 112,107 paths over an output vocabulary of 9,324 words; GeoQuery and Sportscaster graphs are much smaller. The paper concludes that polyglot and mixed-language decoding are practically useful for API QA and related applications, but also that technical documentation translation poses low-resource and sparsity challenges where simpler lexical methods can outperform neural approaches, motivating broader evaluation and releasing new datasets to support further work.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/1803.06966_Polyglot_Semantic_Parsing.pdf"
}