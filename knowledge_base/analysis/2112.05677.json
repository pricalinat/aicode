{
  "paper_id": "2112.05677",
  "title": "Concept Representation Learning",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper argues that concept-oriented deep learning, or CODL, is a needed extension of todays deep learning to address three long-term gaps: learning with little or no external supervision, robustness under distribution shift, and tighter integration with symbolic AI. It frames concepts as the core unit of human-like learning and proposes learning concept representations from a small number of concept exemplars rather than from large labeled datasets, with contrastive self-supervised learning, or CSSL, as a particularly good fit because it learns from data-driven associations, supports incremental and continual learning, and can accommodate changing objectives. *learning concept representations, which are general and transferable, is crucial to meet the future challenges for deep learning*\n\nThe paper adopts a dual-level view of concept representations inspired by cognitive neuroscience: an embodied, sensory-derived level and a symbolic, language-derived level. It maps these to CODL as concept-oriented feature representations at the embodied level and a concept graph at the symbolic level, with the two associated for concrete concepts and potentially linked to a large background concept graph. The main focus is how to learn the embodied level using CSSL: exemplars may be labeled or unlabeled, but labels are used to identify and connect learned representations to the symbolic level rather than as supervised targets, and pseudo labels can be generated when exemplars are unlabeled.\n\nKey technical discussion centers on how standard contrastive losses are feature-oriented and may be insufficient for contrasting concepts, motivating concept-oriented objectives based on exemplar similarity and relational reasoning. It highlights exemplar similarity measures from memory research, including pattern robustness defined via within-concept versus between-concept similarity across concept taxonomies, and encoding-retrieval similarity as a predictor of later memory, emphasizing that useful similarity need not rely on low-level visual overlap. It also describes self-supervised relational reasoning as a pretext task with a relation head trained to distinguish intra-reasoning positive pairs made from two augmentations of the same exemplar versus inter-reasoning negative pairs from different exemplars, using losses that push relation scores toward 1 for positives and 0 for negatives. For continual learning, it reviews catastrophic forgetting in incremental CSSL and summarizes Continual Contrastive Self-supervised Learning, which combines rehearsal via clustering and exemplar selection, self-supervised knowledge distillation to preserve similarity distributions over saved exemplars, and an extra sample queue to enforce contrast between old and new data, building on MoCo with query and key encoders and a memory bank of negatives. Finally, it contrasts this with contrastive self-supervised class incremental learning, presenting SSCIL as adding CSSL to class incremental setups by removing classifiers and labels, using different incremental schemes (random classes, semantic splits guided by WordNet to reduce semantic overlap, and clustering), and evaluating with linear evaluation and generalization protocols; it closes by proposing future work on exemplar-based incremental learning that combines relational reasoning with exemplar similarity measures. *Contrastive self-supervised learning (CSSL) aims at embedding augmented versions of the same sample close to each other while pushing away embeddings from different samples*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2112.05677_concept_representation_learning.pdf"
}