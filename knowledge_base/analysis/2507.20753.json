{
  "paper_id": "2507.20753",
  "title": "Dl Gbdt Ltr",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper reports an empirical, industry-focused comparison of deep neural networks and a production-grade GBDT baseline for e-commerce learning-to-rank at OTTO. It aims to reduce practitioner uncertainty by benchmarking multiple DNN architectures and loss functions on a large proprietary interaction-log dataset, then validating the best candidate with an 8-week online A/B test against a LightGBM LambdaMART system. *The results show that a simple DNN architecture outperforms a strong tree-based baseline in terms of total clicks and revenue, while achieving parity in total units sold.*\n\nThe setting is a contextualized ranking problem over a retrieved candidate list, trained from historical contexts with multi-item implicit feedback represented as binary click and order vectors. Three DNN architectures are evaluated: Two-Tower for efficient scoring via precomputed item embeddings, Cross-Encoder for richer feature interactions, and a Transformer-enhanced Cross-Encoder using self-attention without positional encodings. Inputs combine normalized numerical features, learned embeddings for categorical features, and bag-of-words style embeddings for textual features; the study compares RankNet and Softmax cross-entropy losses, including modifications to handle multiple positives and a weighted combination of click and order objectives.\n\nKey findings (offline and online) prioritize engagement and monetization outcomes over purely offline ranking metrics:\n- Offline (NDCG at cutoff 15 for clicks and orders, plus average item value as a revenue proxy): all DNNs improve click-oriented NDCG and AIV versus the LambdaMART baseline, while generally underperforming on order-oriented NDCG, with some models reaching parity. The strongest overall candidate is the Two-Tower model trained with Softmax cross-entropy, and the Transformer performs notably worse on order-oriented NDCG than prior industry reports cited by the authors.\n- Online (8-week A/B test, Two-Tower + Softmax cross-entropy vs production LambdaMART): *the DNN achieved statistically significant uplifts of 1.86% (p < 0.0001) in the total number of clicks* and a smaller significant revenue increase, while units sold remained stable; the authors note higher training and serving costs but consider them negligible relative to the gains.\n\nThe paper concludes that, when properly tuned and validated online, a relatively simple Two-Tower DNN can be a viable alternative to GBDT-based rankers in large-scale e-commerce, and that increased architectural complexity does not necessarily translate into better business outcomes in this setting. It also highlights a broader research gap: common public LTR datasets are not representative of industrial e-commerce interaction logs, strengthening the case for cautious generalization from academic benchmarks.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2507.20753_dl_gbdt_ltr.pdf"
}