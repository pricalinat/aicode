{
  "paper_id": "2602.11518",
  "title": "Kuaisearch",
  "category": "product_matching",
  "year": 2026,
  "timestamp": "2026-03-01T14:35:53.866847",
  "summary": "KuaiSearch introduces a publicly released, large-scale e-commerce search dataset built from real user search interactions on Kuaishou, designed to support research across the full search pipeline: recall, ranking, and relevance judgment. The paper motivates the dataset by highlighting persistent real-world difficulties in e-commerce search such as ambiguous, short queries; noisy, weakly structured product text; and strong personalization needs, then argues that many prior public datasets are limited by heuristic queries, filtered long-tail data, anonymized text, or single-stage coverage. KuaiSearch aims to preserve authentic queries and natural-language product text, include cold-start users and long-tail products, and provide behavior signals that better fit LLM-based semantic and intent modeling.\n\n*KuaiSearch—to the best of our knowledge, the largest e-commerce search dataset currently available.*\n\nThe dataset scale and schema are central: it contains 331,930 users, 18,605,582 products, and 2,574,949 real queries, with components spanning recall logs, a much larger ranking dataset, and a manually labeled relevance set. Key fields include user demographics (gender, age, location), rich item metadata (title, brand, seller, and a 3-level category hierarchy), recall logs with exposed items plus clicks and purchases, and ranking examples augmented with user and item statistical features, entry-point context, and recent sequential behaviors (most recent 20 clicked and 20 purchased items). For relevance modeling, 46,422 query–product pairs are manually annotated by domain experts on a 4-level graded scale from clearly irrelevant to highly relevant; identifiers are remapped and timestamps are re-encoded as relative times for privacy, and a smaller KuaiSearch-Lite subset is released to enable faster experimentation.\n\n*We randomly sampled 331,930 users who exhibited core interaction behaviors on the platform after June 1, 2025.*\n\nThe analysis section characterizes the data distributions and user behavior: users come from 62+ regions with 99.82% located in China; female users are 59.34%, and ages 12 to 50 account for over 80% with 31–40 the largest group. Product interactions show a power-law long tail, and user search frequency varies widely, including sparse-history and heavy-history users. Query analysis reports that medium-length queries dominate (lengths 5–6 and 7–8 total 44.68%), and engagement varies by length and by search entrance: homepage and mall account for over 93% of requests, while placeholder-triggered queries show lower engagement and context-driven entrances like commodity detail pages show comparatively higher click depth; query semantics are labeled via aggregated exposure statistics into about 78 first-level categories with a long-tailed distribution dominated by apparel-related categories.\n\nBenchmark experiments on KuaiSearch-Lite cover recall, ranking, and relevance tasks with standard splits and metrics. For recall, embedding-based dual encoders (DPR variants) outperform lexical baselines like BM25 and generative retrieval methods such as DSI and LTRGR, with DPR-SDE reported as strongest overall in the table; the paper attributes weaker generative retrieval to the difficulty of generating precise item identifiers amid ambiguous intents, noisy product text, and huge catalogs. For ranking as CTR prediction, several classic models (DNN, Wide and Deep, DCN, DCN-v2, DIN) perform similarly, with DNN best Logloss and DIN best ROC-AUC, suggesting both calibration and interest modeling matter and that richer features may drive future gains. For relevance, cross-encoders outperform bi-encoders, and LLM-based generative classification via LoRA fine-tuning on label tokens yields the best results overall, with Qwen3-1.7B reported as the top performer on ROC-AUC and PR-AUC; the conclusion reiterates that KuaiSearch is intended as a realistic foundation for multi-stage, personalized e-commerce search research, especially for LLM-centered approaches.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}