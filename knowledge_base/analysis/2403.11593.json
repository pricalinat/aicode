{
  "paper_id": "2403.11593",
  "title": "Multimodal Product Matching",
  "category": "product_matching",
  "status": "success",
  "summary": "The paper describes an end-to-end, production-oriented system for multi-modal product matching in a fashion marketplace, where different sellers represent the same product with shifting image styles and inconsistent titles. The core idea is to embed each offer using both images and text, then retrieve nearest neighbors across a chosen query domain and index domain to propose matches with high precision. It argues that fashion matching is especially reliant on visual cues, making the problem closer to person re-identification than classic SKU-like matching in categories such as electronics.\n\nTheir proposed encoder, fashionID, uses late fusion: it average-pools embeddings from an offer image set, concatenates them with a text embedding built from lightly normalized brand plus title, optionally adds a small vector of numerical features, then learns a low-dimensional projection into a metric space. Training uses supervised contrastive learning over large mini-batches, made feasible by freezing large pretrained encoders and training only the shallow projection layer, with precomputed image and text embeddings to cut GPU cost. Retrieval uses cosine similarity over stored embeddings, with a blocking step based on fuzzy string similarity of brand names to reduce search, then nearest-neighbor search and a distance threshold tuned from precision-recall curves; evaluation emphasizes AUCPR because both high-precision low-recall and higher-recall regimes matter in production.\n\nKey empirical findings and engineering takeaways:\n- Image-only baselines are stronger than text-only baselines for fashion, and adding text plus three simple numerical features gives a small but consistent boost in AUCPR.\n- CLIP-based visual encoders strongly outperform DINOv2 for this product matching setup, both as raw embeddings and when paired with a trained projection; larger CLIP variants improve performance further.\n- Very large batch sizes are important for contrastive learning here, and replacing the linear projection with a small MLP improves validation metrics but slightly hurts test generalization, suggesting the linear layer better preserves pretrained generalization.\n- Lone-negative offers, despite being common at test time, reduce performance when included heavily in training for this contrastive objective; downsampling them improves results.\n- A human-in-the-loop validation stage can raise precision to production requirements by presenting the query offer and the top three predicted neighbors, while deliberately hiding hard-to-interpret fields like price and sizes and focusing validators on strong visual evidence; performance depends on the underlying model precision and can be characterized via validator TPR and FPR, with a likelihood-ratio-based formula used to predict final validated precision.\n\n*we show how a human-in-the-loop process can be combined with model-based predictions to achieve near perfect precision in a production system.*  \n\n*a relatively straightforward projection of pretrained image and text encoders, trained through contrastive learning, yields state-of-the-art results, while balancing cost and performance.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2403.11593_multimodal_product_matching.pdf"
}