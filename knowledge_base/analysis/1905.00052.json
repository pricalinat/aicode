{
  "paper_id": "1905.00052",
  "title": "Personalized Ranking",
  "category": "product_matching",
  "status": "success",
  "summary": "The paper *Personalized Ranking in eCommerce Search* proposes a lightweight, session-based personalization approach for eCommerce search ranking that augments an existing generic ranker optimized for conversion and relevance. Instead of building long-term user profiles or running an explicit re-ranking stage, it derives personalization signals from the most recent clicks within the current session and injects them directly as additional ranking features. The key contribution is a direct comparison and combination of content-based features and content-agnostic latent features, showing they complement each other in improving ranking quality measured by Mean Reciprocal Rank, MRR. *Our approach does not require an explicit re-ranking step, does not rely on learning user profiles from long term search behavior, and does not involve complex modeling of query-item-user features.*\n\nMethodologically, the authors learn item embeddings purely from historical in-session co-click sequences using word2vec skip-gram with hierarchical softmax, treating item IDs as the vocabulary and using 32-dimensional vectors with a window size of 5 (with experiments over other dimensions). Personalization features are computed by comparing candidate items on the current search results page to up to the last 5 clicked items in the same session, combining embedding-space proximity with simple item-attribute similarity:\n- Embedding features: average cosine distance to recent clicked items, and cosine distance to the most recent clicked item with an available embedding\n- Content features: price ratio to the mean price of recent clicked items, and title Jaccard similarity to the last clicked item\n\nOffline learning-to-rank experiments use LambdaMART and evaluate sold-item MRR, comparing a baseline (generic features only) to models that add embedding features, content features, or both. On a filtered high-coverage dataset (constructed to ensure the new features are applicable and not trivially matching recently clicked identical items), embedding features improve MRR by about 8 to 15 percent depending on the variant, content features by about 19 percent, and the combined model by about 26 percent with statistical significance via bootstrap confidence intervals. On the full dataset, coverage becomes the limiting factor for embedding features (around 8 percent feature coverage vs around 80 percent for content features), yielding smaller gains from embeddings alone but still adding incremental benefit when combined; the paper also reports diminishing returns at very low embedding dimensionality and suggests future work to improve coverage by training on longer click sequences and exploring longer-term user activity. *By combining all of the features we get an even higher improvement in MRR - 26%.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/1905.00052_personalized_ranking.pdf"
}