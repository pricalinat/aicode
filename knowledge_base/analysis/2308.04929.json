{
  "paper_id": "2308.04929",
  "title": "Ab Testing Survey",
  "category": "ecommerce_evaluation",
  "year": 2023,
  "timestamp": "2026-03-01T13:51:09.168676",
  "summary": "# A/B Testing: A Systematic Literature Review\n\nThis paper presents a comprehensive systematic literature review (SLR) analyzing 141 primary studies on A/B testing in software engineering. The review addresses four research questions: the subject of A/B testing, how A/B tests are designed and who participates, how tests are executed, and what open research problems exist.\n\n## Overview and Methodology\n\nA/B testing (also called online controlled experimentation) is a form of hypothesis testing where two variants of software are compared in production with real end-users. The researchers conducted a rigorous SLR following established protocols, searching major digital libraries and applying inclusion/exclusion criteria. The review included 90 focus papers (proposing new A/B testing approaches) and 51 applied papers (using A/B testing for evaluation). Most authors came from industry (51.1%), reflecting A/B testing's practical importance.\n\n## Key Findings\n\n**Subject of A/B Testing:** The primary targets are algorithms (58 occurrences), visual elements (33), and workflow/process changes (28), together comprising 86.2% of targets. Application domains include web platforms (38 studies), search engines (35), e-commerce (27), and interaction software (22). Algorithm testing spans all domains, while visual elements are particularly popular in search engines.\n\n**Design of A/B Tests:** Single classic A/B tests with two variants dominate (95 occurrences). Engagement metrics like conversion rates are most common (225 occurrences), followed by click metrics (82) and monetary metrics (64). Hypothesis testing for equality is the standard statistical approach, though notably 37 studies report p-values without specifying the concrete test used.\n\n**Stakeholder Roles:** Three roles emerged in test design: concept designer (127 occurrences), experiment architect (111), and setup technician (31). During execution, stakeholders serve as experiment contributors (managing execution) or experiment assessors (evaluating results). Common tasks include designing variants, determining experiment duration, and supervising tests.\n\n**Execution and Results:** Empirical evaluation in live systems is the dominant approach (100 studies). Primary data collected includes product/system data (48), user-centric data (26), and spatio-temporal data (20). Test results primarily inform feature selection (71) and rollout decisions (24).\n\n## Open Problems and Future Directions\n\nThe review identifies seven categories of open problems, primarily related to improving proposed approaches, extending evaluations, and adding process guidelines. Three promising research directions emerge: automating stages of A/B testing to enable continuous experimentation; improving the testing process through better sensitivity techniques, systematic solutions for managing multiple concurrent tests, and better A/B metric selection; and adopting more sophisticated statistical methods like bootstrapping, which remains underutilized despite its potential benefits.\n\n*Algorithms, visual elements, and workflow/process changes are the most commonly tested targets.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}