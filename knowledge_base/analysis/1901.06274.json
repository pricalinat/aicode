{
  "paper_id": "1901.06274",
  "title": "Review Ranking",
  "category": "ecommerce_evaluation",
  "status": "success",
  "summary": "The paper tackles the problem of overwhelming volumes of online product reviews and the unfair visibility that results when sites sort by recency or by accumulated helpfulness votes, which can lock early top reviews in place via the Matthew effect. It proposes an automated ranking system that predicts review helpfulness and uses that prediction to place reviews fairly near the top as soon as they are posted, so customers can see more high-quality content without reading hundreds of reviews. *The proposed system provides fair review placement on review listing pages and making all high quality reviews visible to customers on the top.*\n\nMethodologically, the system is a hybrid pipeline: it first classifies reviews into low-quality vs high-quality, then predicts a helpfulness score only for the high-quality reviews and ranks them by that predicted score, appending low-quality reviews to the end because they are unlikely to appear in the top k. Data were crawled from Amazon.in and Snapdeal.com (October to December 2016) across five product categories, producing tens of thousands of reviews plus product description text and customer question-answer content. The study extracts 17 features, including 15 text and readability features from reviews (such as length, lexical diversity, entropy, spelling-related counts, POS counts, and rating) plus two proposed similarity features: Desc_sim for similarity between product description and review text, and QA_sim for similarity between question topics and review text, computed with cosine similarity using bag-of-words vectors.\n\nExperiments address heavy class imbalance (far more low-quality than high-quality reviews) using SMOTE before training classifiers. For classification, the authors compare Naive Bayes, SVM, and Random Forest, and define four feature-set cases: baseline text-only, adding product description similarity, adding QA similarity, and adding both. Random Forest performs best, and adding both proposed features yields the strongest results, improving F1 for the high-quality class to 0.93 on Amazon data and 0.85 on Snapdeal data. For ranking, gradient boosting regression outperforms linear regression, and training the regressor only on high-quality reviews substantially reduces error versus training on all reviews: MSE is 0.267 for Amazon and 0.623 for Snapdeal with the classifier-first approach, versus 2.545 and 3.434 without classification. Agreement with existing site top-10 lists averages about 5 to 6 overlapping reviews, with the remaining spots often filled by newer high-quality reviews that lacked votes due to low exposure; an additional 1,000-review test and a small human study with graduate students favors the proposed predicted top 10 list in most cases. *We obtained 0.267 MSE for the Amazon dataset.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/ecommerce_evaluation/1901.06274_review_ranking.pdf"
}