{
  "paper_id": "2512.01179",
  "title": "Ctr Benchmark",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T13:48:48.607716",
  "summary": "# Benchmark for CTR Prediction in Online Advertising\n\nThis paper presents Bench-CTR, a comprehensive benchmark platform for evaluating and comparing click-through rate (CTR) prediction models in online advertising. The research addresses a critical gap: despite the proliferation of CTR prediction models over the past decade, no standardized benchmark existed for fair model comparison.\n\n## The CTR Prediction Problem\n\nCTR prediction estimates the probability that users will click on advertisements, making it essential for platforms to plan and optimize expected revenues. The U.S. online advertising market reached $225 billion in 2023 and is projected to grow to $431.76 billion by 2029. Researchers have developed numerous CTR prediction models spanning multiple categories: multivariate statistical models (logistic regression, polynomial), tree-based models (GBDT, XGBoost), factorization machines (FM, FFM, NFM), deep learning models (DNN, CNN, RNN, GNN, Transformer), ensemble models (Wide&Deep, DeepFM, DCN), and more recently, large language model (LLM)-based approaches.\n\nHowever, the diversity of models, evaluation procedures, and metrics has created reproducibility challenges. Studies using incompatible procedures often produce conflicting results, undermining confidence in model comparisons.\n\n## Bench-CTR Architecture\n\nThe proposed platform consists of three major components. The **datasets handler** provides interfaces for public datasets (Criteo, Avazu, AntM2C, KDD12, iPinYou, Avito, Outbrain, Ali-CCP, Aliyun) and synthetic data generation algorithms. The **CTR modeling suites** encapsulate 15 state-of-the-art models through modular interfaces supporting embedding, representation learning, and click probability prediction. The **evaluation system** implements a comprehensive protocol with validation methods, hyper-parameter settings, multi-level metrics, and experimental guidelines.\n\nThe CTR prediction modeling process encompasses five components: data preparation (quality assessment, preprocessing), feature engineering (transformation, interaction modeling), embedding (categorical, numerical, temporal, text, image, graph), representation learning (various model architectures), and prediction (logistic or softmax functions).\n\n## Evaluation Protocols\n\nThe paper establishes a comprehensive system of evaluation protocols. The **multi-level metric taxonomy** organizes 20 metrics into four levels: first-level (TP, FP, TN, FN, predicted probability), second-level (precision, recall, accuracy, MCC, Logloss, MSE, COPC, KLD, field-level calibration error), third-level (AUC-ROC, AUC-PR, F1, RMSE, RIG), and fourth-level (RelaImpr).\n\nThe evaluation procedure includes dataset selection criteria (advertising format alignment, feature requirements, dataset popularity), validation methods (hold-out and cross-validation), hyper-parameter optimization (grid search, random search), metrics selection guidelines, model comparison protocols, sensitivity analysis, and ablation study methodology.\n\nAnalysis of metric usage from 2012-2024 reveals that AUC-ROC and Logloss are the most frequently used metrics, with AUC-ROC maintaining high usage rates (reaching 100% in some years) and Logloss usage increasing from 25% to 68%. These two metrics are recommended as complementary and reliable for CTR prediction tasks.\n\n## Synthetic Data Generation\n\nBench-CTR includes two synthetic data generation algorithms. The **rule-based approach** generates synthetic instances through three steps: feature generation (categorical from categorical distributions, numerical from truncated normal and log-normal), feature interaction modeling (first, second, and third-order interactions), and click label sampling from Bernoulli distribution based on computed probabilities. The **diffusion-based approach** uses a latent diffusion model with VAE encoder-decoder, trained with CTR prediction loss to capture feature interactions and behavioral patterns relevant to CTR prediction. This generates synthetic data that maintains realistic distributions while mitigating data sparsity and privacy concerns.\n\n## Comparative Study Results\n\nThe experimental validation evaluated 15 models across 11 metrics on three public datasets (Criteo: 36.7M training instances; Avazu: 40.4M; AntM2C: 1.5M) and two synthetic datasets.\n\n**Key findings:**\n\n1. **High-order models outperform low-order models** - DeepFM achieved the best results on Criteo, PNN on Avazu, and GDCN on AntM2C. All top performers are high-order feature interaction models.\n\n2. **LLM-based models demonstrate remarkable data efficiency** - Uni-CTR achieved an AUC-ROC of 0.8191 using only 2% of the training data compared to other models, while competitive with models trained on much larger datasets.\n\n3. **Performance plateau since 2016** - Both average and best model performance showed significant improvements from 2015 to 2016 (correlated with deep learning adoption), then entered a period of slow progress that remains consistent across datasets.\n\n4. **Metric correlations observed** - Strong correlations exist between AUC-ROC and AUC-PR, between MCC and F1, and among Logloss, MSE, and RMSE.\n\n5. **Reproducibility challenges confirmed** - Experiments revealed discrepancies in relative performance between GDCN and DeepFM compared to original papers, highlighting the importance of standardized benchmarks.\n\nThe rule-based synthetic dataset produced minimal performance gaps among models, demonstrating learnable patterns. The TSTR (train on synthetic, test on real) evaluation showed AUC-ROC scores of 0.63-0.65, proving synthetic data effectively captures underlying distributions despite lower absolute performance due to limited training size (80K instances).\n\n## Challenges and Future Directions\n\n**Main challenges identified:**\n\n- **Low code availability**: Only a small portion of CTR prediction papers share code, and shared code often lacks essential components, hindering reproducibility.\n- **Dataset limitations**: Public datasets like Criteo and Avazu are over a decade old and fail to support emerging advertising formats (interactive video, VR, AI-generated ads). Features may be anonymized, hampering semantic-based models.\n- **Computational demands**: Embedding parameters consume substantial memory, deep learning increases architecture complexity, and LLM integration escalates resource requirements (Uni-CTR uses a 1.5B parameter model).\n- **Data imbalance**: Non-clicks vastly outnumber clicks, causing models to excel at predicting the majority class while performing poorly on clicks.\n\n**Future research directions:**\n\n- **LLM integration**: Combining LLMs' semantic understanding with GNN structural signals for unified CTR prediction architectures.\n- **Contrastive learning for multi-modal CTR**: Using contrastive learning to bridge heterogeneity gaps between modalities (text, images, video) and enhance discriminative capability.\n- **Multi-task learning**: Capturing underlying relationships between CTR prediction and related tasks like conversion rate estimation.\n- **Causal inference**: Moving beyond correlation-based models to learn causal relationships, improving generalization to unseen data.\n\nThe benchmark platform and code are available at https://github.com/NuriaNinja/Bench-CTR, providing a standardized environment for model development and evaluation in CTR prediction research.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}