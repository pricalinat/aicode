{
  "paper_id": "2307.04571",
  "title": "Matthew Effect",
  "category": "ecommerce_evaluation",
  "year": 2023,
  "timestamp": "2026-03-01T13:55:52.197040",
  "summary": "# Summary: Alleviating Matthew Effect of Offline RL in Interactive Recommendation\n\nThis SIGIR 2023 paper addresses a critical flaw in applying offline reinforcement learning to recommender systems: the Matthew effect, where popular items get increasingly recommended while unpopular ones are suppressed. The authors propose DORL (Debiased model-based Offline RL), a method that introduces entropy-based penalties to counteract this bias while maintaining user satisfaction.\n\n## Problem and Background\n\nOffline RL has become attractive for recommendation because it learns policies from logged data without requiring online interaction. However, offline RL suffers from value overestimation when extrapolating to unseen state-action pairs. Existing solutions add conservatism—constraining learned policies near behavior policies or penalizing rarely-visited items—but this creates a unintended consequence in recommendation: the Matthew effect.\n\nThe paper demonstrates empirically that higher repeat rates in recommendations directly hurt Day-1 Retention across both video (KuaiRand-27K) and music (LFM-1b) datasets. When conservative methods penalize uncertain samples, they inadvertently promote dominant categories from training data, narrowing recommendations and shortening user interaction sessions.\n\n## Theoretical Analysis and Solution\n\nThe authors analyze why standard model-based offline RL (specifically MOPO) amplifies the Matthew effect. The penalty term designed to reduce extrapolation error treats uncertainty as something to suppress—but in recommendation, uncertainty often reflects unexplored user interests rather than poor recommendations.\n\nDORL modifies the reward function to include two penalties:\n\n- **Uncertainty penalty (PU)**: Captures both epistemic uncertainty (model uncertainty via ensemble variance) and aleatoric uncertainty (data-dependent variance via Gaussian probabilistic modeling)\n- **Entropy penalty (PE)**: Measures the KL divergence between the behavior policy and a uniform distribution, penalizing states where the logging policy推荐 only a few items\n\nThe key insight is that the entropy penalty doesn't directly target specific actions; instead, it penalizes policies that operate in low-entropy states, indirectly encouraging diverse recommendations throughout the interaction trajectory. This provides \"counterfactual exploration\" within offline data.\n\n## Experimental Results\n\nEvaluated on KuaiRec and KuaiRand-Pure datasets against nine baselines, DORL achieves the highest cumulative reward by balancing single-round reward quality with interaction length. Key findings:\n\n- Model-based methods significantly outperform model-free approaches due to sample efficiency in sparse recommendation data\n- MOPO captures user interests better (higher single-round reward) but suffers from short interactions due to Matthew effect\n- DORL sacrifices some single-round reward for substantially longer interactions, resulting in maximum cumulative reward\n- The entropy penalty successfully reduces Majority Category Domination (MCD) while maintaining user satisfaction\n- DORL remains robust across varying user tolerance settings for repeated content\n\nThe paper provides valuable analysis showing that optimizing purely for immediate engagement harms long-term user retention, and that careful penalty design in offline RL can balance both objectives.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}