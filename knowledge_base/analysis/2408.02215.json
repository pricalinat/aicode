{
  "paper_id": "2408.02215",
  "title": "Amazon Query Understanding",
  "category": "ecommerce_evaluation",
  "year": 2024,
  "timestamp": "2026-03-01T13:52:31.136081",
  "summary": "# Summary: Exploring Query Understanding for Amazon Product Search\n\nThis paper presents Amazon's year-long research on how query understanding (QU) impacts product search ranking. The study addresses a gap in existing literature, as most prior work examined query understanding and ranking separately rather than exploring their interaction in real-world e-commerce search engines.\n\n## Background and Problem Context\n\nProduct search engines differ fundamentally from general web search. Over 80% of Amazon search queries contain fewer than 6 words, and more than 90% are attribute combinations like \"red shoes for running\" rather than natural language questions. The search space is also constrained to products with structured attributes (brand, color, size), unlike the unstructured data in web search. These characteristics make query understanding critically important for matching user intent to product offerings.\n\n## Query Understanding Components\n\nAmazon's query understanding system comprises several key modules. **Product Intention Detection** uses a multilingual transformer model trained on DistilBERT, consuming marketplace as a signal to support multiple languages with a single model. **Query Parsing** employs a Retrieval Augmented NER (RA-NER) system that retrieves relevant external knowledge before named entity recognition, addressing the challenge of knowledge-intensive entities like media titles. **Conversation and Session Understanding (CSU)** handles natural language queries beyond simple keyword searches, generating question intent classification, context switch detection, and question-to-keywords rewriting signals. The Q2K rewrite reduced irrelevance rate from 54.62% to 0.67% for single-turn queries.\n\n## QU-Based Ranking Features\n\nThe authors demonstrate that incorporating query understanding into ranking features significantly improves performance. By extracting attributes from queries and matching them against product catalog attributes, they created boolean features indicating brand match, color match, and product type match. A/B testing across six countries (US, UK, India, Canada, Japan, Germany) showed an average improvement of 0.79% in NDCG@16 when QU features were added, validating their effectiveness in production.\n\n## Multi-Task Learning Framework\n\nThe paper proposes a novel approach to evaluating and training ranking models using query segments. Rather than assessing performance only in aggregate, the method analyzes how models perform on specific query types (broad vs. specific, high vs. low frequency, branded vs. unbranded). This enables **dynamic task weight adjustment** during training, where the model increases weight on underperforming query segments. The approach outperformed manual grid search, achieving 8.18% improvement in NDCG@16 while consuming fewer computational resources by evaluating approximately 5 alpha choices versus over 100 parameter combinations.\n\n## Key Findings\n\nThe research demonstrates that query understanding is indispensable for product search optimization. The dynamic weighting mechanism particularly benefits revenue-related tasks, showing 150 basis points improvement over fixed-weight approaches. Query segment analysis revealed that low specificity queries (where customers know what they want) generate the most revenue, informing different treatment strategies across query types. The framework enables more nuanced ranking model development by understanding performance at the query segment level rather than only in aggregate.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}