{
  "paper_id": "1907.00937",
  "title": "Amazon Semantic Search",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper presents a deep learning approach to semantic product search: given a customer query, retrieve all semantically related products, not just lexically overlapping ones. It argues inverted-index lexical matching misses intent due to synonyms and hypernyms, morphological variants, and spelling errors, and can also return antonym-like mismatches such as latex free vs latex. Using large-scale customer behavior signals (purchases, impressions, random negatives), the authors train a Siamese semantic matching model and report offline gains of at least 4.7% in Recall@100 and 14.5% in MAP over strong semantic search baselines with the same tokenization, plus positive results from online A/B tests.\n\n*We study the problem of semantic matching in product search, that is, given a customer query, retrieve all semantically related products from the catalog.*\n\nKey technical choices are driven by product-search constraints like very short queries, sparse purchase positives, and strict latency for high QPS. The model shares embeddings between query and product, uses average pooling (instead of LSTM or GRU) to build fixed-size embeddings, applies batch normalization to address scale differences between query and product representations, then scores with cosine similarity; product embeddings are precomputed and retrieval uses nearest-neighbor search. Core contributions emphasized in the experiments include:\n- A 3-part hinge loss with built-in thresholds that separates purchased positives from two negative types: impressed but not purchased, and random; this addresses bimodal negative score distributions seen with a simpler 2-part hinge.\n- Bag-of-tokens tokenization that mixes word unigrams, word n-grams, and character trigrams to capture phrase cues and spelling robustness without heavy interaction-model costs.\n- Consistent hashing for out-of-vocabulary tokens so unseen terms that appear in both query and product can still align, with larger OOV bin sizes reducing harmful collisions.\n- Practical scaling via model-parallel training by splitting the embedding dimension across 8 GPUs and decomposing cosine similarity so communication is constant-sized per GPU.\n\nFor data, the system uses 11 months of search logs for training and 1 month for evaluation, initially sampling 54 billion queryâ€“product pairs and aggregating them down to about 650 million weighted rows. Evaluation separates matching (Recall@100 and MAP against a 1 million product sub-corpus over 20K queries) from ranking (metrics like NDCG and MRR over purchased and impressed sets), and many ablations show the 3-part hinge loss, richer tokenization, OOV hashing, and normalization choices drive matching gains. Online experiments across toys and games, kitchen, and pets show statistically significant lifts in conversion rate and revenue, while also noting the need for additional guardrails and filtering to meet production precision expectations.\n\n*In the future, we hope to improve the precision of our models and eliminate the need for additional heuristics to filter irrelevant results online.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/1907.00937_amazon_semantic_search.pdf"
}