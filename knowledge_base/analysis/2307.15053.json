{
  "paper_id": "2307.15053",
  "title": "Ndcg Off Policy",
  "category": "ecommerce_evaluation",
  "year": 2023,
  "timestamp": "2026-03-01T13:54:40.700367",
  "summary": "# On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top-n Recommendation\n\nThis KDD 2024 paper by Jeunen, Potapov, and Ustimenko from ShareChat critically examines whether (n)DCG can accurately approximate the gold standard outcome of online experiments in recommender systems. The authors formally derive the assumptions required for DCG to serve as an unbiased offline estimator, prove that normalization renders nDCG inconsistent with DCG, and validate their findings through large-scale experiments on a social media platform.\n\n## Theoretical Framework and Assumptions\n\nThe authors derive DCG from first principles as an importance sampling estimator for online reward. They identify five key assumptions needed for DCG to be an unbiased estimator: reward independence across trajectories, the position-based model (PBM) describing user scrolling behavior, reward independence across ranks, the examination hypothesis linking reward to exposure, and full support of the logging policy. Under these assumptions, DCG can be viewed as reweighting exposure allocated to items in specific contexts or as a de-biasing mechanism that estimates true quality from observed interactions and position bias.\n\nThe derivation shows that traditional DCG formulations assuming a logarithmic discount function are likely biased in real-world applications, though the general form in Equation 9 maintains theoretical guarantees for counterfactual evaluation when these assumptions hold.\n\n## The Normalization Problem\n\nThe paper's central theoretical contribution proves that nDCG is inconsistent with respect to DCG. While both metrics yield consistent rankings for individual samples in isolation (Lemma 5.1), they can produce inverted orderings when aggregated over multiple samples (Lemma 5.2). The authors provide a concrete counterexample showing two contexts where DCG ranks method R above R', but aggregated nDCG reverses this ordering. This occurs because normalization divides by ideal DCG, which varies across samples, breaking the mathematical relationship between the two metrics.\n\nThe practical implication is significant: researchers comparing recommendation methods using nDCG may select different best models than they would using DCG, even under ideal theoretical conditions where DCG provides unbiased estimates.\n\n## Empirical Validation\n\nThe authors conducted experiments on a large-scale short-video platform with over 40 million users, comparing offline metrics against online A/B test results. The correlation analysis revealed striking differences: unbiased DCG strongly correlated with online reward (Pearson r ≈ 0.98 for explicit feedback on first-level feed), while nDCG showed strongly negative correlation (r ≈ -0.91). Both directional alignment and sensitivity to detect statistically significant improvements favored DCG over nDCG across varying inverse propensity score clipping parameters.\n\nExperiments on MovieLens-1M data using the RecPack toolkit further demonstrated that approximately 25% of pairwise method orderings disagree between DCG and nDCG in typical offline evaluation setups.\n\n## Recommendations and Future Directions\n\nThe authors conclude that practitioners should prefer DCG over nDCG for offline evaluation and model selection. When normalized metrics are required, they propose a post-normalization approach that maintains consistency with DCG, though they caution against computing ideal DCG in practice due to partial observability of true item quality. The paper outlines a research agenda for relaxing the five core assumptions, including connections to contextual position-based models, cascading user behavior, and handling large action spaces through recent advances in off-policy evaluation.\n\nThe work suggests nDCG's widespread adoption stems from convenience rather than theoretical merit, and its practical utility for offline evaluation in real-world recommendation systems may be limited.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}