{
  "paper_id": "2305.11755",
  "title": "Visual Explainability",
  "category": "ecommerce_evaluation",
  "year": 2023,
  "timestamp": "2026-03-01T14:00:22.005934",
  "summary": "This survey reviews how visualization is used to explain recommendations, arguing that visual explanations can improve transparency, trust, and user understanding by leveraging fast perceptual processing and enabling interactive exploration. *The human visual system is the highest-bandwidth channel into the human brain [144].* It frames the area as visually explainable recommendation using charts and information-visualization idioms, distinct from image-highlight explanations over deep models, and positions the work at the intersection of explainable AI, interactive recommendation, and interactive explanation.\n\nThe authors conduct a systematic review of peer-reviewed English-language work (search run June 2021 across ACM Digital Library, IEEE Xplore, ScienceDirect, and Springer Link), narrowing 440 retrieved papers to 33 concrete tools that implement explainable recommendation with visualization and charts. They classify the tools along four dimensions: explanation aim (for example justification, transparency, trust, effectiveness, satisfaction), explanation scope (input user model, process algorithm, output items), explanation method (content-based, collaborative, social, hybrid), and explanation format grounded in the Munzner what-why-how framework. Key findings include that justification is the most common aim, transparency is less frequent, persuasiveness and scrutability are relatively underexplored, and no reviewed tools primarily target efficiency; almost all tools explain outputs, fewer explain process, and a newer subset opens user models for scrutiny and adjustment. *The purpose of visualization is insight [96].*\n\nOn explanation format, the review maps what data types are visualized (tables, networks, sets, and rarely geometry) to why tasks (for example identifying paths in networks, comparing similarity, summarizing features, identifying distributions) and how idioms and interactions implement them. Node-link diagrams and bar charts are the most common encodings, with tag clouds also prominent; Venn diagrams, heatmaps, scatterplots, treemaps, radar charts, pie charts, and maps appear less often and in more specific roles. Most tools support interactivity such as selection, filtering, coordinated multiple views, and navigation, but the authors caution that explanation complexity can overwhelm end users and may produce inappropriate trust if hard to understand. The paper closes with design suggestions that emphasize aligning idiom to task and data, keeping explanations simple but sufficient for accurate mental models, offering layered detail and personalization based on user characteristics, and adopting established visualization design frameworks, plus a future research agenda covering which visual explanations best serve which aims and methods, how to visualize and explain user models, and how much information and interaction is optimal for different users.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}