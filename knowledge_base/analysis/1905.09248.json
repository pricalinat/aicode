{
  "paper_id": "1905.09248",
  "title": "Long Seq Ctr",
  "category": "product_matching",
  "year": 2019,
  "timestamp": "2026-03-01T13:41:21.673204",
  "summary": "# Summary: Long Sequential User Behavior Modeling for CTR Prediction\n\nThis KDD 2019 paper from Alibaba addresses a critical challenge in industrial recommender systems and online advertising: handling extremely long sequential user behavior data (up to 1000+ items) for click-through rate prediction while meeting strict latency requirements. The authors present a co-design solution combining a novel serving system architecture with a memory-based deep learning model, achieving both superior model performance and practical deployability.\n\n## Problem Context and Challenges\n\nModern CTR prediction models increasingly rely on mining user interests from rich historical behavior data. Deep CTR models like DIN (Deep Interest Network) and DIEN (Deep Interest Evolution Network) have achieved state-of-the-art performance by modeling user behavior sequences. However, deploying these models in real-time serving systems presents two major obstacles when dealing with long sequential user behavior data.\n\nFirst, storage constraints become prohibitive. With over 600 million users in Alibaba's system and maximal behavior sequence length of 150, approximately 1 terabyte of storage is required. Scaling to length 1000 would потребовать 6 TB, creating unacceptable memory pressure. Second, latency constraints make real-time inference impractical. While DIEN achieves 14ms latency with 500 QPS for sequence length 150, extending to length 1000 increases latency to 200ms—far exceeding the 30ms limit in display advertising systems.\n\nThe paper demonstrates that longer behavior sequences directly correlate with better model performance. Experiments show that basic model with sequence length 1000 achieves 0.6% AUC improvement compared to length 100, indicating substantial value in capturing long-term user interests.\n\n## Solution Architecture: UIC and MIMN\n\nThe authors propose a dual-component solution addressing both system and algorithmic challenges. The User Interest Center (UIC) serves as a separate module that maintains the latest interest representation for each user. Critically, UIC updates depend only on real-time user behavior trigger events, not on traffic requests. This design makes UIC latency-free for real-time CTR prediction, reducing DIEN's latency from 200ms to 19ms with 1000-length sequences.\n\nThe Multi-channel user Interest Memory Network (MIMN) represents the algorithmic contribution. MIMN improves upon the Neural Turing Machine (NTM) architecture. The memory with two key innovations utilization regularization addresses unbalanced memory usage where popular items dominate updates. By regularizing the variance of write weights across memory slots, this mechanism pushes utilization to be balanced, enabling the fixed-size memory tensor to store more information from source behavior data.\n\nThe memory induction unit (MIU) captures high-order information by processing user interests through multi-channel GRUs. MIU selects top-k memory channels and updates them based on both current behavior embeddings and memorized information from NTM, effectively modeling the evolving process of user interests over time.\n\nMIMN follows the Embedding&MLP paradigm but with the user interest modeling sub-network implemented in UIC server. This incremental architecture eliminates the need to store raw behavior sequences during serving, reducing storage from 6TB to 2.7TB in Alibaba's system.\n\n## Experimental Results and Deployment\n\nThe paper validates the solution on both public datasets (Amazon Books and Taobao) and Alibaba's industrial dataset containing 12.2 billion instances. MIMN achieves state-of-the-art performance, outperforming Embedding&MLP, DIN, GRU4Rec, ARNN, RUM, and DIEN across all datasets. On the Taobao dataset, MIMN achieves 0.9179 AUC compared to DIEN's 0.9081. On the industrial dataset, MIMN improves AUC by 0.01 over DIEN.\n\nAblation studies confirm the contributions of both memory utilization regularization and the memory induction unit. Memory utilization regularization alone improves AUC from 0.9070 to 0.9112 on Taobao, while adding MIU further increases it to 0.9179.\n\nThe solution was deployed in Alibaba's display advertising system. Online A/B testing from March 30 to May 10, 2019 demonstrated significant business impact: 7.5% CTR improvement and 6% RPM (Revenue Per Mille) gain compared to DIEN.\n\nThe paper also shares practical deployment insights including synchronization challenges between UIC and RTP servers, handling big-sale data (like 11.11), warm-up strategies using pre-calculated user representations from 120 days of historical data, and rollback strategies storing daily snapshots to defend against training data pollution.\n\n## Significance\n\nThis work represents one of the first industrial solutions capable of handling long sequential user behavior data with lengths scaling to thousands in production systems. The co-design approach—optimizing both the serving system architecture and the learning algorithm together—proved essential for achieving both model accuracy and system efficiency in large-scale real-world applications.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}