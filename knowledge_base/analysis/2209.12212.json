{
  "paper_id": "2209.12212",
  "title": "Long Seq Ctr",
  "category": "product_matching",
  "year": 2022,
  "timestamp": "2026-03-01T13:38:58.057241",
  "summary": "# ETA-Net: Efficient Long Sequential User Data Modeling for Click-Through Rate Prediction\n\nThis paper presents ETA-Net (Efficient Target Attention Network), an end-to-end solution for modeling long user behavior sequences in CTR prediction systems. The method was developed by researchers at Alibaba and deployed in production at Taobao, one of the world's largest e-commerce platforms.\n\n## Problem and Motivation\n\nClick-Through Rate prediction is fundamental to recommender systems, and modeling longer user behavior sequences has shown promise in improving prediction accuracy. However, traditional approaches face significant computational challenges. The standard target attention mechanism used in models like DIN has complexity that scales linearly with sequence length (O(L·d)), making it impractical for industrial applications where latency and memory are strictly constrained. Two-stage methods like SIM and UBR4CTR emerged as state-of-the-art solutions by first training a retrieval model to truncate long sequences, then using those subsequences for CTR prediction. The critical limitation of this approach is that the retrieval model and CTR model are trained separately, causing representation mismatch that degrades final performance.\n\n## Technical Approach\n\nThe core innovation of ETA-Net is replacing expensive dot-product operations in standard target attention with low-cost bit-wise operations using SimHash. The method generates binary fingerprints for both target items and behavior items, then selects the top-K most relevant behavior items by comparing their Hamming distance through fast XOR operations. This reduces computational complexity from O(L·d) to O(L) + O(K·d) with K << L, enabling end-to-end training where the retrieval and CTR components are optimized together. The hashing function itself is parameter-free, while the multi-head attention structure that processes the retrieved items can be trained jointly with the base model, eliminating the representation mismatch problem inherent in two-stage methods.\n\nThe authors also propose a general system architecture for industrial deployment. Key optimizations include pre-computing item fingerprints offline (since SimHash requires no parameters) and using int64 integers to represent 64-bit binary fingerprints, which saves storage and memory. The architecture handles approximately 120,000 queries per second at peak traffic while maintaining response times under 20 milliseconds.\n\n## Experimental Results\n\nExtensive experiments were conducted on both public and industrial datasets. On the Taobao public dataset, ETA-Net achieved AUC improvements of 0.0046 over SIM and 0.006 over DIN with long sequences. On the industrial dataset containing 142 billion instances, ETA-Net outperformed SIM by 0.0034 AUC while actually reducing inference time (19ms vs 21ms). The online A/B test running on Taobao's production system demonstrated 6.33% CTR improvement and 9.7% GMV improvement compared to the previous baseline, and 1.8% CTR and 3.1% GMV improvement specifically over SIM. Ablation studies revealed that longer sequences (up to 2048 items) provide progressively better performance, and that fingerprint bit-lengths larger than twice the embedding size yield diminishing returns.\n\n## Conclusion and Impact\n\nETA-Net represents the first end-to-end solution capable of efficiently modeling long sequential user data at scale. The combination of SimHash-based retrieval and multi-head attention enables training stability while maintaining computational efficiency suitable for real-world deployment. The method now serves the main traffic of Taobao, providing recommendations to hundreds of millions of users across billions of items daily, demonstrating both practical viability and significant business impact in one of the world's largest e-commerce platforms.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}