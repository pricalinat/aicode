{
  "paper_id": "2410.06011",
  "title": "Text To Sql Survey",
  "category": "mini_program_service",
  "year": 2024,
  "timestamp": "2026-03-01T14:02:39.040281",
  "summary": "This survey reviews how large language models have reshaped text-to-SQL, framing the task as generating SQL from a natural language question plus a database schema. It argues that recent gains come from stronger LLM reasoning and generalization, and that prior surveys under-covered LLM-centered approaches, so it organizes the field around how models are used and trained. *Text-to-SQL translates natural language queries into Structured Query Language SQL commands, enabling users to interact with databases using natural language.*\n\nThe paper highlights core challenges that drive method design: ambiguity in language (including word segmentation and pragmatic context), large and diverse schemas that cannot always fit in a prompt, complex SQL with joins and nested logic, and the need for robustness and execution efficiency in real systems. It summarizes evaluation through Exact Matching Accuracy, Execution Accuracy, Valid Efficiency Score, and Test-suite Accuracy, emphasizing that exact string match can be too strict while execution-based metrics capture semantic correctness and performance tradeoffs. It also catalogs datasets by domain scope, interaction style, and SQL complexity, covering single-domain sets like ATIS and GeoQuery, cross-domain benchmarks like WikiSQL and Spider, conversational datasets like CoSQL and SParC, and augmented or robustness-focused variants such as Spider-SYN, Spider-DK, Spider-Realistic, ADVETA, TrustSQL, and BigTable-0.2k.\n\nMethodologically, it groups LLM-enhanced text-to-SQL into four training-strategy families and connects them to common design patterns like schema linking, guided decoding, decomposition, self-correction, and tool feedback:\n- Prompt engineering: zero-shot, few-shot, and reasoning-style prompting such as chain-of-thought, including example selection and structure-plus-content prompting.\n- Fine-tuning: full-parameter approaches and parameter-efficient methods like adapter-style tuning, often combined with instruction tuning, augmentation, or preference optimization.\n- Task-training: training text-to-SQL models from scratch or in task-specific pretraining regimes, including Transformer and mixture-of-experts directions.\n- LLM agents: multi-agent and tool-assisted systems that iteratively draft, check, execute, and repair SQL while handling schema mismatches and improving reliability.\n\nIn closing, the survey calls for more accurate, scalable, and cost-effective text-to-SQL that generalizes across domains and languages, especially under realistic enterprise constraints and noisy user inputs. *We are looking forward to future research being able to expand the methods that allow the generation to be smarter and more cost-effective.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}