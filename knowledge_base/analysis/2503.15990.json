{
  "paper_id": "2503.15990",
  "title": "Eckg Benchmark",
  "category": "product_matching",
  "status": "success",
  "summary": "ECKGBench presents a benchmark for testing large language model factuality in e-commerce using an e-commerce knowledge graph as a reliable ground-truth source. The paper argues that common evaluation approaches struggle in this domain due to unreliable question and answer construction, high evaluation cost when using human or model judges, and insufficient domain expertise, all of which matter because hallucinations can directly harm user experience and revenue. The authors propose a multiple-choice, minimal-token question answering setup to improve both reliability and efficiency, and release the benchmark code and data online.\n\nThe dataset is generated from a large Taobao-derived e-commerce knowledge graph with 4.8 million candidate triples, of which 2.1 million are annotated as true, spanning 15 relations split into common consumption relations and more abstract e-commerce concept relations. The construction pipeline includes relation templating to turn condensed graph relations into natural language predicates, plus a three-stage negative sampling workflow to create challenging but fair distractors: build a negative pool from graph false triples, filter by semantic similarity using embeddings, then apply fine-grained LLM-assisted selection with constraints on confusion value and diversity. Questions are assembled with a fixed instruction, an unfinished sentence derived from the triple, and four answer options, followed by a two-layer verification process using two LLMs to flag inconsistencies and human experts to review flagged items and apply safety-related filtering.\n\nThe benchmark evaluates models across two dimensions, common knowledge and abstract knowledge, under zero-shot and few-shot prompting, and also introduces a framework to probe base-model knowledge boundaries. It defines three knowledge types per question based on the probability of returning the correct option across multiple stochastic returns, then summarizes them with strict correct, precision, and recall metrics. Experiments across several closed and open models show generally modest accuracy levels in this e-commerce setting, stronger results on common than abstract knowledge, and a size scaling trend; ablations indicate prompt design is the largest driver of question quality, while the negative sampling workflow mainly improves stability as measured by lower answer inconsistency across resampled distractors. An efficiency study reports low average input and output token counts and generally short response times for batch evaluation, supporting the claim that the multiple-choice format avoids expensive judge-based scoring.\n\n*We propose a novel benchmark, ECKGBench, to bridge the evaluation gap.*\n\n*In general, LLMs have a low absolute value of accuracy score in most cases.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2503.15990_eckg_benchmark.pdf"
}