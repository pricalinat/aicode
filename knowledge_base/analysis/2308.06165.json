{
  "paper_id": "2308.06165",
  "title": "Task Conditioned Bert",
  "category": "mini_program_service",
  "status": "success",
  "summary": "## Summary\n\nThis paper proposes Task Conditioned BERT, a Transformer-encoder approach for task-oriented dialogue systems that jointly performs intent detection and slot filling while explicitly conditioning the encoder on the target inference tasks. The core claim is that adding task-specific tokens that act as attention hubs helps the model learn richer interactions than single-task setups, because self-attention across all layers becomes aware of what inference is being requested. Experiments show consistent gains from conditioning on more dialogue inference tasks, including improved joint intent and slot results on MultiWOZ 2.2 and strong performance on real Farfetch customer conversations.\n\nThe model family extends BERT-DST with special tokens and heads for different dialogue state tracking subtasks. BDST-I introduces an *[INTENT]* token with an intent classification head and adds intent loss to the slot-filling objective, motivated by measured correlations between intents and slot mentions (Cramer’s V reported as 0.62 on MultiWOZ and Farfetch-Costumers, 0.53 on Farfetch-Sim, and 1 on Sim-R). BDST-C handles categorical slots by adding slot-specific tokens and predicting values from an ontology via a classifier head, while span-based extraction remains for non-categorical slots; its loss weights categorical slots relative to total slots. BDST-J combines both intent and multiple slot conditioning, using a flexible input sequence that can include intent and multiple slot tokens together.\n\nEvaluation covers Sim-M and Sim-R (M2M), MultiWOZ 2.2, and two Farfetch datasets (a large simulated set for training/validation/testing and a small real user testing set for evaluation). Training largely follows BERT-DST settings (BERT base uncased, batch size 32, learning rate 2e−6, ADAM; 100 epochs except Farfetch at 20), and metrics include joint-goal accuracy for multi-turn DST plus intent accuracy and slot F1 for single-turn-style evaluation. On MultiWOZ 2.2, the conditioned models improve over BERT-DST baselines (with BDST-C and BDST-J around the high 40s joint-goal accuracy versus low-to-mid 30s for the base setup), and BERT-large yields small additional gains. On Farfetch real customer dialogues (trained only on simulated dialogues), BDST-I shows the strongest robustness, with intent accuracy reported at 95.4% and joint-goal accuracy at 71.0%, outperforming the compared baselines.\n\n*Conditioning the Transformer encoder on multiple target inferences over the same corpus, i.e., intent and multiple slot types, allows learning richer language interactions than a single-task model would be able to.*\n\n*Finally, we evaluated the proposed model in an online shopping assistant with both simulated and real costumer dialogues.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2308.06165_Task_Conditioned_BERT.pdf"
}