{
  "paper_id": "2006.02282",
  "title": "Personalized Semantic Retrieval",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper presents DPSR, an end to end embedding retrieval system for e commerce search that targets two common failure modes in candidate retrieval: semantic mismatch between query terms and item titles, and lack of personalization for the same query across different users. The authors focus on the retrieval stage rather than ranking, arguing that this stage produces a large share of user dissatisfaction and that embedding based nearest neighbor search can retrieve semantically related items that inverted indexes miss. They report deployment in JD.com search production since 2019 and claim measurable online impact, including improved search experience and especially strong gains on long tail queries.\n\n*DPSR model outperforms existing models*  \n*around 20% dissatisfaction cases of search traffic of JD.com, one of the largest e commerce search engine in the world, can be attributed to the failure of this stage.*\n\nAt the core is a supervised two tower model trained on click logs, where the item tower outputs a single normalized embedding from concatenated item features, and the query tower outputs multiple embeddings via a projection layer and separate MLP heads to represent different user intents for ambiguous queries. Query item scoring uses a soft dot product: a weighted sum of inner products across query heads, with weights from a softmax attention controlled by a temperature parameter; training uses a margin based hinge loss over positive clicked items and sampled negatives. Negative sampling is hybrid: random negatives shared within a batch for efficiency plus batch negatives formed by permuting positives inside the batch, with a mixing ratio that trades off popularity versus relevance; the model can also incorporate human supervision such as skipped items, curated hard negatives, and labeled bad cases.\n\nThe system design emphasizes industrial constraints: offline training produces separate query and item towers; offline indexing computes item embeddings for the full corpus and builds an approximate nearest neighbor index for fast top K retrieval; online serving computes query embeddings and performs embedding lookup under tens of milliseconds latency and high QPS. The implementation is built on TensorFlow Estimator and TensorFlow Serving, with optimizations for offline online consistency via a single C plus plus tokenizer reused in preprocessing, training, and serving; compressed input formats that separate user, item, and interaction files to reduce storage; and distributed training improvements that move embedding aggregation into parameter servers to reduce network bandwidth. Experiments include t SNE visualizations showing category structure in item embeddings and multi head disambiguation for polysemous queries, offline comparisons against BM2.5 and DSSM using top k hit rate, AUC, and latency, plus online A B tests and efficiency measurements showing millisecond level nearest neighbor retrieval and practical serving throughput on both CPU and GPU.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2006.02282_personalized_semantic_retrieval.pdf"
}