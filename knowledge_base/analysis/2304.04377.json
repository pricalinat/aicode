{
  "paper_id": "2304.04377",
  "title": "Paper",
  "category": "mini_program_service",
  "status": "success",
  "summary": "The paper *Delving into E-Commerce Product Retrieval with Vision-Language Pre-training* proposes a vision language pre-training approach to improve the retrieval phase of Taobao Search, where the system must retrieve tens of thousands of candidates from billions of products given a text query and product titles plus images. The core goal is better text to multimodal matching under large scale constraints, with a focus on efficiency, latency, and robustness in real online serving. *Our proposed method is employed as one retrieval channel of Taobao Search and serves hundreds of millions of users in realtime.*\n\nKey method ideas center on making image representations more retrieval suitable and scaling contrastive learning with many negatives:\n- A contrastive visual pre-training task (an adapted masked patch modeling loss) intended to emphasize discriminating between images rather than reconstructing patches, addressing limits of regression style image reconstruction objectives for retrieval. *We design a visual pre-training task based on contrastive learning, outperforming common regression-based visual pre-training tasks.*\n- Two large negative sampling schemes tailored for distributed, large corpus retrieval training: Cross Device Negative Sampling that uses product embeddings gathered across devices after embeddings stabilize, and Memory Bank Negative Sampling that uses a FIFO queue of prior batch embeddings without back propagation; both are applied only after later portions of training to avoid unstable representations.\n- A dual encoder style architecture with separate encoders for queries and products (title and image encoders forming a product encoder), using CLS outputs as sequence representations, plus a query product matching objective with a popularity related correction term to reduce over focus on popular items.\n\nThe paper also details deployment in a multi channel retrieval and multi stage ranking pipeline. Offline, product embeddings are exported and indexed with an ANN system (Proxima) using hierarchical clustering; queries are preprocessed via Chinese word segmentation and lexicographic sorting, hashed into query ids, and indexes plus model parameters are updated weekly and published to the online search service (HA3). Online relevance control handles restrictive terms like brands by applying Boolean constraints over inverted indexes to filter ANN results, reducing bad cases; in experiments, the method outperforms several baselines offline and shows ablation gains from the new visual task and negative sampling, and in online A B tests improves GMV by +0.46 percent and transactions by +0.39 percent while increasing candidate relevance by +1.20 percent, with added cost reported as about +2 ms latency and +12 GB memory.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2304.04377_paper.pdf"
}