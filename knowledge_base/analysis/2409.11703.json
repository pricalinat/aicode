{
  "paper_id": "2409.11703",
  "title": "Llm Api Classification",
  "category": "mini_program_service",
  "year": 2024,
  "timestamp": "2026-03-01T13:48:55.583964",
  "summary": "# Harnessing LLMs for API Interactions: A Framework for Classification and Synthetic Data Generation\n\nThis paper proposes a novel system that integrates Large Language Models for two key functionalities: classifying natural language inputs into corresponding API calls and automating the creation of sample datasets tailored to specific API functions. The research addresses the challenge of enabling non-technical users to interact with complex software systems through simple natural language commands, while also providing developers with efficient tools to evaluate different LLMs for customized API management.\n\n## System Architecture\n\nThe framework consists of two primary components. The **API Retrieval System** is an automated pipeline that receives natural language queries from users, uses LLMs to classify them into appropriate API modules and functions, executes the API calls, and returns results in a readable format. The system incorporates a load balancer for resource distribution and Redis caching for performance optimization. The **Dataset Generation Pipeline** employs batch prompting to generate 100 unique synthetic queries per batch, varying phrasing and contexts while adhering to predefined API hierarchies to ensure diversity without sacrificing alignment with intended API functions.\n\n## Experiments and Results\n\nThe researchers generated datasets for six API modules commonly found in web and mobile applications: Calculator, Notes, Weather, Email, Notification, and Calendar, plus a Routes-Not-Exist module for testing error handling. Using GPT-4-turbo, they created 1,300 labeled samples that underwent manual review achieving 99.9% accuracy. The evaluation tested six prominent LLMs using two metrics: Module Level Classification Accuracy (MLC-Acc) and Function Level Classification Accuracy (FLC-Acc).\n\nThe results revealed significant performance variability across models. GPT-4 achieved the highest performance with an overall MLC-Acc of 0.992 and FLC-Acc of 0.996, demonstrating exceptional ability to handle both module-level and function-level classification across all API categories. LLaMA3-70B followed closely with 0.964 MLC-Acc and 0.990 FLC-Acc, performing particularly well in Calculator, Email, and Notification modules. Gemini-1.5 showed strong performance at 0.957 MLC-Acc but struggled with nuanced queries in Email and Notification. Smaller models exhibited notably lower accuracy, with LLaMA3-8B achieving only 0.758 MLC-Acc and GPT-4o-mini at 0.854. The performance gap between LLaMA3-8B and LLaMA3-70B directly illustrates how model size impacts capacity to understand complex language patterns and generalize across different contexts.\n\n## Conclusions and Limitations\n\nThe findings demonstrate that LLMs, particularly GPT-4 and LLaMA3-70B, can effectively understand and translate natural language inputs into precise API calls, significantly improving user interaction and API management. The dataset generation framework provides a scalable solution for systematically assessing LLM capabilities across different API modules and functions. However, the authors note that larger LLMs' computational cost makes them less feasible for real-time applications in resource-constrained environments, suggesting future work could explore fine-tuning smaller models for API classification without significantly sacrificing accuracy.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}