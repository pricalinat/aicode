{
  "paper_id": "2008.00727",
  "title": "Deep Bayesian Bandits",
  "category": "product_matching",
  "year": 2020,
  "timestamp": "2026-03-01T14:42:21.222771",
  "summary": "This paper tackles the feedback-loop problem in continuously trained recommender systems, where the model’s own past choices bias future training data and push it toward greedy behavior that over-serves already-successful items while under-exploring new or uncertain ones. The authors frame display advertising recommendation as a contextual bandit problem and focus on practical exploration methods that rely on uncertainty, especially UCB and Thompson sampling, while addressing the fact that standard deep CTR models do not natively provide uncertainty estimates. Their core contribution is a computationally efficient deep Bayesian bandits approach that approximates posterior samples using a hybrid design: a multihead-like structure implemented by placing dropout only in the second-to-last layer so bottom layers are computed once and uncertainty sampling is cheap enough for production constraints.\n\nThey compare exploration strategies and posterior-approximation techniques in both an offline simulator and a large-scale online A/B setting. Offline, they build a simulation environment from the public ADS-16 dataset (120 users, 300 ads) by converting 1–5 star ratings into binary clicks, using rich user and ad features (one-hot encoded), holding out 5 ads per user, and measuring both accumulated average CTR (reward) and PR-AUC (predictive quality). Results show a consistent trade-off: more exploration can improve long-term reward but can reduce PR-AUC due to training on more biased or less representative data, and UCB generally yields higher reward than Thompson sampling. Bootstrap UCB achieves the largest CTR gains but is the most expensive; cheaper approximations (multihead variants, SGD randomness, dropout, and the proposed hybrid) tend to reduce reward, though the hybrid can become competitive when warm-started with longer offline training because dropout-based networks converge more slowly.\n\nIn online experiments using a modified wide-and-deep model, the hybrid approach is validated offline to match production-like predictive performance, then tested on 2% of production traffic for two weeks, with unbiased evaluation using randomly served ads from 1% traffic. Exploration cost is reported as small in serving and training throughput and roughly flat revenue, and the authors do not see immediate product-metric gains; however, models trained on data collected by the hybrid exploratory policy show improved predictive metrics (higher RCE and ROC-AUC than the greedy control), suggesting exploration improves data quality for learning over time. They also contrast this with prior ϵ-greedy testing that sharply increased negative engagement, while the hybrid UCB-style directed exploration avoids that degradation.\n\n*We combine the ideas of bootstrapping and dropout with the following objectives: (1) dynamically assign membership of each data point to subsets without storing the mask*\n\n*We did not observe direct or immediate benefit in production metrics; however, the improvement in model performance will potentially lead to revenue gain in the long-run.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}