{
  "paper_id": "2508.04032",
  "title": "Serendipity Llm Kg",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper tackles the filter bubble problem in industrial recommender systems, where repeated recommendation and feedback cycles push users toward increasingly homogeneous content and reduce satisfaction. It proposes using large language models to infer users potential new interests more reliably and then operationalize those interests in a production-friendly retrieval pipeline to improve serendipity while keeping relevance.\n\nThe approach is a two-stage framework. First, an LLM dynamically builds a user knowledge graph from static profile signals such as age and gender plus recent search queries, then performs constrained two-hop reasoning to move from observed behaviors to underlying core demands and finally to plausible new interests; a multi-agent debate procedure is used to reduce reasoning errors and irrelevant outputs. Second, to meet latency constraints, inferred interests are generated nearline and cached for a time window, and a multi-task dual-tower u2i retrieval model is trained to combine strong conversion performance with interest alignment using a contrastive learning loss alongside standard binary cross-entropy.\n\n*Nearline computing is a compromise between real-time online processing and batch offline computation, achieving a balance between efficiency and timeliness.*\n\nExperiments are reported on the Dewu app with an online A/B test on 10 percent of users and an added serendipity retrieval channel that generates up to 16 interests per user and retrieves 40 items per interest. Offline, InterestGPT is produced by distilling DeepSeek-R1 outputs into a smaller model via supervised fine-tuning, with a test-set score distribution dominated by the highest quality grade. Online, the method improves novelty and engagement metrics over the baseline, including higher exposure novelty and click novelty, plus smaller gains in view duration, click-through rate, and interaction penetration; the paper argues this added channel can also help other retrieval channels pick up new interest signals faster and weaken feedback loops.\n\n*Online experiments demonstrate that the method increased the exposure novelty rate by 4.62%, the click novelty rate by 4.85%.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2508.04032_serendipity_llm_kg.pdf"
}