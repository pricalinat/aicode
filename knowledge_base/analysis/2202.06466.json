{
  "paper_id": "2202.06466",
  "title": "Explainable Eval",
  "category": "ecommerce_evaluation",
  "year": 2022,
  "timestamp": "2026-03-01T14:01:27.471539",
  "summary": "This paper surveys how to evaluate explanations in explainable recommender systems, focusing on the difficulty of judging explanation quality when ground truth is hard to define and human perceptions matter. It positions evaluation as central for model selection and benchmarking, and argues the literature is fragmented across many independently proposed strategies with different prerequisites and tradeoffs. The authors synthesize over 100 papers and aim to provide taxonomies, pros and cons, and practical guidance for choosing evaluation approaches.\n\n*In specific, we introduce the main evaluation perspectives and methods in the previouswork.*  \n*In addition,quantitativemetricsare usually very efficient, since most of them can be computed in a short time.*\n\nThe survey organizes evaluation goals into four major perspectives tied to different stakeholders: users, providers, and model designers. Effectiveness asks whether explanations help users make better or faster decisions and improve satisfaction or trust; transparency asks whether explanations reveal internal model mechanisms to support debugging and understanding; persuasiveness focuses on whether explanations increase user interactions such as clicks or purchases (often aligned with provider goals, not necessarily honest model reflection); and scrutability evaluates whether explanations truly correspond to the produced recommendation results, emphasizing faithfulness of explanation-output relations.\n\nIt then groups evaluation methods into four categories and discusses their strengths and shortcomings. Case studies visualize or present example explanations (including attention heatmaps, attribute matches, reasoning paths, and natural-language templates or generations) and are intuitive but biased and hard to compare quantitatively. Quantitative metrics approximate utilities with computable scores, including NLG-style overlap metrics like BLEU and ROUGE for review-based explanations, diversity and coverage measures such as USR, FCR, and FD, control and alignment measures like FMR, counterfactual-style faithfulness measures like probability of necessity and probability of sufficiency, performance-shift tests when removing purportedly key features or history items, and explainability coverage metrics such as MEP and MER. Crowdsourcing directly captures human judgments via public datasets, hybrid dataset injection, or fully constructed studies, but is costly and sensitive to annotation design and quality control (e.g., voting schemes or inter-annotator agreement). Online experiments compare real user behavior (e.g., with and without explanations) and are viewed as more reliable but expensive and potentially disruptive; the paper concludes with guidelines emphasizing reliability for high-stakes domains, budget-driven tradeoffs for general settings, and opportunities like learning better evaluation functions from small labeled data and building broader benchmarking datasets, plus exploring more task-agnostic evaluation directions.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}