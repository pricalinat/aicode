{
  "paper_id": "2508.04266",
  "title": "Shoppingbench",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T14:02:20.497994",
  "summary": "ShoppingBench presents an end-to-end benchmark for evaluating LLM-based shopping agents on realistic, intent-grounded e-commerce tasks that go beyond simply finding or buying an item. The paper argues that real users often have multi-constraint goals such as applying vouchers, staying within budgets, combining items from a single seller, or satisfying service requirements, and that existing e-commerce agent benchmarks and QA-style datasets do not adequately test these behaviors in an interactive setting. To close this gap, the authors introduce a large simulated shopping environment and a dataset of user instructions designed to steadily increase intent complexity.\n\n*Experimental results demonstrate that even state-of-the-art language agents such as GPT-4.1 achieve absolute success rates under 50% on our benchmark tasks*  \n*We provide a large-scale shopping sandbox that serves as an interactive simulated environment, incorporating over 2.5 million real-world products*\n\nAt the core is a shopping sandbox built from Lazada-sourced product data, paired with 3,310 simulated user instructions spanning four intent types with increasing difficulty: ProductsFinder, Knowledge, Multi-products seller, and Coupon and Budget. The benchmark is framed as a tool-using sequential decision process, where an agent interacts via a fixed API tool pool that supports product search, viewing product details, web knowledge lookup, budget or discount calculation via code execution, recommendation, and termination. For automatic evaluation, the paper defines intent-specific constraint scores and aggregates them into two main metrics: Absolute Success Rate for strict completion and Cumulative Average of product relevance based on title, price, and feature matching.\n\nResults show substantial difficulty across models: the paper evaluates 17 agents and reports that GPT-4.1 achieves the best overall average performance but still falls below 50% ASR overall, with a steep drop on the Coupon and Budget intent. The authors analyze failures from sampled GPT-4.1 trajectories and categorize common error types such as attribute mismatch, missing products, unsatisfied constraints, knowledge errors, and occasional metric sensitivity. They also introduce a trajectory distillation approach: GPT-4.1 generates tool-use trajectories for training instructions, low-quality trajectories are filtered via the automatic metric, and a smaller agent based on Qwen3-4B is trained with supervised fine-tuning and reinforcement learning using GRPO and tool-based rewards, reaching competitive performance relative to the GPT-4.1 agent. Additional analysis highlights which tool behaviors correlate with success, shows that removing web search significantly hurts knowledge tasks, and suggests that explicit reasoning helps more on complex budget and voucher problems than on simpler single-product retrieval.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}