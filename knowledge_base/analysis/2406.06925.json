{
  "paper_id": "2406.06925",
  "title": "Nonautoregressive Bundle",
  "category": "product_matching",
  "year": 2024,
  "timestamp": "2026-03-01T14:41:05.210841",
  "summary": "This paper studies personalized bundle generation: selecting a size K set of items for a user from a larger candidate set, where items must both match user preference and be mutually compatible. It argues that prior work often treats a bundle as an ordered sequence and generates items autoregressively, which injects an ordering inductive bias despite bundles being order-invariant, and also increases inference latency because items are predicted step by step. The authors propose a non-autoregressive alternative that predicts the whole bundle in one shot while explicitly modeling compatibility.\n\nThe proposed encoder-decoder framework, BundleNAT, combines two signals in a compatibility-aware encoder: (1) a preference signal learned from user-item interactions via a standard recommender (implemented with MF-BPR, noted as replaceable by other models), and (2) a compatibility signal derived from an item co-occurrence graph built from bundle-item affiliations and encoded with a GNN. These are fused and passed through a self-attention encoder to learn global dependency patterns. The one-shot decoder starts from a copied summary of the encoder output (mean pooling) to avoid from-scratch decoding issues and reduce multi-modality problems, then uses attention blocks and an MLP projection to output a distribution over all items; training uses an order-agnostic cross-entropy objective that searches for the best permutation alignment via Hungarian matching. *which can effectively output the targeted bundle in one-shot without relying on any inherent order.*\n\nExperiments on three constructed datasets from Youshu and Netease compare against POP, BPR, NCF, UltraGCN, SASRec, and bundle-specific baselines Exact-k and BYOB using Precision@K, Precision+@K, and Recall@K under an 80/20 split. BundleNAT is reported to achieve large absolute gains over the second-best baseline, including up to 35.92 points in Precision, 10.97 points in Precision+, and 23.67 points in Recall on average across datasets, with strong results in the main tables (for example, Precision@5 of 0.8091 on Youshu K=5 and 0.8551 on Netease K=5). Ablations show compatibility encoding is especially important, the copy mechanism materially improves results, and mean pooling beats max pooling; efficiency analysis reports much lower inference latency than Exact-k and BYOB and better overall training time scaling, especially as bundle size increases. The paper concludes that non-autoregressive, order-invariant modeling is a better fit for bundle generation, and suggests future work on richer item relations and dynamic, time-varying scenarios. *BundleNAT significantly outperforms the existing state-of-the-art methods by a large margin.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}