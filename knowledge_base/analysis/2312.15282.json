{
  "paper_id": "2312.15282",
  "title": "Causal Forecasting",
  "category": "product_matching",
  "year": 2023,
  "timestamp": "2026-03-01T14:51:27.803776",
  "summary": "This paper presents a causal forecasting approach for pricing, aiming to forecast demand under future price or discount policies that were not observed in training, which is the key challenge in off-policy settings for retail pricing decisions. The authors propose the DML Forecaster, an opinionated framework that prioritizes predictive accuracy while using causal inference machinery to better handle policy shifts, and they contribute both a synthetic but realistic simulation setup and a real-world evaluation based on commonly occurring natural experiments such as cyber week discount changes. *Our paper bridges the gap between forecasting and causal inference in the context of demand forecasting for pricing.*\n\nThe core idea adapts Double Machine Learning to a continuous treatment variable, weekly average discount, and combines three transformer-based submodels: an outcome model that predicts demand without using future discounts, a treatment model that predicts discounts from covariates, and an effect model that estimates how discounts causally shift demand after residualizing both outcome and treatment. This orthogonalization is meant to reduce regularization bias that arises when discounts are treated like ordinary covariates in standard forecasting, and the final forecast combines the causal effect with the baseline outcome forecast to produce demand under a desired future discount path, including a formulation tied to price elasticity. *The benefit of orthogonalization is that we account for regularization bias.*\n\nEmpirically, the DML Forecaster is roughly on par with strong transformer forecasting baselines in on-policy scenarios, but it shows clear gains in off-policy scenarios where discount distributions shift, both in fully controlled synthetic experiments and in real-world cyber week tests. In the synthetic study over thousands of simulated items, DML variants improve off-policy demand metrics relative to a naively causal transformer, with the advantage increasing for more price-elastic items, while ablations show simplified two-stage variants that skip treatment residualization perform notably worse for off-policy behavior and effect estimation. On real-world data, the DML Forecaster outperforms transformer and SARIMAX baselines on off-policy cyber week demand error, MAE, and MSE, while differences on-policy are smaller and sometimes favor the purely forecasting transformer; additional appendices compare cross-fitting versus sample-splitting, detail dataset construction and features, and report that cross-fitting can modestly improve off-policy performance at a meaningful training-time cost. The paper closes by noting evaluation difficulty in pricing counterfactuals and suggesting future extensions such as probabilistic treatment modeling, inverse propensity weighting, more flexible outcome models, and multivariate forecasting.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}