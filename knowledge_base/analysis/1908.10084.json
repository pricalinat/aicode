{
  "paper_id": "1908.10084",
  "title": "Sentence Bert",
  "category": "product_matching",
  "status": "success",
  "summary": "Sentence-BERT, or SBERT, adapts pretrained BERT and RoBERTa into siamese and triplet network architectures that produce fixed-size sentence embeddings suitable for fast cosine-similarity comparison. The paper motivates SBERT by showing that standard BERT cross-encoder usage is accurate for sentence-pair tasks but computationally impractical for large-scale semantic search and clustering, since all sentence pairs must be jointly encoded. As an example, exhaustively comparing 10,000 sentences requires about 49,995,000 inferences and roughly 65 hours on a V100 GPU, while SBERT reduces this to computing 10,000 embeddings in about 5 seconds plus a small cost for similarity computations, enabling efficient retrieval with indexing.\n\n*SBERT is computationally efficient.*  \n*This reduces the effort for finding the most similar pair from 65 hours with BERT to about 5 seconds with SBERT.*\n\nArchitecturally, SBERT adds a pooling layer over token embeddings to create a single vector per sentence and explores three pooling strategies, with MEAN pooling as the default. The authors fine-tune SBERT mainly on Natural Language Inference data, using either a classification objective over paired sentence embeddings or a regression objective that directly optimizes cosine similarity, and they also use a triplet loss setup for datasets that naturally form anchor, positive, and negative examples. An ablation study highlights that pooling choice matters more for regression than for NLI classification, and that element-wise difference features are especially important in the classification setup, while some commonly used concatenation components can hurt performance.\n\nEmpirically, SBERT substantially improves semantic textual similarity performance over naive BERT sentence embeddings and prior sentence-embedding baselines: across seven STS tasks, SBERT models report much higher average Spearman correlations than average GloVe, average BERT embeddings, and the CLS vector, and they outperform or closely match established methods like InferSent and Universal Sentence Encoder on many benchmarks. On the STS Benchmark, SBERT fine-tuned with a regression objective reaches performance close to cross-encoder BERT, with additional experiments showing benefits from pretraining on NLI before STS fine-tuning. The paper also evaluates harder similarity settings such as Argument Facet Similarity, where SBERT performs well in cross-validation but drops more in cross-topic generalization than BERT, and a Wikipedia section-based triplet task where SBERT outperforms a BiLSTM triplet baseline. For transfer-style evaluations via SentEval, SBERT embeddings are competitive and often best across tasks, while the work emphasizes that SBERTs core value is scalable similarity search; it also reports practical speed gains from smart batching that reduces padding overhead, improving throughput on both CPU and GPU.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/1908.10084_sentence_bert.pdf"
}