{
  "input": {
    "kind": "file",
    "filePath": "/Users/rrp/Documents/aicode/data/papers/product_matching/2405.12442_concept_recommendation_llm.pdf",
    "timeoutMs": 120000,
    "length": {
      "kind": "preset",
      "preset": "long"
    },
    "maxOutputTokens": null,
    "model": "auto",
    "language": {
      "mode": "auto"
    }
  },
  "env": {
    "hasXaiKey": false,
    "hasOpenAIKey": false,
    "hasOpenRouterKey": false,
    "hasApifyToken": false,
    "hasFirecrawlKey": false,
    "hasGoogleKey": false,
    "hasAnthropicKey": false
  },
  "extracted": {
    "kind": "asset",
    "source": "/Users/rrp/Documents/aicode/data/papers/product_matching/2405.12442_concept_recommendation_llm.pdf",
    "mediaType": "application/pdf",
    "filename": "2405.12442_concept_recommendation_llm.pdf"
  },
  "prompt": "<instructions>\nHard rules: never mention sponsor/ads; never output quotation marks of any kind (straight or curly), even for titles.\nNever include quotation marks in the output. Apostrophes in contractions are OK. If a title or excerpt would normally use quotes, remove them and optionally italicize the text instead.\nYou summarize files for curious users.\nSummarize the file content below.\nBe factual and do not invent details.\nWrite a detailed summary that prioritizes the most important points first, followed by key supporting facts or events, then secondary details or conclusions stated in the source.\nParagraphs are optional; use up to 3 short paragraphs. Aim for 2-4 sentences per paragraph when you split into paragraphs.\nFormat the answer in Markdown.\nUse short paragraphs; use bullet lists only when they improve scanability; avoid rigid templates.\nIf a standout line is present, include 1-2 short exact excerpts (max 25 words each) formatted as Markdown italics using single asterisks only. Do not use quotation marks of any kind (straight or curly). Remove any quotation marks from excerpts. If you cannot format an italic excerpt, omit it. Never include ad/sponsor/boilerplate excerpts and do not mention them.\nDo not use emojis.\nTarget length: around 4,200 characters (acceptable range 2,500-6,000). This is a soft guideline; prioritize clarity.\nMatch the dominant source language. If you can't confidently detect it, use English.\nFinal check: remove any sponsor/ad references or mentions of skipping/ignoring content. Remove any quotation marks. Ensure standout excerpts are italicized; otherwise omit them.\nReturn only the summary.\n</instructions>\n\n<context>\nFilename: 2405.12442_concept_recommendation_llm.pdf\nOriginal media type: application/pdf\nProvided as: text/markdown\nExtracted content length: 73,717 characters. Hard limit: never exceed this length. If the requested length is larger, do not padâ€”finish early rather than adding filler.\n</context>\n\n<content>\nLearning Structure and Knowledge Aware Representation with\nLarge Language Models for Concept Recommendation\nQingyaoLi WeiXia KounianhuaDu\nly890306@sjtu.edu.cn xiawei24@huawei.com kounianhuadu@sjtu.edu.cn\nShanghaiJiaoTongUniversity HuaweiNoahâ€™sArkLab ShanghaiJiaoTongUniversity\nShanghai,China Shenzhen,China Shanghai,China\nQijiZhang WeinanZhang RuimingTang\nzqjpeter@sjtu.edu.cn wnzhang@sjtu.edu.cn tangruiming@huawei.com\nShanghaiJiaoTongUniversity ShanghaiJiaoTongUniversity HuaweiNoahâ€™sArkLab\nShanghai,China Shanghai,China Shenzhen,China\nYongYuâˆ—\nyyu@apex.sjtu.edu.cn\nShanghaiJiaoTongUniversity\nShanghai,China\nABSTRACT learningcontenttothem.Thiscontentcanvaryinscope,including\nConcept recommendation aims to suggest the next concept for bothbroadcourserecommendationsanddetailedexercisesugges-\nlearnerstostudybasedontheirknowledgestatesandthehuman tions[18,19].Thisarticlefocusesonknowledgeconceptrecommen-\nknowledgesystem.Whileknowledgestatescanbepredictedusing dations,whichalignwiththeusersâ€™needstoacquirespecificskills\nknowledgetracingmodels,previousapproacheshavenoteffec- orunderstandcertainconceptswhentheycometotheplatform.\ntivelyintegratedthehumanknowledgesystemintotheprocess Toenhancelearnersâ€™masterylevels,thestrategyforplanning\nofdesigningtheseeducationalmodels.Intheeraofrapidlyevolv- theirconceptlearningpathsshouldbebasedontwokeytypesof\ningLargeLanguageModels(LLMs),manyfieldshavebegunusing information:1)Thelearnerâ€™sknowledgestate(learnerside),which\nLLMstogenerateandencodetext,introducingexternalknowl- referstotheirproficiencyintheconcepts-whattheyhavemastered\nedge.However,integratingLLMsintoconceptrecommendation wellandwhattheyhavenâ€™t.Thepredictionofalearnerâ€™sknowl-\npresentstwourgentchallenges:1)Howtoconstructtextforcon- edgestatecanbeachievedthroughknowledgetracingmodels,and\nceptsthateffectivelyincorporatethehumanknowledgesystem? manyrelatedmodelshavebeenproposedinthisarea[21,22,29].\n2) How to adapt non-smooth, anisotropic text encodings effec- 2)Thehumanknowledgesystem(conceptside),whichisthese-\ntivelyforconceptrecommendation?Inthispaper,weproposea manticmeaningsandstructuralrelationshipsbetweenconcepts.\nnovelStructureandKnowledgeAwareRepresentationlearning Humanteachersoftenmakeeducationalplansbasedonagoodun-\nframeworkforconceptRecommendation(SKarREC).Weleverage derstandingofeachconcept.Forinstance,â€œmultiplicationâ€logically\nfactualknowledgefromLLMsaswellastheprecedenceandsucces- followsâ€œadditionâ€inlearningprogressionduetoitsconceptual\nsionrelationshipsbetweenconceptsobtainedfromtheknowledge dependencyonâ€œadditionâ€.Manypreviousapproacheshaveintro-\ngraphtoconstructtextualrepresentationsofconcepts.Further- ducedrelationshipsbetweenconceptsthroughknowledgegraphs,\nmore,weproposeagraph-basedadaptertoadaptanisotropictext yetthesemethodsoverlooktheintrinsicsemanticsoftheconcepts.\nembeddingstotheconceptrecommendationtask.Thisadapteris Consequently,whiletheycapturehowlearnersinteractwithcon-\npre-trainedthroughcontrastivelearningontheknowledgegraph ceptsandthedependenciesbetweenthem,theyfailtointegrate\ntogetasmoothandstructure-awareconceptrepresentation.Then, concrete,factualknowledgeabouttheconcepts,leavingpotential\nitâ€™sfine-tunedthroughtherecommendationtask,formingatext-to- deepersemanticconnectionsbetweenconceptsunexplored.\nknowledge-to-recommendationadaptationpipeline,whicheffec- Nowadays,thedevelopmentofLargeLanguageModels(LLMs)\ntivelyconstructsastructureandknowledge-awareconceptrepre- providesawayofintroducingthesemanticmeaningsofthecon-\nsentation.Ourmethoddoesabetterjobthanpreviousadaptersin cepts.Inproductrecommendation,manystudieshaveutilizedLLMs\ntransformingtextencodingsforapplicationinconceptrecommen- togenerateandencodetextualinformationtoassistintherepre-\ndation.Extensiveexperimentsonreal-worlddatasetsdemonstrate sentationsofitemsforrecommendationpurposes.However,intro-\ntheeffectivenessoftheproposedapproach. ducingtheknowledgefromLLMstorepresentconceptsfacestwo\nchallenges:C1)Howcanconceptsberepresentedinawaythat\n1 INTRODUCTION includesinformationfromthehumanknowledgesystem?Adirect\nwayistousethetextoftheconceptâ€™sname,butsomeconcepts\nOnlineeducationisincreasinglybecomingavitalmeanforindivid-\nmayhaveambiguousdefinitionsindifferentlearningcontexts.For\nualstoacquireknowledgeandskills.Toservelearnersâ€™knowledge\ninstance,â€œTableâ€canmeanapieceoffurnitureineverydaylifebut\nacquisition,onlineeducationplatformsrecommendappropriate\nalsoreferstoatabulardatarepresentationindatascience.Hence,\nâˆ—Correspondingauthor. constructingappropriatetextualinformationtorepresentaconcept\n4202\nyaM\n12\n]RI.sc[\n1v24421.5042:viXra\n\nConferenceâ€™17,July2017,Washington,DC,USA QingyaoLi,etal.\nThe next concept to learn is \"multiplication\". toasknowledgespace).Thus,thefinalrepresentationsencompass\nthehumanknowledgesystemfromLLMsaswellastheprecedence\nHuman Knowledge System Knowledge State\nandsuccessionrelationships(structureinformation)ofconceptsin\nGraph-based Adaptation\ntheknowledgegraph.Finally,therecommendationismadebased\nContrastive Learning\nonthelearnerâ€™sknowledgestateandthehumanknowledgesystem\ninformationcontainedintheconceptsâ€™representations.\nStructure and Knowledge aware Concept Interpretation Knowledge Tracing\nI it n s t p h r e e d c e o c n e te s x s t o o rs f \" a H n i d st o su g c ra c m es a so s r T s a , b it l e lik o e r l y G p ra e p rt h a \" in a s n t d o c p o r n e s s i e d n e t r in in g g ...... Ourcontributionsaresummarizedasfollow:\ndata in tabular or graphical form for visualizing and analyzing\ninformation. â€¢ WeproposeStructureandKnowledgeAwareRepresentation\n......\nLLM learningframeworkforconceptRecommendation(SKarREC).\nLearning History Bybringingopen-worldfactualknowledgefromLLMsintocon-\nceptsâ€™representation,weintegratepotentialdeepersemantic\n: What concept should i learn next?\nconnectionsforrecommendation.Tothebestofourknowledge,\nwearethefirsttointroducelargelanguagemodelâ€™sknowledge\nFigure1:Theconceptrecommendationstrategyofcombin- intoconceptrecommendationscenario.\ningthelearnerâ€™sknowledgestateandthehumanknowledge â€¢ Weutilizethestructuralrelationshipsbetweenconceptstohelp\nsystem.Theknowledgestateisestimatedbyknowledgetrac- LLMsgeneratetheirexplanation,whichresolvestheambiguity\ning,andthehumanknowledgesystemismodeledbygraph- probleminconceptexplanation.\nadaptedencodingoftheenhancedtextofeachconcept. â€¢ Weconstructanovelgraph-basedadapterfortextencodingadap-\ntation.Theadapterispre-trainedthroughcontrastivelearning\nontheknowledgegraph,gettingasmoothandstructure-aware\nisakeyissue.C2)Howtoadaptthelanguagemodelâ€™sencodings\nconceptrepresentationtohelpwithconceptrecommendation.\nofthetexttotheconceptrecommendationproblem?Researchon\nâ€¢ Wehaveconductedextensiveexperimentstoshowtheeffective-\nlanguagemodelsindicatesthattextencodingsproducedbythem\nnessofourmethods.Resultsdemonstratethattheproposedap-\nhavenon-smoothandanisotropicdistributions[13],meaningthat\nproachoutperformspreviousbaselinesandimprovestheknowl-\nsentencessimilarinthesemanticwayarenotsimilarintheem-\nedgeusageefficiencyoftextinformation.\nbeddingspace,whicharenotsuitablefordirectuseindownstream\ntasks.Therefore,previousworksoftenestablishanadaptertotrans-\nferthetextencodingstosuittherecommendationtask.However, 2 RELATEDWORK\ntheseadapterstypicallyhavetwodrawbacks:i)Theyarestruc-\n2.1 ConceptRecommendation\nturallysimple,oftenbuiltwithoneorcombiningmultipleMLPsin\namixtureofexperts(MoE)way,andii)Theylackatrainingtarget Inconceptrecommendation,priorresearchhaspredominantlyre-\ndesign.Theyareoftentrainedend-to-endalongsidethefinalrecom- volvedaroundtwodistinctproblemformulations.Thefirstformula-\nmendationtask,lackingatailedtrainingobjectivethatguarantees tionisgroundedintheconstructionofsimulators,whichprimarily\ntheeffectivenessofthetextadaptation.Therefore,usingLLMsâ€™ employsimulatorstoemulatetheprogressionofalearnerâ€™sknowl-\nknowledgetointroducethehumanknowledgesysteminconcept edge and utilize the simulator to evaluate the recommendation\nrecommendationrequiresconstructingconcepttextsthatinclude performance.Owingtotheinteractivenatureofthesimulator,rein-\nthesemanticmeaningsaswellasrelationshipsbetweenconcepts forcementlearning(RL)techniqueshavedemonstratedremarkable\nandbuildingatextadapterthattransformstextrepresentations effectivenessinthiscontext,likeCSEAL[20],GEHRL[16],etc.Such\nintoaspacethatbetterassistsconceptrecommendation. methodsusuallytakethelearnerâ€™spromotionfromthesimulatoras\nInlightofthechallenges,weproposeStructureandKnowledge therewardtotrainRLmodels,whichnaturallybeatothernon-RL\nAwareRepresentationlearningframeworkforconceptRecommen- methods.\ndation(SKarREC).AsshowninFigure1,weutilizetheknowledge Conversely, the focus of this paper diverges toward the sec-\nfrom the concept side (external factual knowledge provided by ondproblemformulation,whichconcernsnext-itemprediction.\nLLMsandrelationshipsbetweenconceptsprovidedbytheknowl- Inthissetting,theprimaryobjectiveofthemodelistoaccurately\nedgegraph)andthelearnerside(theknowledgestatepredictedby alignwiththeexistinglearningpathsoflearnersasreflectedinthe\nknowledgetracing)toassistsequencemodelsinrecommending dataset,whichtakesthelearnersâ€™choiceasthebestchoiceinstead\nconcepts.Specifically,forexpressiveconceptsinterpretation(C1), ofthesimulatorâ€™sprediction.Inthissetting,thegraph-basedmeth-\nwedevelopastructureandknowledge-awaremethodthatuses ods mainly take advantage of using the structured information\nLLMstogenerateconceptexplanationwithambiguityresolution. betweenlearnersandconcepts.Forexample,ACKRec[8]lever-\nCombiningtherelationshipbetweenconcepts,weconstructthetext agesbothcontentinformationandcontextinformationtolearnthe\ndescriptionofeachconcept,whichisfedintolanguagemodelsâ€™en- representationofentitiesviaagraphconvolutionnetwork.The\ncodersfortextencoding.Toeffectivelyadapttextencodingstailed drawbackofthiskindofmethodisthattheyaremostlybuilton\nfor concept recommendation (C2), we introduce a graph-based heterogeneousgraphs.Theydemandhigh-qualitydata.Thisiswhy\nadapter.Specifically,theadapterisbasedonagraphneuralnet- suchmethodsaretypicallytestedonlyoncomprehensivedatasets\nwork(GNN)andtrainedbycontrastivelearninginaself-supervised likeMOOCCube[30],whereallnecessaryinformationisreadily\nmanner,transferringtherepresentationfromtextspacetotherep- available.However,manyplatformsdonotnaturallypossesssuch\nresentationspacethatalignswiththeknowledgestructure(referred heterogeneousgraphs,whichsignificantlylimitstheapplicability\n\nLearningStructureandKnowledgeAwareRepresentationwithLargeLanguageModelsforConceptRecommendation Conferenceâ€™17,July2017,Washington,DC,USA\nofthesemethodsinvariouscontexts.Thispaperexploresthenext whereeachnodeğ‘˜isaconcept,andeachedgeğ‘’fromğ‘˜ 1toğ‘˜ 2rep-\nconceptpredictionform,delvingintotheintricaciesofaligning resentsthatconceptğ‘˜ 1istheprerequisiteconceptofğ‘˜ 2.However,\neducationalrecommendationswithindividualstudentinterestsand sincesuchgraphsareoftennotreadilyavailableinmanysituations\nchoices,therebycontributingtoamorepersonalizedandeffective ordatasets,weconstructtransitiongraphs[23]toapproximatethese\nlearningexperience. knowledgegraphs.Inthesetransitiongraphs,anedgeğ‘’ fromğ‘˜ 1\n2.2 LLM-enhancedRecommendation toğ‘˜ 2isdrawnif,typically,ğ‘˜ 2islearnedafterğ‘˜ 1accordingtothe\nsequencesobservedinlearnersâ€™activities.Ifthegraphisprovided,\nWiththedevelopmentofLLMs,moreandmoreworkshavebeen\nthenweuseitdirectly.\nproposedbyusingLLMsforrecommendation[17].LLMshavetwo\nmainadvantagesinrecommendation:1)Interprettextinformation.\n4 METHODOLOGY\nThatbeingsaid,thetextinformationintherecommendationsce-\nnariothatcouldnotbefullyusedbeforecouldbeutilizednow. 4.1 Overview\n2)Introducingopen-worldknowledge.LLMsaretrainedonamas-\nWehavedevelopedaconceptrecommendationframeworkthatinte-\nsivecorpusthatcontainsalmostallthehumanknowledge,which\ngratestheknowledgestatesoflearnersandthehumanknowledge\nmakes it suitable to be used as an external knowledge base for\nsystem,asshowninFigure2.Toincorporatethehumanknowledge\nrecommendationtasks.\nsystemusingLLMsandknowledgegraphsandtoadaptthisinfor-\nBytakingthetwoadvantages,LLMsareusedindifferentways.\nmationintotextfortherecommendationprocess,weintroduced\nOnewayistousetheopen-worldknowledgeandreasoningabil-\nthreemodules:\nityoftheLLMs.Thatbeingsaid,thiskindofmethodismainly\nStructureandKnowledge-awareConceptInterpretation.\nbased on prompt engineering to make the LLMs generate the\nThehumanknowledgesystemisbasedonthedefinitionsandre-\nrecommendation-relatedtextinformationtheywant.Forexam-\nlationshipsofconcepts.Toincorporatethemintotherecommen-\nple,P5[7]presentsatext-to-textparadigmforrecommendation.It\ndationprocess,weenhancethetextualdescriptionsofconcepts\nconvertsalldataintonaturallanguagesequencesandtrainsthelan-\nthroughLLMsandtheknowledgegraph.Byusingtheknowledge\nguagemodelwithalanguagemodelingtask.Inthisapproach,the\ngraph,weenabletheLLMtogenerateunambiguoussemanticin-\nrecommendationistakenasanaturallanguageprocessingproblem\nterpretationsofeachconcept.Subsequently,wepresentthename,\nanddependsmoreontheLLMsâ€™inherentreasoningabilitylearned\nexplanation,andprecedingandsucceedingnodesoftheconceptsas\nfromthepre-trainingcorpus.Theotherkindistouseittogenerate\ntheconceptâ€™stextdescriptionforthesubsequentrecommendation.\ndensevectorsassomesortofrepresentationorfeature.Forexam-\nGraph-basedTextAdaptation.Toimprovehowwetransform\nple,UniSRec[9]encodeseachitemusingalanguagemodeland\ntextintorecommendations,wedevelopagraph-basedadapter.This\ndevelopsamixture-of-experts(MoE)adaptorthattransformsthe\nadapteradoptscontrastivelearningonthegraphtooptimizethe\ntextsemanticintoauniversalformsuitedtothecross-domainrec-\nadaptationprocess,whichemphasizesthestructuralconnections\nommendationtasks.RECFORMER[14],ontheotherhand,re-train\nbetweenconcepts,enhancingtheeffectivenessofthefinalrecom-\nLongformer[2]modelontherecommendationdatasetswithmask-\nmendationtask.\nitem-predictionandmask-token-predictiontasks,whichdevelops\nLearner Knowledge State Representation. In addition to\narecommendersystemthatispurelybasedontextinformation.\nmodelingtheconceptsidebyincorporatingthehumanknowledge\nDespiteallthesuccessthesemethodsget,theypaidlittleattention\nsystem,wehaveintroducedtheresultsofknowledgetracingas\ntotheadapterthatadaptsthetextrepresentationtotherecommen-\nauxiliaryinformationonthelearnerside.Thisapproachensures\ndationtask,whichcouldcausethetextinformationnottobefully\nthattheconceptrecommendationnotonlyconsiderstherelevance\nutilized.\nbetweenconceptsbutalsobasesrecommendationsonthecurrent\n3 PRELIMINARIES\nknowledgestateofthelearner.\nInthissection,weformulatethetaskofconceptrecommendation Next,wegothroughthedetailsoftheframework.\nandtheknowledgegraph.\n4.2 StructureandKnowledge-awareConcept\nConceptRecommendationTask.Intheconceptrecommen-\nInterpretation\ndationscenario,wehaveasetoflearners,denotedbyU,asetof\nconceptsdenotedbyK.Attimestepğ‘¡,werecommendaconcept Thekeyfeatureoftheconceptrecommendationproblemisthatthe\nğ‘˜ âˆˆKbasedonthelearnerâ€™slearninghistoryHğ‘¡.Thehistoryiscon- learnerâ€™schoiceislargelybasedonthehumanknowledgesystem,\nstructedasasequenceoftuplesHğ‘¡ = ((ğ‘˜ 1 ,ğ‘ 1 ),(ğ‘˜ 2 ,ğ‘ 2 )...(ğ‘˜ ğ‘¡ ,ğ‘ ğ‘¡)), whichisconstructedbythesemanticmeaningsordefinitionsand\nwhereğ‘˜ ğ‘¡ istheconceptlearnedattimestepğ‘¡;ğ‘ ğ‘¡ âˆˆ{0,1}represent- therelationshipsbetweenconcepts.Humanteachersoftenmake\ningthefeedbackofthelearner(liketheanswercorrectnessofthe educationalplansbasedonagoodunderstandingofeachconcept.\nconcept-relatedquestion).Ourgoalistorecommendğ‘˜ ğ‘¡+1basedon Therefore,incorporatingthesemanticsofeachconceptiscrucial\nHğ‘¡. fordevelopingaconceptrecommendationsystem.\nEachconceptğ‘˜ âˆˆKcorrespondstoatextcorpusğ‘¤ğ‘˜ ,likeâ€œaddi- Directlyusingtheconceptnameorlettingadialoguemodel,\ntionâ€orâ€œvenndiagramâ€thatcouldbeusedasauxiliaryinformation likeGPT-3.5[1],togenerateexplanationsforeachconceptwould\nfortherecommendation. causetheambiguityproblem.Thatis,aconceptcanhavedifferent\nKnowledgeGraph.Theconceptsofacertainareacouldbefor- meaningsbasedonthefieldofstudyorthecontext.Forinstance,\nmulatedasagraphrepresentingtheprerequisiterelationshipsbe- â€œTableâ€couldrefertoadatastructureindatascienceortoapieceof\ntweenthem.Theknowledgegraphisadirectedgraphğº ={K,E}, furnitureineverydayconversation.IftheLLMisaskedtoexplain\n\n| Conferenceâ€™17,July2017,Washington,DC,USA |     |     |     | QingyaoLi,etal. |     |\n| ---------------------------------------- | --- | --- | --- | --------------- | --- |\nStructure and Knowledge-aware Concept Interpretation\n|                     |                    | Recommendation |     | Knowledge Tracing |     |\n| ------------------- | ------------------ | -------------- | --- | ----------------- | --- |\n| Concept name: Table | Transformer Blocks |                |     |                   |     |\nConcept Explanation: The concept of [Table] refers to a structured\narrangement of data or information in rows and columns.\n| Predecessors: Histogram as Table or Graph |     | ... |     |     |     |\n| ----------------------------------------- | --- | --- | --- | --- | --- |\nSuccessors: Venn Diagram, Mean\n|     | ID Embedding Ans Embedding  | Text Encoding | KT Prediction |     |     |\n| --- | --------------------------- | ------------- | ------------- | --- | --- |\n| LLM | Graph-based Text Adaptation |               |               |     |     |\nContrastive Learning\nConcept name: Table\n| Please tell me the meaning of the concept \"Table\".            | Concept Explanation: ...           |         |     |     |     |\n| ------------------------------------------------------------- | ---------------------------------- | ------- | --- | --- | --- |\n| If this concept has ambiguity, please guess its meaning based | Predecessors: Histogram as Table.. | Encoder |     |     |     |\n| on its predecessors: Histogram as Table or Graph  and its     | Successors: Venn Diagram, Mean     |         |     |     |     |\nsuccessors: Venn Diagram, Mean.\nFigure2:TheoverallframeworkofSKarRec.Theleftpartshowstheconstructionofconceptinterpretationsintegratesstructure\nandknowledge.Therightpartisthedemonstrationofknowledgetracing.Thebottomcenterdetailsthegraph-basedtext\nadaptationprocess.Thetopmiddleoutlinestherecommendationmechanismusingtransformedembeddings.\nâ€œTableâ€,itmightincorrectlyinterpretitasapieceoffurniture.To 4.3.1 ConceptTextEncoding. Aftergainingtheenhancedtextof\neliminatesuchambiguities,werefineourpromptstotheLLMby eachconceptğ‘¤Ëœ ğ‘˜ ,weencodethetextutilizingthepre-trainedlan-\nincludingtheconceptâ€™spredecessorandsuccessornodesofthe\nguagemodelBART[12]duetoitsstraightforwardarchitectureand\nconsiderabletextcomprehensioncapabilities.Givenaconceptğ‘˜\nconceptfromtheknowledgegraph.ThismethodhelpstheLLMto\n|     |     |     |     | ğ‘˜ ğ‘˜,ğ‘¤Ëœ ğ‘˜,...ğ‘¤Ëœğ‘ ğ‘˜}.Weadd |     |\n| --- | --- | --- | --- | ------------------------ | --- |\ndeliveranexplanationtailoredtothespecificeducationalcontext, anditscorrespondingenhancedtextğ‘¤Ëœ ={ğ‘¤Ëœ\n1 2\ngreatlydiminishingtheriskofambiguity. aspecialtoken[CLS]atthebeginningandusethelanguagemodel\n| Toensurethatthetextofaconceptcontainssufficientinforma- | toencode. |     |     |     |     |\n| ------------------------------------------------------- | --------- | --- | --- | --- | --- |\ntion,wehavedesignedthetextualrepresentationofeachconcept ğ‘‡ğ‘˜ ğ‘˜,ğ‘¤Ëœ ğ‘˜,...ğ‘¤Ëœğ‘ ğ‘˜\n|     |     | =ğ¿ğ‘€([[ğ¶ğ¿ğ‘†],ğ‘¤Ëœ |     | ])  | (1) |\n| --- | --- | ------------- | --- | --- | --- |\n|     |     |               | 1   | 2   |     |\ntoincludethefollowingfourparts:\nwhereğ‘‡ğ‘˜ isthefinalhiddenvectorcorrespondingtotoken[CLS]\n| â€¢ ConceptName:Thenameoftheconceptğ‘¤ğ‘˜ . |     |     |     |     |     |\n| ------------------------------------- | --- | --- | --- | --- | --- |\nastheencodingoftheenhancedtextofconceptğ‘˜;ğ‘isthelength\n| â€¢ ConceptExplanation:WeutilizeGPT-3.5toinfertheexplanation |                     |     | ğ‘˜   |     |     |\n| ---------------------------------------------------------- | ------------------- | --- | --- | --- | --- |\n|                                                            | ofthewordsequenceğ‘¤Ëœ |     | .   |     |     |\noftheconceptbasedonitspredecessorsandsuccessorsonthe\nknowledgegraph. 4.3.2 Graph-basedAdapter. Togetasmoothandstructure-aware\nâ€¢ Predecessors:Thenamesoftheprerequisiteconceptsofthis\nconceptrepresentation,weproposetolearnfromtheknowledge\nconceptintheknowledgegraph. graphğº ={K,E}.Specifically,wefirstsettheinitialencodingof\nâ€¢\nSuccessors:Thenamesofthesuccessorconceptsofthisconcept eachnodeasthetextencodingofthecorrespondingconcept.\nintheknowledgegraph.\nâ„0 =ğ‘‡ğ‘˜\n|     |     |     | ğ‘˜   |     | (2) |\n| --- | --- | --- | --- | --- | --- |\nEachitemisrepresentedinthekey-valuetextcontainingthefour\nğ‘˜\npartsabove.Wedenotetheenhancedtextasğ‘¤Ëœ . ThenweutilizeGraphConvolutionalNetwork(GCN)[11]tolearn\ntheembeddingofeachconceptforitssimplicityandeffectiveness.\n|     |     | â„ğ‘™ =ğ¶ğ‘œğ‘šğ‘ğ‘–ğ‘›ğ‘’(ğ»(â„ğ‘™ | âˆ’1),ğ´ğ‘”ğ‘”(ğ»(â„ğ‘™ | âˆ’1)|ğ‘– |     |\n| --- | --- | ---------------- | ------------ | ----- | --- |\n|     |     |                  |              | âˆˆNğ‘˜)) | (3) |\n|     |     | ğ‘˜                | ğ‘˜            | ğ‘–     |     |\nwhereâ„ğ‘™ denotesthenoderepresentationofconceptğ‘˜atthel-th\n4.3 Graph-basedTextAdaptation\nğ‘˜\nlayer;â„ğ‘™âˆ’1isthatofpreviouslayer;ğ»(Â·)isanon-lineartransforma-\n| Duetotheanisotropynatureofvanillatextembeddings[13],rec- |     | ğ‘˜   |     |     |     |\n| -------------------------------------------------------- | --- | --- | --- | --- | --- |\ntionfunction(usuallyimplementedbyMLP);ğ´ğ‘”ğ‘”(Â·)istheaggrega-\nommendationtasksoftenrequireanadaptertotransferthetext\nembeddingstosuitthetask[9].Previously,thistransferprocess tionfunctionoftheneighborsâ€™embeddingsofnodeğ‘˜;ğ¶ğ‘œğ‘šğ‘ğ‘–ğ‘›ğ‘’(Â·)\nwastrivialintwoaspects:i)Theconstructionoftheadapterisoften denotesthecombinationfunctionofthepreviouslayerâ€™srepresen-\noneorseveralMulti-LayerPerceptron(MLP)units.ii)Thetraining tationsofnodeğ‘˜andtheaggregationresult.\nobjectivefortheadapterisnotspecificallytailoredbutratheris UsingaGNNastheadapterforconceptrecommendationof-\ntrainedinconjunctionwiththerecommendationtask,whichdidnâ€™t fers two key benefits: 1) In terms of the distribution of embed-\nshowmuchadaptationeffect. dings,GNNâ€™slearningmechanismcanresultinsmootherembed-\nIneducationalcontexts,textualinformationiscloselylinkedwith dings[15],potentiallyresolvingtheproblemofanisotropyintext\nthehumanknowledgesystem.Ourideaistoadapteducationaltexts embeddingdistributions.2)Regardingtheconceptrecommendation\nbasedonthestructureofhumanknowledgeratherthanmerely task,GNNâ€™sabilitytoaggregateinformationallowseachconceptâ€™s\nusingasimpleMLPandtrainingitalongsiderecommendationtasks. embeddingtoincorporaterelationshipsbetweenconcepts,leading\nInlightofthis,weproposeagraph-basedadapterthatisbuiltupon torecommendationsthataremorealignedwiththeunderlying\naGNNandtrainedbycontrastivelearningonthegraph. prerequisiterelationshipsbetweenconcepts.\n\nLearningStructureandKnowledgeAwareRepresentationwithLargeLanguageModelsforConceptRecommendation Conferenceâ€™17,July2017,Washington,DC,USA\n4.3.3 Graph-based Learning. The aim of utilizing a knowledge embedding layers to map the IDs of concepts and the answers.\n|                                                          |     |     |     |     |     | Specifically,wehaveaconceptembeddingmatrixğ‘€ |     |     |     |     | R|K|Ã—ğ‘‘ |\n| -------------------------------------------------------- | --- | --- | --- | --- | --- | ------------------------------------------- | --- | --- | --- | --- | ------ |\n| graphistoenhancethesuitabilityofvanillatextembeddingsfor |     |     |     |     |     |                                             |     |     |     | ğ¾   | âˆˆ      |\nâˆˆR2Ã—ğ‘‘\nconceptrecommendationtasks.Weachievethisbytransforming andananswerembeddingmatrixğ‘€ ğ´ representinganswer-\ntheembeddingofconceptsfromthetextrepresentationspaceto ingrightorwrong.Eachtimestepâ€™srecord(ğ‘˜ ğ‘¡ ,ğ‘ ğ‘¡)isencodedby\narepresentationspacethatalignswiththeknowledgestructure fourpartsofembeddings:\n(referredtoasknowledgespace).Thisisdonethroughlearningon\n|     |     |     |     |     |     |     | ğ‘¥ ğ‘¡ =ğ‘– | ğ‘˜ğ‘¡ âŠ•ğ‘ ğ‘ğ‘¡ | âŠ•â„ ğ‘˜ğ‘¡ âŠ•ğ‘  ğ‘¡ |     | (8) |\n| --- | --- | --- | --- | --- | --- | --- | ------ | -------- | ---------- | --- | --- |\nthegraph.Toaccomplishthis,weemploythecontrastivelearning\nm e th o d t o t r a i n t h e g r a p h a d a p t e r . I t t r a i n s t h e a d a p t e r t o l e a r n ğ‘¥ ğ‘– ğ‘€\n|     |     |     |     |     |     | w h e r e ğ‘¡ | i s t h e fi n a l r e | p r e s e n ta t i o | n ; ğ‘˜ âˆˆ | ğ¾ i s t h e | i d e m b e d - |\n| --- | --- | --- | --- | --- | --- | ----------- | ---------------------- | -------------------- | ------- | ----------- | --------------- |\ns ta b l e n o d e r e p r e s e n t a t i o n s d e s p i t e p e r t u r b a t io n s in t h e g r ap h s t r u c - ğ‘˜ ğ‘ ğ‘€ ğ‘¡ ğ‘ â„\n|     |     |     |     |     |     | d i n g o f | ğ‘¡ ; ğ‘ âˆˆ ğ´ i s | t h e a n s w | e r e m b e d d | i n g o f ğ‘¡ ; | ğ‘˜ is t h e |\n| --- | --- | --- | --- | --- | --- | ----------- | ------------- | ------------- | --------------- | ------------- | ---------- |\ntu r e , t h e r e b y c a p t u r in g t h e g r a p h â€™s d e e p s t r u c t u r a l i n fo r m a ti o n . ğ‘¡ ğ‘¡\n|     |     |     |     |     |     | gr a p h - a d | a p t e d e n c o d in g | o f t h e c o n | c e p t ; ğ‘  ğ‘¡ is | t h e k n o w | le d g e s t a t e |\n| --- | --- | --- | --- | --- | --- | -------------- | ------------------------ | --------------- | ---------------- | ------------- | ------------------ |\nSpecifically,wefirstgaindifferentviewsoftheknowledgegraph\nrepresentation.\nbyedgedropout[28]:\n|     |             |       |           |     |     | Givenasequenceoftheencodingofthelearningrecord(ğ‘¥ |     |     |     |     | ,ğ‘¥ ,ğ‘¥ ...ğ‘¥ ğ‘¡) |\n| --- | ----------- | ----- | --------- | --- | --- | ------------------------------------------------ | --- | --- | --- | --- | ------------- |\n|     |             |       |           |     |     |                                                  |     |     |     |     | 1 2 3         |\n|     | ğ‘£ (ğº)=(K,M1 | âŠ™E),ğ‘£ | (ğº)=(K,M2 | âŠ™E) |     |                                                  |     |     |     |     |               |\n1 2 (4) ,wefurtherutilizethewidelyusedTransformerstructure[27]to\n|         | ,M2 âˆˆ{0,1}|E| |                                   |     |     |     | encodethesequenceand |     |     |     |     |     |\n| ------- | ------------- | --------------------------------- | --- | --- | --- | -------------------- | --- | --- | --- | --- | --- |\n| whereM1 |               | aretwomaskingvectorsontheedgeset. |     |     |     |                      |     |     |     |     |     |\nAhyper-parameterğ›¾ issettobethemaskingratio. [ğ¹ ,ğ¹ ,...ğ¹ ğ‘¡] =ğ¹ğ¹ğ‘(ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›([ğ‘¥ ,ğ‘¥ ,...ğ‘¥ ğ‘¡])) (9)\n|     |     |     |     |     |     |     | 1 2 |     | 1   | 2   |     |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\nByrandomlymaskingthegraphedgesbyacertainratio,weget\nRğ‘‘\ntwosetsofedgescorrespondingtotwoviewsoftheknowledge whereğ¹ğ¹ğ‘(Â·) isthefeedforwardnetwork;ğ¹ ğ‘¡ âˆˆ istheout-\nputencodingattimestepğ‘¡;ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›(Â·)istheMulti-headattention\ngraph.Wetreattheidenticalnodesacrossdifferentviewsaspositive\n| pairsanddifferentnodesfromtheseviewsasnegativepairs.We |     |     |     |     |     | algorithm. |     |     |     |     |     |\n| ------------------------------------------------------ | --- | --- | --- | --- | --- | ---------- | --- | --- | --- | --- | --- |\nthentraintheadapterthroughInfoNCEloss[24]: Wecalculatethescoresforallconcepts:\n|     |     |        | ğ‘’ğ‘ ğ‘–ğ‘š(â„ (1),â„ (2))/ğœ |              |     |     |     | ğ‘ƒ(Â·|Hğ‘¡)=ğ‘Šğ‘‡ğ¹ |     |     |      |\n| --- | --- | ------ | ------------------- | ------------ | --- | --- | --- | ----------- | --- | --- | ---- |\n|     | ğº   | âˆ‘ï¸     | ğ‘˜ ğ‘˜                 |              |     |     |     |             | ğ‘¡   |     | (10) |\n|     | L   | =âˆ’ ğ‘™ğ‘œğ‘” |                     |              | (5) |     |     |             |     |     |      |\n|     | ğ‘ ğ‘ ğ‘™ |        | ğ‘’ğ‘ ğ‘–ğ‘š(â„              | (1),â„ (2))/ğœ |     |     |     |             |     |     |      |\nğ‘˜âˆˆK (cid:205) ğ‘˜ ğ‘– whereğ‘Šğ‘‡ âˆˆR|K|Ã—ğ‘‘ isalearnableparametermatrix.\nğ‘–âˆˆK,ğ‘–â‰ ğ‘˜\nwhereğ‘ ğ‘–ğ‘š(Â·)isthesimilaritymeasurementfunction,whichisset\nasthecosinesimilarity;â„(1) andâ„(2) 4.6 OverallTrainingProcedure\naretworepresentationofthe\nnodesfromtwodifferentviews;ğœ\nisatemperatureparameter. Herewepresentourpre-trainingandfine-tuningprocedure,the\nThegoalofintroducingcontrastivelearningontheknowledge detailedalgorithmcouldbefoundintheAppendixAAlgorithm1.\ngraphistoenhancethevanillatextembeddingsandtransferthem\n|     |     |     |     |     |     | 4.6.1 Pre-training. | During | the pre-training |     | phase, our | training |\n| --- | --- | --- | --- | --- | --- | ------------------- | ------ | ---------------- | --- | ---------- | -------- |\nintoarevampedembeddingspace.Withinthisspace,therepresen-\nfocusesonthreeparts:1)Graph-basedpre-training.Trainthegraph\ntationofeachconceptisenrichedwithrelationalcharacteristics\nadaptertotransferembeddingsfromtextspaceintoagraphor\nfromtheknowledgegraph,ensuringthatnodesstructurallyakin\nknowledgespace.2)Knowledge-Tracingpre-training.Trainthe\ntoeachotheralsoexhibitsimilarityintheembeddingspace.\nknowledgetracingmoduletorepresentalearnerâ€™sknowledgestate\naccurately.3)Sequence-basedpre-training.TraintheTransformer\n4.4 LearnerKnowledgeStateRepresentation\nnetworktoutilizethegraph-adaptedencodingsandknowledge\nPersonalizedconceptrecommendationshouldnotonlyalignwith\ntracingencodingstounderstandthesequentialorderoflearning\nthehumanknowledgesystembutalsobebasedontheindividual\nsequences.Theformertwocouldbeachievedbythecontrastive\nlearnerâ€™sknowledgestate.Therefore,weemploythewidelyused\n|     |     |     |     |     |     | learning | loss on the knowledge | graph | Lğº  | and the | knowledge |\n| --- | --- | --- | --- | --- | --- | -------- | --------------------- | ----- | --- | ------- | --------- |\nğ‘ ğ‘ ğ‘™\n| deepknowledgetracing(DKT)model[25]totrackthelearnerâ€™s |     |     |     |     |     | tracinglossLğ¾ğ‘‡ |     |     |     |     |     |\n| ----------------------------------------------------- | --- | --- | --- | --- | --- | -------------- | --- | --- | --- | --- | --- |\n,respectively.\n| knowledgestate.ThelearninghistoryHğ‘¡ |     |     | isencodedbyarecurrent |     |     |     |     |     |     |     |     |\n| ----------------------------------- | --- | --- | --------------------- | --- | --- | --- | --- | --- | --- | --- | --- |\nForthesequence-basedpre-training,varioustrainingstrategies\nneuralnetwork(RNN)topredictthelearnerâ€™smasterylevelofall\nhavebeensuggested,allrevolvingaroundtheprincipleofpartially\nconcepts.Wechoosethegatedrecurrentunit(GRU)[3]networkas\nhidingsequenceinformationandthenpredictingitthroughse-\nthesequenceencoderduetoitssimplicityandeffectiveness.\nquencemodeling.Here,weadoptthesameschemeasS3Rec[33]\nğ‘  =ğºğ‘…ğ‘ˆ(Hğ‘¡)\nğ‘¡ (6) sinceitcoversmostofthesequence-basedself-supervisedlearning\nobjectives:maskitemprediction(Lğ‘€ğ¼ğ‘ƒ),masksegmentprediction\nThetrainingobjectiveofDKTisthepredictionofthecorrectness\n(Lğ‘€ğ‘†ğ‘ƒ),maskattributeprediction(Lğ‘€ğ´ğ‘ƒ),andassociatedattribute\n| ğ‘ ğ‘¡+1ofthenextconceptğ‘˜ |     | ğ‘¡+1: |     |     |     |     |     |     |     |     |     |\n| ---------------------- | --- | ---- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\nprediction(Lğ´ğ´ğ‘ƒ).Thefinalsequence-basedpre-trainingobjective\nğ¾ğ‘‡\n|     |     | L =(ğ‘  ğ‘¡(ğ‘˜ | ğ‘¡+1 )âˆ’ğ‘ ğ‘¡+1 )2 |     | (7) |     |     |     |     |     |     |\n| --- | --- | --------- | -------------- | --- | --- | --- | --- | --- | --- | --- | --- |\nis:\nğ‘ ğ‘’ğ‘\nTheknowledgestateofthelearnerisconsistentlychanging,sowe L =Lğ‘€ğ¼ğ‘ƒ +Lğ‘€ğ‘†ğ‘ƒ +Lğ‘€ğ´ğ‘ƒ +Lğ´ğ´ğ‘ƒ (11)\nğ‘ ğ‘ ğ‘™\npredicttheknowledgestaterepresentationateachtimestepand\nInsummary,inthepre-trainingstage,wepre-traingraph-based\nconcatenateittotheinputoftherecommendationmodel.\nadapter,knowledgetracingmoduleandTransformernetworkby\n|     |     |     |     |     |     | Lğº ,Lğ¾ğ‘‡ | ğ‘ ğ‘’ğ‘ |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | ------- | --- | --- | --- | --- | --- |\nandL .\n| 4.5 | Self-AttentionBasedRecommendation |     |     |     |     | ğ‘ ğ‘ ğ‘™ | ğ‘ ğ‘ ğ‘™ |     |     |     |     |\n| --- | --------------------------------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\nHavingtheencodingofthegraph-adaptedtextualrepresentation 4.6.2 Fine-tuning. Afterpre-training,wefine-tunethewholemodel\noftheconceptâ„ ğ‘˜ğ‘¡ andtheknowledgestateğ‘  ğ‘¡,wefurtheraddtwo inanend-to-endmannerbasedontheconceptrecommendation\n\n| Conferenceâ€™17,July2017,Washington,DC,USA |     |     |     |     |     |     |     | QingyaoLi,etal. |     |\n| ---------------------------------------- | --- | --- | --- | --- | --- | --- | --- | --------------- | --- |\nTable1:DatasetStatistics settingusingbothIDandtextconsideringthatitgetsbetter\nresultsthantext-onlysettingintheoriginalpaper.\n| Dataset   | Junyi | ASSIST12 | ASSIT09 |     | (3)Text-Onlymethods: |     |     |     |     |\n| --------- | ----- | -------- | ------- | --- | -------------------- | --- | --- | --- | --- |\n| #Concepts | 835   | 265      | 145     |     |                      |     |     |     |     |\nâ€¢ ZESRec[6]:utilizesapre-trainedlanguagemodeltoencode\n#Learners 341,195 24,155 3,322 theitemtextsastheinputfeaturefornext-itemprediction.\n| #Records | 21,460,249 | 1,853,338 | 187,914 |     | â€¢   |     |     |     |     |\n| -------- | ---------- | --------- | ------- | --- | --- | --- | --- | --- | --- |\nRECFORMER[14]:threatsitemkey-valueattributesastexts\n| Correctrate | 54.38% | 69.55% | 64.12% |     |     |     |     |     |     |\n| ----------- | ------ | ------ | ------ | --- | --- | --- | --- | --- | --- |\nandformulatestheinteractionsequencesassentencessequences.\nThenapre-trainedlanguagemodelisfine-tunedforthenext\nitemprediction.\ntask.Weadoptthecross-entropyloss:\n(4)Graph-basedconceptrecommendationmethods:\nğ‘ |K|\n|     | 1 âˆ‘ï¸âˆ‘ï¸  |            |     |      | â€¢ ACKRec[8]:buildsaheterogeneousgraphanusesattention |     |     |     |     |\n| --- | ------- | ---------- | --- | ---- | ---------------------------------------------------- | --- | --- | --- | --- |\n|     | Lğ‘Ÿğ‘’ğ‘ =âˆ’ | ğ‘¦ ğ‘›ğ‘— ğ‘™ğ‘œğ‘”(ğ‘ƒ | ğ‘›ğ‘—) | (12) |                                                      |     |     |     |     |\n|     | ğ‘       |            |     |      | mechanismtolearntherepresentationfromdifferentmeta-  |     |     |     |     |\nğ‘›=0ğ‘—=0\npaths.\nwhereğ‘ isthetrainingsamplenumber;ğ‘¦ ğ‘›ğ‘— isabinaryindicator â€¢ GCARec[31]:developstopology-levelandfeature-levelaug-\nofwhetherconceptğ‘— istheactualnextoneofsampleğ‘›;ğ‘ƒ isthe mentationstogeneratedifferentviewsoflearner-conceptgraph\nğ‘›ğ‘—\n| predictedprobabilityoftheconceptğ‘— |     | ofsampleğ‘›. |     |     |     |     |     |     |     |\n| --------------------------------- | --- | ---------- | --- | --- | --- | --- | --- | --- | --- |\ntoconductcontrastivelearning.\n5 EXPERIMENTS 5.1.3 Evaluationmetrics. Toevaluatetheperformanceofsequen-\ntialrecommendation,weadoptHitRatio(HR),NormalizedDis-\n5.1 ExperimentalSettings.\n|     |     |     |     |     | counted Cumulative | Gain | (NDCG), and | Mean Reciprocal | Rank |\n| --- | --- | --- | --- | --- | ------------------ | ---- | ----------- | --------------- | ---- |\n5.1.1 Datasets. Weconductedexperimentsthroughthreedatasets: (MRR)astheevaluationmetrics.Weemployaleave-one-outstrat-\nJunyi1,ASSIT122andASSIT093.Theyallprovidelearningsequences egy,whereeachsequenceâ€™slastconceptisreservedfortesting,the\n| of learners. | These datasets are | transformed | from their | original |     |     |     |     |     |\n| ------------ | ------------------ | ----------- | ---------- | -------- | --- | --- | --- | --- | --- |\nsecondtolastforvalidation,andallprecedingonesfortraining.\nquestion-answerpairsequencesintoconcept-answerpairs.Each\nconcepthasanIDandaname.Weprovidethestatisticsofthese 5.2 OverallPerformance\nthreedatasetsinTable1.\nTable2reportstheoverallperformanceofallmethodsinthree\nFortheknowledgegraph,Junyidatasetoffersadedicatedknowl-\ndatasets.Itdemonstratesthat:\nedgegraphthatoutlinesconceptdependencies.WhileforASSIST09\nandASSIST12,weutilizetransitiongraphs[23]asanestimationof â€¢ OurproposedSKarRecachievesthebestoverallperformance.\nDifferentfrombaselines,ourproposedenhancedtextandgraph-\nknowledgegraphs.\nbasedadaptermaketherecommendationmodelunderstandthe\n| 5.1.2 Baselines. | Wearecomparingfourdifferentsetsofbaseline |     |     |     |     |     |     |     |     |\n| ---------------- | ----------------------------------------- | --- | --- | --- | --- | --- | --- | --- | --- |\nstructureandsemanticlinksbetweenconcepts,leadingtoim-\nmethods.TheseincludemethodsthatuseonlyconceptIDs,meth-\nprovedperformanceinconceptrecommendation.\nodsthatutilizeitemIDsandtreatthetextdescriptionofitemsas\n|     |     |     |     |     | â€¢ Addingtextualinformationtoconceptrecommendationsproves |     |     |     |     |\n| --- | --- | --- | --- | --- | -------------------------------------------------------- | --- | --- | --- | --- |\nadditionalinformation,methodsthatrelysolelyonthetextofthe beneficial, allowing methods that use both IDs and text (ID-\nconceptsasinput,andmethodsbasedonheterogeneousgraphs\n|     |     |     |     |     | Text methods) | to outperform | ID-Only | methods. Nonetheless, |     |\n| --- | --- | --- | --- | --- | ------------- | ------------- | ------- | --------------------- | --- |\nproposedforconceptrecommendation.\ntheimprovementisconstrainedbytheabsenceofadedicated\n(1)ID-Onlymethods:\nadapterinearlierapproaches.Withtheintroductionofthegraph-\nâ€¢ SASRec[10]:utilizesaself-attentionmodeltopredictthenext basedadapter,theadvantagesofincorporatingtextintoconcept\n| item.                                                  |     |     |     |     | recommendationsaremoresignificantlyrealized. |     |     |     |     |\n| ------------------------------------------------------ | --- | --- | --- | --- | -------------------------------------------- | --- | --- | --- | --- |\n| â€¢ BERT4Rec[26]:utilizestheoriginalBert[5]schemefornext |     |     |     |     | â€¢                                            |     |     |     |     |\nTheperformanceofgraph-basedmethodsdesignedforconcept\nitemprediction. recommendationdeclinesintheabsenceofheterogeneousin-\n(2)ID-Textmethods: formation.Originally,thesemethodsreliedonheterogeneous\ngraphsthatincludedavarietyofelementslikecourses,videos,\nâ€¢ FDSA[32]:utilizesself-attentiontocaptureitemandfeature\nandteachers.However,suchcomprehensivedataisrarelyavail-\ntransitionpatterns.\nableinmostopen-sourceeducationaldatasets,leadingtolimita-\nâ€¢\nS3-Rec[33]:pre-trainsself-attentionmodelswithmutualin-\ntionsintheseapproaches.\nformationmaximizationobjectivesrelatedtoattributes,items,\nsub-sequences,etc.\n|     |     |     |     |     | 5.3 AblationStudy |     |     |     |     |\n| --- | --- | --- | --- | --- | ----------------- | --- | --- | --- | --- |\nâ€¢ UniSRec(Transductive)[9]:usesanMoE-basedadapterto\ntransittheinformationfromtexttorecommendationarea.We Weconductablationexperimentstoanalyzehowourproposed\nchoosetocomparewiththemodelfine-tunedintransductive modulesinfluencethefinalconceptrecommendationperformance.\n1https://www.kaggle.com/datasets/junyiacademy/learning-activity-public-dataset- 5.3.1 Graph-basedAdapter. Wecomparedifferentversionsofour\nby-junyi-academy\n|     |     |     |     |     | model to assess | the impact | of our suggested | graph-based | adap- |\n| --- | --- | --- | --- | --- | --------------- | ---------- | ---------------- | ----------- | ----- |\n2https://sites.google.com/site/assistmentsdata/datasets/2012-13-school-data-with-\n|     |     |     |     |     | tation strategy. | The results | are shown in | Table 3. In | â€œSKarRec- |\n| --- | --- | --- | --- | --- | ---------------- | ----------- | ------------ | ----------- | --------- |\naffect\n3https://sites.google.com/site/assistmentsdata/home/2009-2010-assistment-data No-GraphSSLâ€,weremovethecontrastivelearning(retainingthe\n\nLearningStructureandKnowledgeAwareRepresentationwithLargeLanguageModelsforConceptRecommendation Conferenceâ€™17,July2017,Washington,DC,USA\nTable2:Performancecomparisonofdifferentrecommendationmodels.Thebestandthesecond-bestperformanceisboldand\nunderlinedrespectively.â€œ*â€denotesthattheimprovementaresignificantatlevelofğ‘ <0.05withpairedt-testcomparingwith\nthesecondbestbaseline.\nGraph-basedMethods ID-OnlyMethods Text-OnlyMethods ID-TextMethods\nDataset Metric ACKRec GCARec SASRec BERT4Rec ZESRec RECFORMER FDSA S3-Rec UniSRec SKarRec\nHR@1 0.2408 0.2513 0.7649 0.1935 0.5809 0.6072 0.7627 0.7661 0.7597 0.7922âˆ—\nASSIST09 NDCG@5 0.3787 0.3904 0.8736 0.2203 0.7269 0.6540 0.8722 0.8731 0.8647 0.8838âˆ—\nMRR 0.3627 0.3756 0.8503 0.2342 0.6998 0.6711 0.8484 0.8505 0.8439 0.8646âˆ—\nHR@1 0.1976 0.2119 0.2838 0.2540 0.2596 0.1990 0.2861 0.2904 0.2888 0.2933âˆ—\nASSIST12 NDCG@5 0.3078 0.3169 0.4895 0.4247 0.4649 0.2055 0.4886 0.4957 0.4934 0.4954\nMRR 0.3064 0.3182 0.4574 0.4047 0.4343 0.2536 0.4579 0.4633 0.4617 0.4645âˆ—\nHR@1 0.1447 0.1284 0.8469 0.3550 0.8546 0.8279 0.8492 0.8608 0.8407 0.8730âˆ—\nJunyi NDCG@5 0.1872 0.1650 0.8907 0.4826 0.8930 0.7625 0.8917 0.9022 0.8890 0.9076âˆ—\nMRR 0.1918 0.1740 0.8825 0.4661 0.8860 0.8497 0.8840 0.8942 0.8799 0.9014âˆ—\nTable 3: Ablation study about the proposed graph-based Table4:Ablationstudyaboutthestructureandknowledge-\nadapter.Theâ€œ-MoEâ€suffixmeansthatwechangetheadapter awareconceptinterpretation.Theâ€œ-No-LLMâ€suffixmeans\ntoMoEadapterandtheâ€œ-No-GraphSSLâ€meansthatwere- thatwedroptheLLMâ€™sconceptexplanationandtheâ€œ-Nameâ€\nmovethegraph-basedcontrasrivelearning. meansthatwedropalltheenhancedtext.\nDataset Metric SKarRec-No-GraphSSL SKarRec-MoE SKarRec Dataset Metric SKarRec-No-LLM SKarRec-Name SKarRec\n|     | HR@1 | 0.7727 | 0.7754 0.7922 |     | HR@1 | 0.7898 |     | 0.7775 | 0.7922 |\n| --- | ---- | ------ | ------------- | --- | ---- | ------ | --- | ------ | ------ |\nASSIST09 NDCG@5 0.8764 0.8760 0.8838 ASSIST09 NDCG@5 0.8832 0.8756 0.8838\n|     | MRR  | 0.8543 | 0.8548 0.8646 |     | MRR  | 0.8633 |     | 0.8552 | 0.8646 |\n| --- | ---- | ------ | ------------- | --- | ---- | ------ | --- | ------ | ------ |\n|     | HR@1 | 0.2901 | 0.2855 0.2933 |     | HR@1 | 0.2886 |     | 0.4926 | 0.2933 |\nASSIST12 NDCG@5 0.4946 0.4923 0.4954 ASSIST12 NDCG@5 0.4936 0.4612 0.4954\n|     | MRR  | 0.4626 | 0.4601 0.4645 |     | MRR  | 0.4617 |     | 0.8721 | 0.4645 |\n| --- | ---- | ------ | ------------- | --- | ---- | ------ | --- | ------ | ------ |\n|     | HR@1 | 0.8711 | 0.8717 0.8730 |     | HR@1 | 0.8708 |     | 0.8721 | 0.8730 |\nJunyi NDCG@5 0.9060 0.9071 0.9076 Junyi NDCG@5 0.9065 0.9069 0.9076\n|     | MRR | 0.8998 | 0.9006 0.9014 |     | MRR | 0.9001 |     | 0.9006 | 0.9014 |\n| --- | --- | ------ | ------------- | --- | --- | ------ | --- | ------ | ------ |\nGNNstructure,trainingonlythroughthefine-tunerecommenda-\nhumanknowledgesystem.Inthevariantâ€œSKarRec-Nameâ€,wedrop\ntiontask).Inâ€œSKarRec-MoEâ€,weentirelyreplacedthegraph-based\nalltheenhancedtext(includingLLMâ€™sConceptExplanationand\nadapterwiththeMoEadapterfrompreviouswork[9].Bothvaria-\nthepredecessorsandsuccessorsfromthegraph)andonlyprovide\ntionsexhibitanotabledropinperformancecomparedtotheoriginal\ntheconceptnames.Thissubstitutionresultsinasignificantdropin\nstructure.Thisnotonlydemonstratesthegraph-basedadapterâ€™s\nrecommendationperformance,illustratingthatthetextnamesare\nsuperiorabilitytoadaptthetextencodingsbutalsoindicatesthat\ninsufficientforincorporatingconceptsâ€™semanticmeanings.Our\nthiscapabilitylargelystemsfromthecontrastivelearningonthe\n|     |     |     |     | proposed | method | successfully | incorporates | semantic | and struc- |\n| --- | --- | --- | --- | -------- | ------ | ------------ | ------------ | -------- | ---------- |\ngraphratherthanmerelychangingtheadapterâ€™sstructuretoaGNN.\nturalinformationintothetext,improvingthelearningforconcept\nGraph-basedcontrastivelearninghasacleartrainingobjective:to\nrepresentation.\nensurethateachconceptâ€™sembeddingcapturestheprerequisite\nrelationshipsbetweenconcepts.Thisrelationshipiscrucialforin-\ncorporatingthesemanticsofconceptsandeffectivelyintegrating\nthehumanknowledgesystem,therebyenhancingrecommendation w/o KT w/o KT\n|           |     |     |     |         | w/o S3Pre-training |               | w/o S3Pre-training |                           |     |\n| --------- | --- | --- | --- | ------- | ------------------ | ------------- | ------------------ | ------------------------- | --- |\n| outcomes. |     |     |     | 0 . 9 0 | Ours               |               | 0 . 9 2 Ours       |                           |     |\n|           |     |     |     |         | 0.870.880.88       |               |                    | 0.900.910.91 0.900.900.90 |     |\n|           |     |     |     |         |                    | 0.850.85 0.86 |                    |                           |     |\n|           |     |     |     | 0 . 8 5 |                    |               | 0 . 8 9            |                           |     |\n5.3.2 StructureandKnowledge-awareConceptInterpretation. We 0.870.87\n|     |     |     |     | 0.80 | 0.79 |     | 0.86 0.86 |     |     |\n| --- | --- | --- | --- | ---- | ---- | --- | --------- | --- | --- |\nnowdelvedintotheeffectsoftheproposedstructureandknowledge-\n0.770.76\nawareconceptinterpretation,referredtoasenhancedtext.Wecon- 0.75 0.83\nductcomparisonexperiments,andtheresultsareshowninTable4.\n|     |     |     |     | 0.70 | HR@1 NDCG@5 | MRR | 0.80 HR@1 | NDCG@5 | MRR |\n| --- | --- | --- | --- | ---- | ----------- | --- | --------- | ------ | --- |\nInthevariantâ€œSKarRec-No-LLMâ€,wedroptheconceptexplana-\n|     |     |     |     |     | (a)ASSIST09 |     |     | (b)Junyi |     |\n| --- | --- | --- | --- | --- | ----------- | --- | --- | -------- | --- |\ntionsprovidedbytheLLMandonlykeeptheinformationabout\ntheconceptâ€™spredecessorsandsuccessorsfromthegraph.Weob- Figure3:Performanceofcomparisonwith/withoutsequence\n| serveadeclineinperformance.Itdemonstratesthatcapturingthe |     |     |     | modelingtasks. |     |     |     |     |     |\n| --------------------------------------------------------- | --- | --- | --- | -------------- | --- | --- | --- | --- | --- |\nsemanticmeaningsofconceptsareimportantforintroducingthe\n\n| Conferenceâ€™17,July2017,Washington,DC,USA |     |                 |          |     |                 |     |           |     |     |                 |     | QingyaoLi,etal. |\n| ---------------------------------------- | --- | --------------- | -------- | --- | --------------- | --- | --------- | --- | --- | --------------- | --- | --------------- |\n|                                          |     | ASSISTments2009 |          |     | ASSISTments2009 |     |           |     |     | ASSISTments2009 |     |                 |\n|                                          |     |                 | 10.0     |     |                 |     |           | 15  |     |                 |     |                 |\n|                                          | 10  |                 | DBI:2.03 |     |                 |     | DBI: 1.60 |     |     |                 |     | DBI: 0.66       |\n7.5\n10\n|     | 5   |     | 5.0 |     |     |     |     |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|     |     |     | 2.5 |     |     |     |     | 5   |     |     |     |     |\n0\n0.0\n0\n|     | 5   |     | 2.5 |     |     |     |     |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|     |     |     | 5.0 |     |     |     |     | 5   |     |     |     |     |\n10\n7.5\n7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 4 2 0 2 4 6 8\n|     | (a)VanillaTextembeddings |     |     |     | (b)MoEembeddings |     |     |     |     | (c)SKarRec |     |     |\n| --- | ------------------------ | --- | --- | --- | ---------------- | --- | --- | --- | --- | ---------- | --- | --- |\nFigure4:ComparingdifferentdistributionsofembeddingsinASSIST09.ThesmallertheDBIvalueupperinfigure,thebetter.\n5.3.3 Learnersequencemodeling. Inourframework,wemainly Insub-figure(a),wemeasuretheshareofrecommendationsthat\nhavetwomodulesforlearnersequencemodeling:Oneistheknowl- matchtheknowledgegraphâ€™sstructureonalllearners.Arecom-\nedgetracingmoduletocapturethelearnerâ€™sknowledgestatefrom mendationforalearnerisdeemedconsistentifitisapredecessor\nthelearninghistory.Theotheristhesequence-basedself-supervised orsuccessortothepreviousconcept.Weexpressthisasaratio:\n#ğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’\npre-training(shortenedasâ€œS3Pre-trainingâ€sinceitâ€™sfromtheS3Rec ğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œ = Insub-figure(b),insteadof\n#ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’\nmethod).Wenowexploretheimpactofthem.Theresultsareshown\nevaluatingtheconsistencyonalllearners,weevaluateonthelearn-\ninFigure3.Itwasobservedthatremovingknowledgetracingsig-\nerswhoselearningstyleisadheredtothegraph.Specifically,we\nnificantlydeterioratesthemodelâ€™sperformance.Thereasonisthat\n|     |     |     |     |     |     | evaluate | whether | a learnerâ€™s | learning | sequence | aligns | with the |\n| --- | --- | --- | --- | --- | --- | -------- | ------- | ----------- | -------- | -------- | ------ | -------- |\neachlearnerâ€™slearningjourneyishighlyindividualized,making\nknowledgegraphbeforemakingrecommendations.Wetakeeach\nconceptrecommendationsheavilyreliantontheircurrentknowl- concept a learner has studied with the next as a pair and con-\nedgestate.Therefore,themodelingoflearnersâ€™knowledgestatesis\nsiderapairâ€œconsistentpairâ€iftheyfollowthegraphâ€™sorder.The\ncrucial.Additionally,eliminatingthesequence-basedpre-training\nâ€œconsistencyscoreâ€istheratioofconsistentpairstototalpairs:\nresultsinanotabledecreaseinperformance.Thishighlightsthe #ğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘ğ‘ğ‘–ğ‘Ÿğ‘ \n|     |     |     |     |     |     | ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ | =   | .Ascoreover0.5suggeststhelearnerâ€™s |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | ----- | --- | ---------------------------------- | --- | --- | --- | --- |\nimportanceoflearningpatternswithinlearnersequences,whichis #ğ‘ğ‘™ğ‘™ğ‘ğ‘ğ‘–ğ‘Ÿğ‘ \n|     |     |     |     |     |     | style | aligns with | the graph. | We focus | on  | the rate | of consistent |\n| --- | --- | --- | --- | --- | --- | ----- | ----------- | ---------- | -------- | --- | -------- | ------------- |\nessentialforthemodeltopredictthenextconceptaccurately.\nrecommendationsforthissubsetoflearners.\nTheexperimentsindicatethatourapproachmorefrequentlysug-\n5.4 ConsistencywithKnowledgeStructure gestsconceptsconsistentwiththegraphâ€™sstructureforalllearners.\nThisadvantageisevenmorepronouncedforthoselearnerswhose\n| The | primary | distinction between | concept recommendation |     | and |     |     |     |     |     |     |     |\n| --- | ------- | ------------------- | ---------------------- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\nproductrecommendationliesinthateducationalrecommendation stylesadheretothegraphâ€™sstructure.Thisimprovementstems\nofconceptsshouldadheretothestructureofhumanknowledge[20]. fromthestructure-awareconceptinterpretationandthegraph-\nbasedadapter,whichenricheseachconceptâ€™srepresentationwith\nThisismanifestedindataastheprerequisiterelationshipsbetween\nitsprerequisiterelationshipstootherconcepts.\nconcepts.Inthissection,weconductedexperimentstoevaluateif\n|     |     |     |     |     |     | 5.5 | EmbeddingSpaceTransformation |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | --- | ---------------------------- | --- | --- | --- | --- | --- |\nthemodelsâ€™recommendationsareconsistentwiththeknowledge\ngraph.Wecomparetheproportionofrecommendedconcepts,ğ‘˜ ğ‘¡+1, Tobetterunderstandthegraph-basedadapterâ€™sabilitytoadaptnon-\nhavingaprerequisiteorsuccessorrelationshipwiththeprevious\nsmooth,anisotropictextembeddingspaces,weemployedt-SNEto\nconcept,ğ‘˜\nğ‘¡,ontheknowledgegraph.Theresultsaredisplayedin performvisualizationofthevanillatextembeddings,graph-adapted\nFigure5. embeddings,andMoE-adaptedembeddings.Thevisualizationcom-\nparisononASSIST09isshowninFigure4.Theothervisualizations\nofASSIST12andJunyiareprovidedinAppendixB.3.\nTable5:TheDBIvalueofdifferentrepresentationlearnedin\n|                | UnisRec  |              | UnisRec                |      |     | threedatasets. |                       |            |          |          |        |        |\n| -------------- | -------- | ------------ | ---------------------- | ---- | --- | -------------- | --------------------- | ---------- | -------- | -------- | ------ | ------ |\n|                | S 3 R ec | 0.200.200.20 | S3Rec                  |      |     |                |                       |            |          |          |        |        |\n| 0.20           | O u rs   |              | 0.56 Ours 0.540.530.54 |      |     |                |                       |            |          |          |        |        |\n| oitar gnihctaM |          |              | oitar gnihctaM         | 0.47 |     |                |                       |            |          |          |        |        |\n| 0.15           |          |              | 0.42                   |      |     |                |                       | Embeddings | ASSIST09 | ASSIST12 |        | Junyi  |\n| 0.10           |          |              | 0.28 0.29              |      |     |                | VanillaTextEmbeddings |            | 2.0257   |          | 2.3055 | 3.0333 |\n0.200.20\n|      |      |     |               |     |     |     | MoEAdapted |     | 1.6007 |     | 1.9199 | 2.0392 |\n| ---- | ---- | --- | ------------- | --- | --- | --- | ---------- | --- | ------ | --- | ------ | ------ |\n| 0.05 | 0.05 |     | 0.14 0.090.10 |     |     |     |            |     |        |     |        |        |\n0.010.02\n| 0.00 |                     | 0.000.000.01     | 0.00                  |       |     |     | SKarRec |     | 0.6611 |     | 0.3255 | 1.7208 |\n| ---- | ------------------- | ---------------- | --------------------- | ----- | --- | --- | ------- | --- | ------ | --- | ------ | ------ |\n|      | ASSIST2009          | ASSIST2012 Junyi | ASSIST2009 ASSIST2012 | Junyi |     |     |         |     |        |     |        |        |\n|      | (a)w/olearningstyle |                  | (b)Withlearningstyle  |       |     |     |         |     |        |     |        |        |\nFromtheillustrationsprovided,itisevidentthatthevanillatext\nFigure5:Thematchingratiosoftherecommendationand embeddingsandthoseadaptedthroughMoElackcleardistribution\ngraphstructure.\n\nLearningStructureandKnowledgeAwareRepresentationwithLargeLanguageModelsforConceptRecommendation Conferenceâ€™17,July2017,Washington,DC,USA\npatternsorclusteringtraits.However,embeddingsthathaveun- SequentialRecommendation.arXivpreprintarXiv:2305.13731(2023).\ndergonegraph-basedadaptationdisplaypronouncedmanifoldand [15] QimaiLi,ZhichaoHan,andXiao-MingWu.2018.Deeperinsightsintograph\nconvolutionalnetworksforsemi-supervisedlearning.InProceedingsoftheAAAI\nclusteringproperties,makingthemmoreconducivetosubsequent\nconferenceonartificialintelligence,Vol.32.\nrecommendationtasks. [16] QingyaoLi,WeiXia,Liâ€™angYin,JianShen,RentingRui,WeinanZhang,Xianyu\nTofurtherillustratethetransformationeffect,wecalculatethe Chen,RuimingTang,andYongYu.2023.GraphEnhancedHierarchicalReinforce-\nmentLearningforGoal-orientedLearningPathRecommendation.InProceedings\nDavies-BouldinIndex(DBI)[4].Itassessesthedataseparationstate ofthe32ndACMInternationalConferenceonInformationandKnowledgeManage-\nbetweenclusters.AlowerDBIindicatesbetter-separatedclasses. ment.1318â€“1327.\n[17] JianghaoLin,XinyiDai,YunjiaXi,WeiwenLiu,BoChen,XiangyangLi,Chenxu\nWeuseK-MeanstoclustertheembeddingsandcalculateDBIas\nZhu,HuifengGuo,YongYu,RuimingTang,etal.2023. HowCanRecom-\nshowninTable5.Ourgraph-basedadaptedembeddingsachieve menderSystemsBenefitfromLargeLanguageModels:ASurvey.arXivpreprint\nthelowestDBIscores,demonstratingabetterclusteringfeature. arXiv:2306.05817(2023).\n[18] YuanguoLin,ShiboFeng,FanLin,WenhuaZeng,YongLiu,andPengchengWu.\n2021.AdaptivecourserecommendationinMOOCs.Knowledge-BasedSystems\n6 CONCLUSION 224(2021),107085.\n[19] QiLiu,ZaiHuang,ZhenyaHuang,ChuanrenLiu,EnhongChen,YuSu,and\nWepresentastructureandknowledge-awarerepresentationframe- GuopingHu.2018. Findingsimilarexercisesinonlineeducationsystems.In\nworkforconceptrecommendation.Ourapproachinvolvesgener- Proceedingsofthe24thACMSIGKDDInternationalConferenceonKnowledge\nDiscovery&DataMining.1821â€“1830.\natingtextualdescriptionsforknowledgeconceptsusingexternal\n[20] QiLiu,ShiweiTong,ChuanrenLiu,HongkeZhao,EnhongChen,HaipingMa,\ninformationfromLLMsandstructuraldatafromknowledgegraphs. andShijinWang.2019. Exploitingcognitivestructureforadaptivelearning.\nToaddressthechallengeofconvertingnon-smooth,anisotropic InProceedingsofthe25thACMSIGKDDInternationalConferenceonKnowledge\nDiscovery&DataMining.627â€“635.\ntextencodingsforconceptrecommendation,wehavedeveloped [21] ZitaoLiu,QiongqiongLiu,JiahaoChen,ShuyanHuang,JiliangTang,andWeiqi\nagraph-basedadaptertrainedthroughcontrastivelearning.This Luo.2022.pyKT:apythonlibrarytobenchmarkdeeplearningbasedknowledge\ntracingmodels. AdvancesinNeuralInformationProcessingSystems35(2022),\nmarksthefirstefforttoincorporateexternalknowledgefromLLMs\n18542â€“18555.\nintothefieldofconceptrecommendation.Throughthoroughtest- [22] TingLong,JiaruiQin,JianShen,WeinanZhang,WeiXia,RuimingTang,Xi-\ning, our framework has proven effective in refining the textual uqiangHe,andYongYu.2022.Improvingknowledgetracingwithcollaborative\ninformation.InProceedingsofthefifteenthACMinternationalconferenceonweb\ninformationofconceptstoenhancerecommendationoutcomes.\nsearchanddatamining.599â€“607.\n[23] HiromiNakagawa,YusukeIwasawa,andYutakaMatsuo.2019. Graph-based\nREFERENCES knowledgetracing:modelingstudentproficiencyusinggraphneuralnetwork.\nInIEEE/WIC/ACMInternationalConferenceonWebIntelligence.156â€“163.\n[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,Floren- [24] AaronvandenOord,YazheLi,andOriolVinyals.2018.Representationlearning\nciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,Shyamal withcontrastivepredictivecoding.arXivpreprintarXiv:1807.03748(2018).\nAnadkat,etal.2023. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774 [25] ChrisPiech,JonathanBassen,JonathanHuang,SuryaGanguli,MehranSahami,\n(2023). LeonidasJGuibas,andJaschaSohl-Dickstein.2015. Deepknowledgetracing.\n[2] IzBeltagy,MatthewEPeters,andArmanCohan.2020.Longformer:Thelong- Advancesinneuralinformationprocessingsystems28(2015).\ndocumenttransformer.arXivpreprintarXiv:2004.05150(2020). [26] FeiSun,JunLiu,JianWu,ChanghuaPei,XiaoLin,WenwuOu,andPengJiang.\n[3] JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio.2014. 2019. BERT4Rec:Sequentialrecommendationwithbidirectionalencoderrep-\nEmpiricalevaluationofgatedrecurrentneuralnetworksonsequencemodeling. resentationsfromtransformer.InProceedingsofthe28thACMinternational\narXivpreprintarXiv:1412.3555(2014). conferenceoninformationandknowledgemanagement.1441â€“1450.\n[4] DavidLDaviesandDonaldWBouldin.1979.Aclusterseparationmeasure.IEEE [27] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,\ntransactionsonpatternanalysisandmachineintelligence2(1979),224â€“227. AidanNGomez,ÅukaszKaiser,andIlliaPolosukhin.2017. Attentionisall\n[5] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert: youneed.Advancesinneuralinformationprocessingsystems30(2017).\nPre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.arXiv [28] JiancanWu,XiangWang,FuliFeng,XiangnanHe,LiangChen,JianxunLian,and\npreprintarXiv:1810.04805(2018). XingXie.2021.Self-supervisedgraphlearningforrecommendation.InProceed-\n[6] HaoDing,YifeiMa,AnoopDeoras,YuyangWang,andHaoWang.2021.Zero- ingsofthe44thinternationalACMSIGIRconferenceonresearchanddevelopment\nshotrecommendersystems.arXivpreprintarXiv:2105.08318(2021). ininformationretrieval.726â€“735.\n[7] ShijieGeng,ShuchangLiu,ZuohuiFu,YingqiangGe,andYongfengZhang.2022. [29] YangYang,JianShen,YanruQu,YunfeiLiu,KerongWang,YaomingZhu,Weinan\nRecommendationaslanguageprocessing(rlp):Aunifiedpretrain,personalized Zhang,andYongYu.2021.GIKT:agraph-basedinteractionmodelforknowledge\nprompt&predictparadigm(p5).InProceedingsofthe16thACMConferenceon tracing.InMachineLearningandKnowledgeDiscoveryinDatabases:European\nRecommenderSystems.299â€“315. Conference,ECMLPKDD2020,Ghent,Belgium,September14â€“18,2020,Proceedings,\n[8] JibingGong,ShenWang,JinlongWang,WenzhengFeng,HaoPeng,JieTang, PartI.Springer,299â€“315.\nandPhilipSYu.2020.Attentionalgraphconvolutionalnetworksforknowledge [30] JifanYu,GanLuo,TongXiao,QingyangZhong,YuquanWang,WenzhengFeng,\nconceptrecommendationinmoocsinaheterogeneousview.InProceedings JunyiLuo,ChenyuWang,LeiHou,JuanziLi,etal.2020.MOOCCube:alarge-\nofthe43rdinternationalACMSIGIRconferenceonresearchanddevelopmentin scaledatarepositoryforNLPapplicationsinMOOCs.InProceedingsofthe58th\ninformationretrieval.79â€“88. annualmeetingoftheassociationforcomputationallinguistics.3135â€“3142.\n[9] YupengHou,ShanleiMu,WayneXinZhao,YaliangLi,BolinDing,andJi-Rong [31] MeiYu,ZhaoyuanDing,JianYu,WenbinZhang,MingYang,andMankunZhao.\nWen.2022.Towardsuniversalsequencerepresentationlearningforrecommender 2023.GraphContrastiveLearningwithAdaptiveAugmentationforKnowledge\nsystems.InProceedingsofthe28thACMSIGKDDConferenceonKnowledgeDis- ConceptRecommendation.In202326thInternationalConferenceonComputer\ncoveryandDataMining.585â€“593. SupportedCooperativeWorkinDesign(CSCWD).IEEE,1281â€“1286.\n[10] Wang-ChengKangandJulianMcAuley.2018.Self-attentivesequentialrecom- [32] TingtingZhang,PengpengZhao,YanchiLiu,VictorSSheng,JiajieXu,Deqing\nmendation.In2018IEEEinternationalconferenceondatamining(ICDM).IEEE, Wang,GuanfengLiu,XiaofangZhou,etal.2019. Feature-levelDeeperSelf-\n197â€“206. AttentionNetworkforSequentialRecommendation..InIJCAI.4320â€“4326.\n[11] ThomasNKipfandMaxWelling.2016.Semi-supervisedclassificationwithgraph [33] KunZhou,HuiWang,WayneXinZhao,YutaoZhu,SiruiWang,FuzhengZhang,\nconvolutionalnetworks.arXivpreprintarXiv:1609.02907(2016). ZhongyuanWang,andJi-RongWen.2020.S3-rec:Self-supervisedlearningforse-\n[12] MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,Abdelrahman quentialrecommendationwithmutualinformationmaximization.InProceedings\nMohamed,OmerLevy,VesStoyanov,andLukeZettlemoyer.2019.Bart:Denoising ofthe29thACMinternationalconferenceoninformation&knowledgemanagement.\nsequence-to-sequencepre-trainingfornaturallanguagegeneration,translation, 1893â€“1902.\nandcomprehension.arXivpreprintarXiv:1910.13461(2019).\n[13] BohanLi,HaoZhou,JunxianHe,MingxuanWang,YimingYang,andLeiLi.2020.\nA ALGORITHM\nOnthesentenceembeddingsfrompre-trainedlanguagemodels.arXivpreprint\narXiv:2011.05864(2020).\nWepresentthedetailedpre-trainingandfine-tuningprocedurein\n[14] JiachengLi,MingWang,JinLi,JinmiaoFu,XinShen,JingboShang,andJulian\nMcAuley.2023.TextIsAllYouNeed:LearningLanguageRepresentationsfor Algorithm1.\n\n| Conferenceâ€™17,July2017,Washington,DC,USA |     |     |     |     |     |     |     |     |     |     | QingyaoLi,etal. |\n| ---------------------------------------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --------------- |\nusedtogeneratedifferentviews.Weinvestigatetheimpactofthis\nAlgorithm1:Pre-trainingandFine-tuningFramework\nInput:Knowledgegraphğº,learnersequencesğ· parameter,withtheexperimentalresultsdisplayedinFigure6.It\n| 1   |     |     |     |     |     | train, |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | ------ | --- | --- | --- | --- | --- |\nRandominitializedgraph-basedadapterğ¹ canbeobservedthattheimpactofthisparameterisrelativelyminor\nğ‘”,knowledge\ntracingnetworkğ¹ ğ‘˜ğ‘¡,Transformerblocksğ¹ onASSIST12,whileithasamoresignificanteffectonbothJunyi\nğ‘¡ğ‘Ÿğ‘ğ‘›\n|                    |     |     |     |                    |     |     | a nd A S SI S | T 0 9 da ta s et s . | A c e r ta in d ro p | ra ti o m u s t b | e m a i n t a i n e d |\n| ------------------ | --- | --- | --- | ------------------ | --- | --- | ------------- | -------------------- | -------------------- | ----------------- | --------------------- |\n| Hyper-parameters:ğ‘› |     |     |     | ,edgemaskingratioğ›¾ |     |     |               |                      |                      |                   |                       |\n2 epoch to al lo w t h e G N N t o l e a rn t h e s tru c tu re o f th e g r a p h e ff e c t i v e ly .\n| Output:Trainedğ¹ |     |     | ğ‘”,ğ¹ ğ‘˜ğ‘¡,ğ¹ |     |     |     |     |     |     |     |     |\n| --------------- | --- | --- | -------- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 3               |     |     | ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘     |     |     |     |     |     |     |     |     |\nTherefore,whenthemaskingratioislow,theperformancetends\n|     | 1: ğ·     | â†LLMTextEnhance(ğ·  |     |       | )     |     |             |     |     |     |     |\n| --- | -------- | ------------------ | --- | ----- | ----- | --- | ----------- | --- | --- | --- | --- |\n|     | train    |                    |     | train |       |     | tobepoorer. |     |     |     |     |\n|     | ğ·        | â†LLMTextEncoding(ğ· |     |       | )     |     |             |     |     |     |     |\n|     | 2: train |                    |     |       | train |     |             |     |     |     |     |\nPre-training\nB.3 EmbeddingSpaceTransformation\nforğ‘›inğ‘›\n|     | 3:  | epoch | do  |     |     |     |     |     |     |     |     |\n| --- | --- | ----- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\nğ‘£ ğº ,ğ‘£ (ğº (ğº , ğ›¾ In Figures 7 and Figure 8, we provide a comparison of the em-\n|     | 4 : 1 ( | ) 2 | ) â† E d g eM | a sk | )   |     |     |     |     |     |     |\n| --- | ------- | --- | ------------ | ---- | --- | --- | --- | --- | --- | --- | --- |\nğº b e d d i n g d is t ri bu ti o n s t h ro u g h d iff e r e n t a d a p ta t i o n s o n th e A S S I S T -\n|     | 5 : L | â† C on | t rast iv e L | ea rn in g( | ğ‘£ (ğº),ğ‘£ | (ğº),ğ¹ ğ‘”) |     |     |     |     |     |\n| --- | ----- | ------ | ------------- | ----------- | ------- | -------- | --- | --- | --- | --- | --- |\nğ‘  ğ‘  ğ‘™ 1 2 m e n t s 2 01 2 a n d Ju n y i d a ta se t s ,r es p e c ti v e ly . A d d i t io n a l ly, w e pr e s e n t\n|     | ğ¹   |     | ğº ,ğ¹ |     |     |     |     |     |     |     |     |\n| --- | --- | --- | ---- | --- | --- | --- | --- | --- | --- | --- | --- |\n6: ğ‘” â†Update(L ğ‘  ğ‘ ğ‘™ ğ‘”) theirDavies-BouldinIndex(DBI)values,demonstratingthatthe\n7: endfor distributionpost-transformationbythegraphadapterexhibitssupe-\n|     | 8: forğ‘›inğ‘› |     | do  |     |     |     |     |     |     |     |     |\n| --- | ---------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\nepoch riorclusteringpropertiescomparedtotheMoE(MixtureofExperts)\n|     | Lğ¾ğ‘‡ | â†KnolwedgeTracingTask(ğ¹ |     |     | ,ğ·  |         |          |     |     |     |     |\n| --- | --- | ----------------------- | --- | --- | --- | ------- | -------- | --- | --- | --- | --- |\n|     | 9:  |                         |     |     | ğ‘˜ğ‘¡  | train ) | adapter. |     |     |     |     |\nâ†Update(Lğ‘˜ğ‘¡,ğ¹\n|     | 10: ğ¹ ğ‘˜ğ‘¡ |     |     | ğ‘˜ğ‘¡) |     |     |     |     |     |     |     |\n| --- | -------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n11: endfor\n|     | forğ‘›inğ‘› |             | do  |         |     |     |     |     |     |     |     |\n| --- | ------- | ----------- | --- | ------- | --- | --- | --- | --- | --- | --- | --- |\n|     | 12:     | epoch       |     |         |     |     |     |     |     |     |     |\n|     | ğ·ğ‘€ğ¼ğ‘ƒ    | â†MaskItem(ğ· |     |         |     |     |     |     |     |     |     |\n|     | 13:     |             |     | train ) |     |     |     |     |     |     |     |\nğ·ğ‘€ğ‘†ğ‘ƒ\n|     | 14: | â†MaskSegment(ğ· |     |     | )   |     |     |     |     |     |     |\n| --- | --- | -------------- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\ntrain\n|     | ğ·ğ‘€ğ´ğ‘ƒ | â†MaskAttribute(ğ·             |     |       | )     |                   |     |     |     |     |     |\n| --- | ---- | ---------------------------- | --- | ----- | ----- | ----------------- | --- | --- | --- | --- | --- |\n|     | 15:  |                              |     | train |       |                   |     |     |     |     |     |\n|     | ğ·ğ´ğ´ğ‘ƒ | â†MaskAssociatedAttribute(ğ·   |     |       |       | )                 |     |     |     |     |     |\n|     | 16:  |                              |     |       | train |                   |     |     |     |     |     |\n|     | Lğ‘€ğ¼ğ‘ƒ | â†MaskItemPredict(ğ·ğ‘€ğ¼ğ‘ƒ,ğ¹      |     |       |       | ,ğ¹ ,ğ¹             |     |     |     |     |     |\n|     | 17:  |                              |     |       | ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  | ğ‘” ğ‘˜ğ‘¡)             |     |     |     |     |     |\n|     | Lğ‘€ğ‘†ğ‘ƒ | â†MaskSegmentPredict(ğ·ğ‘€ğ‘†ğ‘ƒ,ğ¹   |     |       |       |                   |     |     |     |     |     |\n|     | 18:  |                              |     |       |       | ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  ,ğ¹ ğ‘” ,ğ¹ ğ‘˜ğ‘¡) |     |     |     |     |     |\n|     | Lğ‘€ğ´ğ‘ƒ | â†MaskAttributePredict(ğ·ğ‘€ğ´ğ‘ƒ,ğ¹ |     |       |       | ,ğ¹ ,ğ¹ ğ‘˜ğ‘¡)         |     |     |     |     |     |\n|     | 19:  |                              |     |       |       | ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  ğ‘”           |     |     |     |     |     |\n|     | Lğ´ğ´ğ‘ƒ | â†AttributePredict(ğ·ğ´ğ´ğ‘ƒ,ğ¹     |     |       |       |                   |     |     |     |     |     |\n|     | 20:  |                              |     |       | ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  | ,ğ¹ ğ‘” ,ğ¹ ğ‘˜ğ‘¡)       |     |     |     |     |     |\nğ‘  ğ‘’ ğ‘\n|     | L     | â†Lğ‘€ğ¼ğ‘ƒ | +Lğ‘€ğ‘†ğ‘ƒ | +Lğ‘€ğ´ğ‘ƒ | +Lğ´ğ´ğ‘ƒ |     |     |     |     |     |     |\n| --- | ----- | ----- | ----- | ----- | ----- | --- | --- | --- | --- | --- | --- |\n|     | 21: ğ‘  | ğ‘  ğ‘™   |       |       |       |     |     |     |     |     |     |\nğ‘ ğ‘’ğ‘\n|     | 22: ğ¹ ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  | ,ğ¹ ğ‘” ,ğ¹ | ğ‘˜ğ‘¡ â†Update(L |     | ,ğ¹ ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  ,ğ¹ | ğ‘” ,ğ¹ ğ‘˜ğ‘¡) |     |     |     |     |     |\n| --- | ----------- | ------- | ------------ | --- | ----------- | -------- | --- | --- | --- | --- | --- |\nğ‘ ğ‘ ğ‘™\nendfor\n23:\nFine-tuning\nforğ‘›inğ‘›\n|     | 24:         | epoch              | do              |     |             |            |     |     |     |     |     |\n| --- | ----------- | ------------------ | --------------- | --- | ----------- | ---------- | --- | --- | --- | --- | --- |\n|     |             | â†NextConceptPred(ğ¹ |                 |     | ,ğ¹ ,ğ¹       | ,ğ·         |     |     |     |     |     |\n|     | 25: Lğ‘Ÿğ‘’ğ‘    |                    |                 |     | ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  ğ‘”     | ğ‘˜ğ‘¡ train ) |     |     |     |     |     |\n|     | 26: ğ¹ ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  | ,ğ¹ ğ‘” ,ğ¹            | ğ‘˜ğ‘¡ â†Update(Lğ‘Ÿğ‘’ğ‘ |     | ,ğ¹ ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  ,ğ¹ | ğ‘” ,ğ¹ ğ‘˜ğ‘¡)   |     |     |     |     |     |\nendfor\n27:\n|     | returnğ¹ |       | ,ğ¹ ,ğ¹ |     |     |     |     |     |     |     |     |\n| --- | ------- | ----- | ----- | --- | --- | --- | --- | --- | --- | --- | --- |\n|     | 28:     | ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  | ğ‘” ğ‘˜ğ‘¡  |     |     |     |     |     |     |     |     |\nB ADDITIONALEXPERIMENTSDETAILS\nB.1 ImplementationDetails.\n| For | baselines | purely | based on | heterogeneous |     | graph-based con- |     |     |     |     |     |\n| --- | --------- | ------ | -------- | ------------- | --- | ---------------- | --- | --- | --- | --- | --- |\nceptrecommendation,weconstructedabipartitegraphoflearner-\nconceptwithtwotypesofedges:correct-answeredgesandwrong-\nansweredges.Inourmethod,SKarRec,weimplementedthreelay-\nersofattentionblocks.ThesizesfortheIDembedding,answerem-\nbedding,andgraph-adaptedembeddingwereallsetto64.Themax-\nimumsequencelengthwasconfiguredtobe200,andthebatchsize\nwassetat256.Themaskingratioğ›¾istunedin{0.1,0.2,0.3,0.4,0.5}.\nWeutilizeGPT-3.5todotheconceptinterpretation.\nImpactofedgemaskingratioğ›¾\nB.2\nAcrucialcomponentofourframeworkinvolvesperformingcon-\ntrastivelearningontheknowledgegraphthroughedgedropout.\nAkeyparameterinthisapproachisthemaskingratioofedgesğ›¾\n\nLearningStructureandKnowledgeAwareRepresentationwithLargeLanguageModelsforConceptRecommendation Conferenceâ€™17,July2017,Washington,DC,USA\n|      |             | NDCG5   |       |         |             | NDCG5   |       |          | NDCG5   |\n| ---- | ----------- | ------- | ----- | ------- | ----------- | ------- | ----- | -------- | ------- |\n|      |             | MRR     |       |         |             | MRR     |       |          | MRR     |\n| 0.89 |             |         | 0.506 |         |             |         | 0.908 |          |         |\n| 0.88 |             |         | 0.492 |         |             |         | 0.906 |          |         |\n| 0.87 |             |         | 0.478 |         |             |         | 0.904 |          |         |\n| 0.86 |             |         | 0.464 |         |             |         | 0.902 |          |         |\n| 0.85 |             |         | 0.450 |         |             |         | 0.900 |          |         |\n| 0.1  | 0.2 0.3     | 0.4 0.5 |       | 0.1 0.2 | 0.3         | 0.4 0.5 | 0.1   | 0.2 0.3  | 0.4 0.5 |\n|      | gamma       |         |       |         | gamma       |         |       | gamma    |         |\n|      | (a)ASSIST09 |         |       |         | (b)ASSIST12 |         |       | (c)Junyi |         |\nFigure6:Comparingtheperformanceacrossdifferentedgemaskingratioğ›¾.\nASSISTments2009\n10.0\nDBI: 1.60\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n|                          |     |     |     | 7.5 5.0          | 2.5 0.0 2.5 | 5.0 7.5 |     |            |     |\n| ------------------------ | --- | --- | --- | ---------------- | ----------- | ------- | --- | ---------- | --- |\n| (a)VanillaTextembeddings |     |     |     | (b)MoEembeddings |             |         |     | (c)SKarRec |     |\nFigure7:ComparingdifferentdistributionsofembeddingsofASSIST12.ThesmallertheDBIvalueupperinfigure,thebetter.\n| (a)VanillaTextembeddings |     |     |     | (b)MoEembeddings |     |     |     | (c)SKarRec |     |\n| ------------------------ | --- | --- | --- | ---------------- | --- | --- | --- | ---------- | --- |\nFigure8:ComparingdifferentdistributionsofembeddingsofJunyi.ThesmallertheDBIvalueupperinfigure,thebetter.\n</content>\n",
  "llm": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  },
  "metrics": {
    "llm": [],
    "services": {
      "firecrawl": {
        "requests": 0
      },
      "apify": {
        "requests": 0
      }
    }
  },
  "summary": "This paper studies concept recommendation in online education, where the goal is to suggest the next knowledge concept a learner should study based on both the learners knowledge state and the human knowledge system that links concepts through meaning and prerequisite structure. It argues that prior methods either model learner behavior well but miss deeper concept semantics, or use large language model text embeddings that are hard to use directly because they can be anisotropic and can encode ambiguous concept names without enough context. The work frames two core challenges: constructing concept text that reflects both semantics and concept relations, and adapting language model encodings into representations that fit concept recommendation.\n\nIt proposes SKarRec, a Structure and Knowledge Aware Representation learning framework with a text to knowledge to recommendation pipeline:\n- Structure and knowledge aware concept interpretation: build an enhanced concept description containing concept name, an LLM generated concept explanation guided by predecessor and successor concepts from a knowledge graph, plus lists of predecessor and successor concept names to reduce ambiguity.\n- Graph based text adaptation: encode the enhanced text with a pretrained language model encoder, then use a GNN based adapter (implemented with GCN layers) to transform text embeddings into a smoother, structure aligned knowledge space. The adapter is pretrained with self supervised graph contrastive learning using edge dropout to create two graph views and an InfoNCE objective that pulls the same node together across views.\n- Learner knowledge state representation and recommendation: use knowledge tracing (DKT with a GRU) to estimate a time varying learner state, combine it with concept ID and answer embeddings plus graph adapted concept embeddings, and apply a Transformer based sequential recommender to score next concept candidates.\n\n*We propose a novel Structure and Knowledge Aware Representation learning framework for concept Recommendation.*\n\nExperiments are reported on three real world datasets, Junyi, ASSIST12, and ASSIST09, with knowledge graphs taken directly when available or approximated via transition graphs. The evaluation uses standard top k sequential recommendation metrics including HR, NDCG, and MRR under a leave one out split, and compares against ID only, text only, ID plus text, and graph based baselines such as SASRec, BERT4Rec, S3-Rec, UniSRec, ZESRec, RECFORMER, ACKRec, and GCARec. Across datasets, SKarRec is shown as the strongest overall performer in the main comparison table, and ablations attribute gains to both the graph contrastive pretraining of the adapter and the enhanced concept text with LLM explanations and graph context. Additional analyses claim recommendations better match knowledge graph structure, and embedding visualizations plus Davies Bouldin Index comparisons indicate the graph adapted embeddings cluster more cleanly than vanilla text embeddings or an MoE style adapter.\n\n*Our proposed SKarRec achieves the best overall performance.*"
}