{
  "paper_id": "2410.01627",
  "title": "Intent Detection Llm",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper studies intent detection for task-oriented dialogue systems in the age of generative large language models, focusing on real deployment constraints like few examples per intent, class imbalance, and the need to reject out-of-scope queries. It benchmarks several instruction-tuned LLMs against a contrastively fine-tuned sentence transformer baseline using SetFit, then proposes practical ways to combine them to balance accuracy and latency. *performance within ∼2% of native LLM performance with∼50%lesslatencythannativeLLM.* *High compute and latency costs of LLMs make themprohibitivelyexpensivetouseinproductionatscale.*\n\nMethodologically, the work compares two main approaches and then blends them:\n- A SetFit baseline, plus negative data augmentation that creates synthetic out-of-scope-like variants by removing or replacing keywords, intended to sharpen decision boundaries.\n- An LLM approach using adaptive in-context learning with a retriever: embed all training examples offline, retrieve top similar examples above a threshold, add intent descriptions generated from training data, and use chain-of-thought style prompting during inference.\n- A hybrid routing system: run SetFit first and send only uncertain queries to an LLM, where uncertainty is estimated via Monte Carlo dropout variance over multiple forward passes.\n\nAcross three open-source datasets (from HINT3) and three internal datasets (AID3: ALC, ADP, OADP), results emphasize tradeoffs: stronger LLMs (notably Claude v2/v3 and Mistral Large) score higher on average F1, while SetFit is dramatically faster but lags in prediction quality; negative augmentation improves SetFit by over 5 points on multiple datasets yet still trails the best LLMs. The hybrid uncertainty routing narrows the performance gap to about 2 percent on in-distribution settings with roughly half the latency of always using an LLM, and helps on out-of-distribution generalization where the smaller SetFit model drops more. Controlled experiments show out-of-scope detection degrades sharply as intent scopes broaden and as the label space grows, making label design and granularity especially important for LLM-based intent detection; the paper also introduces a two-step out-of-scope method that uses internal LLM decoder representations after first predicting an in-scope label, reporting over 5 percent gains in out-of-scope metrics for Mistral-7B with a modest encoding-only latency overhead.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2410.01627_intent_detection_llm.pdf"
}