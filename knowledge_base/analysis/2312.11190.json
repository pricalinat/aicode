{
  "paper_id": "2312.11190",
  "title": "Visiontasker",
  "category": "mini_program_service",
  "year": 2023,
  "timestamp": "2026-03-01T13:56:06.556672",
  "summary": "# VisionTasker: Mobile Task Automation Using Vision-Based UI Understanding and LLM Task Planning\n\n## Overview\n\nThis paper introduces VisionTasker, a two-stage framework for automating mobile tasks on Android smartphones by combining vision-based UI understanding with large language model (LLM) task planning. The system addresses limitations in traditional approaches like Programming by Demonstration (PBD), which relies on predefined scripts and is vulnerable to app updates, and view hierarchy-based methods, which suffer from accessibility issues and missing element descriptions.\n\n## Problem Statement\n\nSmartphones present accessibility challenges for users with motor limitations, elderly users navigating complex interfaces, and scenarios requiring repetitive tasks like sending bulk messages or setting calendar entries. Existing automation methods face significant constraints: PBD requires users to demonstrate tasks beforehand and breaks when apps update, while vision-language models trained on datasets with noisy human trials lack adaptability to new interfaces. View hierarchies—the XML-based UI metadata in Android—frequently contain missing elements, incorrect descriptions, or empty structures that hinder automation effectiveness.\n\n## Technical Approach\n\nVisionTasker operates through two interconnected stages. The first stage involves vision-based UI understanding, where the system analyzes screenshots using multiple components: YOLOv8 for widget detection across 12 common UI element classes, PaddleOCR for text recognition, proximity-based text-widget matching, CLIP-based interpretation of icon-only buttons, and gradient-based semantic block division to group UI elements logically. This approach eliminates dependence on view hierarchies entirely, instead mimicking how humans perceive interfaces through visual recognition.\n\nThe second stage employs LLMs for step-by-step task planning. Rather than generating entire action sequences at once, which leads to hallucination errors, VisionTasker presents one interface at a time to the LLM, asking it to identify relevant elements and determine the next action. The system maintains action history to track progress and incorporates a Programming by Demonstration mechanism where users can manually execute complex tasks once, and the system translates those actions into semantic steps the LLM can reference for similar situations.\n\n## Key Innovations\n\nThe framework introduces several notable innovations. Semantic block division uses gradient-based border detection to identify visually distinct UI sections, enabling more accurate element grouping than flat sequential descriptions. Active tab detection analyzes HSV color distribution to identify selected navigation elements. The natural language UI descriptions require approximately one-quarter the tokens compared to HTML-based view hierarchy representations, allowing handling of longer task sequences. The step-by-step chain-of-screens approach significantly reduces hallucination by constraining the LLM to only interact with elements visible in the current interface.\n\n## Evaluation Results\n\nVisionTasker was evaluated across multiple dimensions. For UI understanding on 136 screens from 31 popular apps, the system achieved 94% precision and recall for element detection, outperforming view hierarchy-based approaches. For one-step action prediction across four public datasets (MoTIF, META, UGIF, AITW), VisionTasker achieved 67% accuracy, exceeding baseline methods by over 15%. In real-world testing with 147 tasks spanning 12 categories on an Android smartphone, VisionTasker completed 76% of tasks compared to 87% human completion rate, but showed advantages over humans on unfamiliar tasks (47% vs 26%). When integrated with the PBD mechanism, completion rate improved to 94%, surpassing human performance.\n\n## Limitations and Discussion\n\nThe system has several limitations. Screenshots cannot capture login screens or payment QR codes. Subtle UI distinctions like disabled dates displayed in lighter colors may be missed. Search boxes located near advertisements can be difficult to detect. Latency averages over 12 seconds per action due to UI understanding (5.9s) and LLM communication (4.5s). The approach is adaptable to desktop environments with retrained detection models, and open-source LLMs like Qwen can replace proprietary models with comparable results when fine-tuned.\n\nThe main contributions include proposing a two-stage framework eliminating view hierarchy dependencies, designing a comprehensive vision-based UI understanding scheme, and demonstrating effectiveness across diverse screen sizes, task types, and complexity levels through extensive experiments. *The approach leverages the principle that well-crafted UIs allow even unfamiliar users to grasp semantics and navigate intuitively based on visual cues alone.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}