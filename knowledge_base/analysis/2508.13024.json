{
  "paper_id": "2508.13024",
  "title": "Webmall",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T13:55:25.950899",
  "summary": "# WebMall: A Multi-Shop Benchmark for Evaluating Web Agents\n\nWebMall is a novel benchmark developed by researchers at the University of Mannheim for evaluating LLM-based web agents on complex e-commerce comparison shopping tasks. It represents the first offline benchmark that simulates multiple online shops with heterogeneous product data, requiring agents to navigate across shops, compare prices, and complete purchase workflows.\n\n## Benchmark Overview\n\nThe WebMall environment consists of four simulated electronics shops built using WooCommerce and Docker containers. These shops contain 4,421 product offers extracted from the October 2024 Common Crawl via schema.org annotations, covering three categories: PC components, PC peripherals, and other electronics like cameras, smartphones, and smartwatches. The products are distributed across all four shops, creating realistic comparison shopping scenarios where agents must visit multiple websites to find the best deals.\n\nThe task set contains 91 tasks across 11 categories grouped into five task groups. Specific Product Search tasks include finding all offers for a named product and searching with specific attribute constraints. Vague Product Search tasks require agents to interpret underspecified user requirements, find substitutes, and identify compatible products. Cheapest Product Search tasks demand price comparison across shops. Action & Transaction tasks involve adding products to carts and completing checkout. End-to-End tasks combine searching, cart management, and checkout into single workflows.\n\n## Experimental Validation\n\nThe researchers evaluated eight baseline agent configurations using the Browsergym/AgentLab framework, varying along three dimensions: observation space (accessibility tree, screenshots, or both), short-term memory availability, and underlying LLM (GPT-4.1 and Claude Sonnet 4).\n\nThe results reveal that WebMall presents significant challenges for state-of-the-art LLMs. The best-performing agents achieve task completion rates below 55% in the most difficult categoriesâ€”cheapest product search and vague product search. Structural grounding via the accessibility tree proves essential for success; agents relying solely on screenshots perform substantially worse and sometimes fail entirely on transactional tasks.\n\nMemory provides task-dependent advantages: GPT-4.1 benefits significantly from memory on specific search and transactional tasks, while Claude Sonnet 4 performs well without memory on cheapest searches. Adding visual inputs to accessibility trees offers situational benefits but does not consistently improve performance.\n\n## Efficiency Analysis\n\nThe study reveals substantial differences between models in token consumption and runtime. GPT-4.1 configurations complete tasks in approximately 2.5-3.3 minutes at $0.26-$0.34 per task, while Claude Sonnet 4 agents require 4.5-8.2 minutes at $0.85-$1.42 per task. Claude agents consume substantially more tokens, with average input tokens between 240,000 and 390,000 compared to roughly 120,000-155,000 for GPT-4.1. The higher token usage stems from WebMall requiring agents to interact with multiple websites, resulting in longer context windows.\n\nCommon failure modes include insufficient cross-shop reasoning where agents stop after finding a single offer, rigid search strategies that miss alternative spellings or variants, and output formatting mistakes when entering URLs on solution pages.\n\n## Contributions and Significance\n\nWebMall fills a critical gap in web agent benchmarking by introducing comparison shopping tasks that require navigation across multiple heterogeneous shops. Existing simulated benchmarks like WebShop and WebArena cover only single-shop scenarios with less complex tasks. The benchmark is publicly available on GitHub along with baseline agent implementations, enabling reproducible evaluation of web agent capabilities in multi-shop e-commerce scenarios.\n\nThe findings highlight the need for agents with improved product comparison capabilities and more efficient architectures. The accessibility tree provides crucial structural information for navigation, while visual inputs alone prove insufficient for reliable task completion.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}