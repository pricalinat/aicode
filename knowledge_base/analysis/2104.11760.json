{
  "paper_id": "2104.11760",
  "title": "Deepcat",
  "category": "product_matching",
  "status": "success",
  "summary": "## Summary of 2104.11760_deepcat.pdf\n\nDeepCAT proposes a deep learning approach for e-commerce query understanding that maps short, often ambiguous user queries to relevant product categories in a hierarchical taxonomy. The paper frames this as difficult largely because training labels derived from customer clicks are heavily imbalanced and because tail queries with little feedback are underrepresented, making category prediction especially weak for minority categories and low-frequency traffic. DeepCAT addresses these issues by learning joint representations that connect words to categories and also explicitly model correlations among categories, aiming to transfer signal toward rare classes and sparse-query regimes.\n\nThe model architecture has three main parts: a Query2Vector network for query representation, a joint word-category representation module, and a category-category representation module. For Query2Vector, the authors choose a CNN with highway layers to balance expressiveness with lower inference latency than recurrent or transformer alternatives. For word-category representation, the model builds a word-category co-occurrence signal by comparing query-word embeddings to category embeddings, then uses multi-head self-attention to estimate how much each word contributes to each category; these attention-derived weights reshape the word vectors before producing a combined representation used for multi-label prediction. For category-category representation, the method constructs a category co-occurrence matrix from training data, normalizes it, and adds a matrix-approximation style loss that encourages learned category embeddings to reproduce observed category-category co-occurrence patterns; the overall objective is a weighted combination of the classification loss and this co-occurrence reconstruction loss.\n\nEmpirically, the study uses two weeks of search logs (over 11M training queries) and evaluates both coarse taxonomy mapping at the first level with 33 categories and fine-grained mapping at the leaf level with 4115 categories, with test queries stratified into head, torso, and tail buckets by frequency. Compared against TF-IDF + SVM, FastText, XML-CNN, and LEAM, DeepCAT reports consistent gains, with emphasis on improvements for minority categories and tail queries: about 10 percent relative improvement on minority classes and 7.1 percent on tail queries over a strong label-embedding baseline, while remaining competitive on head queries. Ablation results attribute performance gains to both the joint word-category representation and the added category co-occurrence loss, suggesting that explicitly modeling label-label relationships helps recover signal where click-derived supervision is sparse.\n\n*DeepCAT reaches a 10% improvement on minority classes and a 7.1% improvement on tail queries over a state-of-the-art label embedding model.*\n\n*DeepCAT consists of three main components: (a) query representation, (b) joint-word-category representation, and (c) category-category representation.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2104.11760_deepcat.pdf"
}