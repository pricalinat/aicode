{
  "paper_id": "2602.08322",
  "title": "Multi Intent Slot Filling",
  "category": "mini_program_service",
  "status": "success",
  "summary": "The paper tackles multi-intent spoken language understanding, where a single user utterance can express multiple intents and associated slots, and argues that common single-intent SLU methods and synthetic multi-intent datasets do not transfer well to realistic multi-intent settings. It introduces a generative approach that jointly predicts intents and slot spans in one sequence-to-sequence output, and also proposes a higher-quality way to construct multi-intent datasets from single-intent corpora. *We propose a generative framework, GEMIS, for joint multiple intent detection and slot filling.*\n\nGEMIS reformulates joint intent detection and slot filling as generating a structured target sequence that lists all intents followed by slot triplets of start index, end index, and slot type. It is built on a modified BART encoder-decoder with a pointer network to predict span positions over the input tokens, and replaces standard decoder cross-attention with an attention-over-attention sublayer that reuses historical attention patterns to reduce interference between the two subtasks. Training follows standard seq2seq cross-entropy with teacher forcing; inference uses greedy decoding, with the authors noting beam search had little effect in their pilot experiments.\n\nFor data, the paper constructs MultiATIS and MultiSNIPS by concatenating single-intent utterances only when BERT next sentence prediction assigns a coherence score above a threshold, aiming to create more natural co-occurring intent combinations than prior random concatenation (MixATIS, MixSNIPS). Experiments evaluate slot F1, intent accuracy, and sentence-level overall accuracy, showing GEMIS improves over prior baselines and prior SOTA systems on MixATIS, MixSNIPS, and the newly constructed datasets; for example, overall accuracy reaches 53.4 on MixATIS and 87.4 on MixSNIPS, and 71.5 on MultiATIS and 91.5 on MultiSNIPS, with attention-over-attention providing additional gains over a GEMIS variant without it. *our analysis demonstrates that the superiority of GEMIS is more significant as the number of intents in one utterance increases.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2602.08322_Multi_Intent_Slot_Filling.pdf"
}