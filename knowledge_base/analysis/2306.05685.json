{
  "paper_id": "2306.05685",
  "title": "Paper",
  "category": "mini_program_service",
  "status": "success",
  "summary": "The paper argues that evaluating chat assistants is hard because traditional benchmarks mostly test closed-ended core knowledge and often fail to reflect what people actually prefer in open-ended, multi-turn interactions. It proposes using strong LLMs as evaluators, LLM-as-a-judge, and introduces two human-preference benchmarks to validate this approach: MT-bench, an 80-question, two-turn set spanning 8 categories, and Chatbot Arena, an anonymous head-to-head platform that collects real user votes from unconstrained prompts. *Strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement.*\n\nThe authors systematically study where LLM judging can go wrong and how to reduce those errors, then compare LLM decisions to expert and crowd human votes at scale. Key issues include position bias, verbosity bias, possible self-enhancement bias, and limited reliability when grading math or reasoning, especially when the judge is misled by provided answers. Mitigations include swapping answer order and requiring consistency, adding few-shot judging examples to improve consistency, using chain-of-thought style judging, and a reference-guided approach that first generates an independent reference answer and then grades against it, which substantially lowers math-judging failures in their tests. *LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.*\n\nEmpirically, GPT-4 as judge shows high agreement with humans on MT-bench and on sampled Chatbot Arena votes, with non-tie agreement reaching the mid 80 percent range and matching or exceeding measured human-human agreement in the same setting; agreement also increases when the quality gap between models is larger. Win-rate rankings produced by LLM judges closely track human rankings across models such as GPT-4, GPT-3.5, Claude, Vicuna, Alpaca, and LLaMA, and the paper argues for hybrid evaluation that combines capability benchmarks with preference-based judging. The release includes MT-bench questions, thousands of expert votes, and tens of thousands of arena conversations with human preferences, and the paper notes limitations around collapsing many helpfulness dimensions into one score and largely not addressing safety-focused evaluation.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2306.05685_paper.pdf"
}