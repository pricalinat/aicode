{
  "paper_id": "2602.21337",
  "title": "Commonground Benchmark",
  "category": "mini_program_service",
  "status": "success",
  "summary": "This paper introduces a benchmark to assess common ground in human–AI collaboration, arguing that effective long-horizon, genuinely collaborative work requires shared beliefs, assumptions, goals, and situation awareness that can be iteratively updated and repaired. It grounds the benchmark in classic theories and findings from human–human communication research on grounding, least collaborative effort, referential coordination, and the role of communication media. The authors position the benchmark as a way to move evaluation beyond one-shot assistant tasks toward interactions where success depends on sustained coordination and recoverable misunderstandings.\n\nThe benchmark is a collaborative puzzle-matching task adapted from prior common-ground studies: a Helper sees a target pattern, a Worker sees 24 ambiguous pieces, and together they must place 4 pieces correctly across multiple trials with repeating pieces to enable learning and convention formation. Two key manipulations are role symmetry, with the AI acting as Helper in some sessions and Worker in others, and situation awareness, with either a shared view of the Worker’s workspace or a non-shared view where partners rely on text alone. A confirmatory 2×2 between-subjects study recruits 40 English-fluent UK participants who collaborate with a fixed model, GPT4.1 with vision, using a web app; logs capture messages and actions and are analyzed via puzzle success, word counts, turn counts, extracted noun phrases for reference types, and dialogue-act labels for grounding functions.\n\nResults validate several expected patterns while also surfacing divergences specific to the human–AI setting and the tested model. Key findings include:\n- Shared view improves task performance, with significantly more exact puzzle matches than the non-shared view condition, but accuracy does not significantly improve across trials, failing to show the expected learning effect in success.\n- Overall communication volume does not significantly differ between shared and non-shared view, partly because Workers often substitute actions for language and because Helpers in non-shared view front-load precision using coordinates and model-introduced identifiers.\n- Lexical convergence is limited: most puzzle-piece noun phrases are used only by the model, shared vocabulary is small and can decrease over trials, and the model tends to persist with highly detailed descriptions rather than shortening to jointly efficient labels.\n- Referential practices show asymmetric adaptation: humans adopt identifier-based references introduced by the AI Worker, which can reduce typing but may raise cognitive load and does not produce symmetric efficiency gains because the AI continues pairing identifiers with descriptions.\n- Grounding behaviors differ by condition: in shared view, Worker clarification requests decline over trials, but Helpers rely on visual feedback to detect errors and continue repairs; in non-shared view, clarifications and repairs drop rather than increase, and acceptances dominate, aligned with observed model failures to engage with clarifications and to reliably update task state after repairs.\n\nThe paper concludes that the benchmark reproduces several theoretically expected effects of shared visual context and provides instruments to measure situation awareness, referential coordination, and grounding acts, while also revealing where human–AI interaction departs from human–human patterns. It emphasizes that observed behaviors and breakdowns are contingent on the specific model, prompting, and interface, and argues the benchmark offers a reusable way to compare models and design choices when the goal is genuine collaboration rather than transactional assistance.\n\n*This integration requires AI to move beyond acting as an assistant for informational or transactional tasks toward a genuine collaborative partner.*\n\n*When there is no shared view, it sometimes ignores or does not build upon clarification requests, and accepts statements without attempting verification.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/mini_program_service/2602.21337_CommonGround_Benchmark.pdf"
}