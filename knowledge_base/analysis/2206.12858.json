{
  "paper_id": "2206.12858",
  "title": "Recsys Eval Libraries",
  "category": "ecommerce_evaluation",
  "status": "success",
  "summary": "This paper examines how common offline recommender-system metrics are defined in research papers versus how they are implemented in popular Python evaluation libraries, arguing that inconsistencies can lead to misleading comparisons and irreproducible results. It finds that Precision is the only metric that is consistently understood and computed across the surveyed sources, while many other metric names hide materially different formulas or averaging choices. The authors also report that many papers do not clearly specify metric definitions, making it difficult to know what was actually measured and to compare results across studies.\n\n*We conclude that 47% of the studied papers do not have proper definitions,whichisadisturbingnumber,*  \n\nTo demonstrate practical impact, the authors compute HitRate, MAP, MRR, NDCG, Precision, Recall, and RocAuc on MovieLens-20m using an EASE recommender with a global timestamp split, treating ratings below 4.5 as negative and using cutoff 20 for most metrics. Across libraries including RePlay, DLRSEvaluation, DaisyRec, BetaRecSys, RecBole, Elliot, OpenRec, MSRecommenders, NeuRec, and rs_metrics, Precision and Recall match, but other metrics diverge due to differing interpretations and occasional bugs. Examples include HitRate being implemented as an average count of correct predictions in one library (producing values above 1), MRR sometimes summing reciprocals for all relevant items (allowing values above 1), MAP differing mainly by the chosen normalization term, NDCG splitting into binary versus weighted variants (plus differences like log base), and AUC having multiple meanings depending on whether predictions are stacked across users, averaged per user, or computed with top-k emphasis.\n\n*Precisionisonemetriceveryoneagreesupon*  \n\nThe paper then analyzes 15 influential papers that introduce models and their evaluation, checking whether metric definitions are fully specified, partially described, or deferred via references. Only about a third provide a full description of the metrics they use, while many rely on vague text or reference chains that still lack explicit formulas, leaving substantial ambiguity about the intended metric variants. The authors conclude that clearer, more comprehensive evaluation protocol descriptions and better standardization or explicit naming of metric variants are necessary, especially as metric complexity increases and leaves more room for divergent implementations.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/ecommerce_evaluation/2206.12858_recsys_eval_libraries.pdf"
}