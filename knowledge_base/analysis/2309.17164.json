{
  "paper_id": "2309.17164",
  "title": "Retail 786K",
  "category": "product_matching",
  "year": 2023,
  "timestamp": "2026-03-01T14:35:17.255605",
  "summary": "# RETAIL-786K: A Large-Scale Dataset for Visual Entity Matching\n\nThis paper introduces RETAIL-786K, the first publicly available large-scale dataset for visual entity matching in the retail domain, based on scanned advertisement leaflets from approximately 130 European retailers collected between 2016 and 2022.\n\n## Dataset Overview\n\nThe dataset contains approximately 786,179 manually annotated high-resolution product images representing about 18.8k different retail products grouped into 3,298 distinct entities. An entity represents an equivalence class of products that should be compared in price monitoring contexts—for example, different flavors of the same yogurt brand that different retailers use as promotional placeholders. The data is split into 748,715 training images and 37,464 test images, with strict constraints ensuring that images from the same retailer or subsidiary do not appear in both splits for any given entity.\n\n## The Visual Entity Matching Problem\n\nVisual entity matching differs fundamentally from standard image classification. The task requires transfer learning where, given only abstract examples of entities as sets of \"things that should belong together,\" the objective is to learn this grouping metric and apply it to unseen objects. This creates unique challenges: high intra-entity variance arises from different retailer leaflet designs, varying product flavors, diverse imaging perspectives, and inconsistent color distributions, while low extra-entity variance means visually similar products from different entities (such as different package sizes) can be difficult to distinguish.\n\n## Baseline Experiments\n\nThe authors conducted two baseline experiments. For classification, they treated entities as fixed classes and evaluated ResNet50, Vision Transformer, and ConvNeXt models, with ConvNeXt achieving the best test accuracy of 85.5% and an F1-score of 83.2%. For retrieval, they used the ROADMAP approach to find the 10 most similar test images to each query, achieving a Recall@10 of 56.34% and mAP@R of 72.23%. These results demonstrate that standard computer vision algorithms cannot adequately solve the visual EM problem—the best F1-score barely exceeds 83% while the more realistic retrieval approach accomplishes only 56% R@10, indicating significant room for novel algorithm development.\n\n## Dataset Availability and Limitations\n\nThe dataset is available for download at https://www.retail-786k.org/ with supporting code on GitHub. Several limitations are acknowledged: potential manual miss-annotations may exist despite cleanup efforts, textual information like prices and product descriptions is not included, and entity groupings are specific to the price monitoring application, so transferability to other tasks is uncertain. The dataset is licensed under CC BY-NC-ND 4.0 International.\n\n*Different retailers depict different flavors as product placeholders in their advertisements, causing high intra-entity variance*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}