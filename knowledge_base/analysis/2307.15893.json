{
  "paper_id": "2307.15893",
  "title": "Online Matching Bandit",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper presents Online Matching, a production real-time contextual bandit system used at YouTube to improve exploration of new and underexposed videos at massive scale. The authors argue that standard deep recommenders trained offline in batch are slow to adapt, reinforce feedback-loop bias, and struggle with distribution shift and fresh items; purely online bandits are hard to deploy due to huge action spaces, update synchronization, and latency constraints. Their core approach is a hybrid offline plus online design that prunes what to explore offline, then learns from direct user feedback online with low policy-update latency.\n\nOffline, a two-tower retrieval model co-embeds users and items, user embeddings are clustered into many user clusters, and a sparse bipartite graph is built by linking each cluster to its top W similar items. Online, they introduce Diag-LinUCB, a scalable approximation to LinUCB that keeps only diagonal covariance terms so updates are lightweight and fully distributed across graph edges, avoiding expensive matrix inversion and item-level synchronization. The system architecture includes a streaming log processor and a Bigtable-based feedback aggregation store keyed by cluster and item, a cluster to candidates lookup service, and a recommender service that ranks candidates by UCB for exploration or by mean reward for exploitation; newly added items enter with effectively infinite confidence to get fast initial exploration.\n\n*We propose Diag-LinUCB – a novel extension of the LinUCB algorithm – to enable distributed updates of bandits parameter in a scalable and timely manner.*  \n*Online Matching has been successfully deployed to YouTube for the use cases of fresh content discovery and item exploration.*\n\nEmpirically, Online Matching is evaluated via live experiments on a major YouTube recommendation surface as an additional candidate generation source. For Fresh Content Discovery, a small, frequently shuffled exploration slice of traffic learns rewards from a fixed UI slot, while an exploitation agent amplifies high-quality fresh videos to the rest of traffic using the learned means; they also use top k randomization to reduce over-exploration under delayed feedback. Reported results show topline satisfied engagement gains alongside much larger gains on fresh-content engagement, with a longer holdback indicating a small improvement in daily active users; they also quantify that added artificial delay in parameter aggregation decreases CTR and total rewards. For Corpus Exploration, they use a user-corpus co-diverted experimental design to measure discoverable corpus growth without leakage between control and treatment, exploring a much larger fresh corpus and reporting increased daily discoverable corpus size at different impression thresholds with only a small short-term engagement cost. The paper concludes that mixing offline pruning with real-time bandit learning can make exploration practical in large recommenders, and highlights open problems around mixed exploration and exploitation setups and dynamic item sets.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2307.15893_online_matching_bandit.pdf"
}