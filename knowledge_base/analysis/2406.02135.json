{
  "paper_id": "2406.02135",
  "title": "Interaction Relevance",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper presents ei-SRC, a robust interaction-based model for semantic relevance calculation in large-scale e-commerce search, aiming to close the robustness and generalization gap versus representation-based (siamese) approaches while still meeting strict online latency constraints. It targets core e-commerce challenges such as short ambiguous queries, long keyword-stuffed item titles, domain terminology that tokenizers fragment poorly, and performance drops when models are pruned or distilled for production.\n\nThe method combines three main ideas: a dynamic-length representation scheme that batch-trims all-zero padding columns to reduce attention cost (which scales with token length squared), plus online caching for frequent query tokens and high-frequency query item relevance scores; a professional terms recognition strategy that extends the tokenizer vocabulary using frequent domain words and adds NER-style embeddings to mark subject and key attributes; and a contrastive adversarial training protocol that strengthens robustness by training on adversarially perturbed inputs while also regularizing output distributions, with an additional heated-up softmax schedule to emphasize hard samples early. Offline experiments use 80 million logged query item pairs for continued pretraining and fine-tuning plus a manually annotated set of 250,000 pairs (with a 100,000 pair evaluation split), and report that a 3-layer ei-SRC achieves AUC 0.9033 and outperforms several larger baselines; ablations show dynamic length and caching cut GPU utilization and latency substantially, while CAT adds the largest accuracy gain.\n\nOnline, ei-SRC is deployed in the final-stage relevance module of alibaba.com search, filtering thousands of candidates per query before ranking and influencing exposure positions through the relevance score. Sequential A/B tests report statistically significant lifts, with the CAT step showing the largest incremental gains in CTR, CVR, and earnings rate, and additional manual audits on fixed exposure positions indicating improved semantic relevance classifications. *Notably, we have deployed it for the entire search traffic on alibaba.com, the largest B2B e-commerce platform in the world.* *This optimized model, with just 3 layers, outperforms traditional 12-layer BERT base models in efficiency and effectiveness.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2406.02135_interaction_relevance.pdf"
}