{
  "paper_id": "1802.00323",
  "title": "Metric Correlation",
  "category": "ecommerce_evaluation",
  "year": 2018,
  "timestamp": "2026-03-01T14:00:20.906845",
  "summary": "This paper studies how common information retrieval evaluation metrics relate to each other and whether missing, unreported metrics can be reliably predicted from the few metrics that papers typically do report. The motivation is practical: published studies often include only a small subset of scores due to time and space limits, reproducing runs is difficult, and broad sharing of system outputs has not taken hold. The authors propose metric prediction as a way to support fairer comparisons across papers and, separately, to reduce evaluation cost by predicting expensive deep measures from cheaper shallow ones.\n\nThey first compute and analyze correlations among 23 metrics using runs and relevance judgments from eight TREC ad hoc collections spanning web and news retrieval. Using a topic-wise dataset to avoid losing information through averaging, they report many very strong Pearson correlations, including tight groupings such as AP with R-Prec and bpref, RR with RBP at low persistence, and nDCG at shallow depth with mid-persistence RBP. Notably, P@10 and P@20 correlate extremely strongly with deeper RBP variants, and P@1000 stands out as comparatively weakly correlated with other measures, suggesting it captures behavior not reflected by most alternatives.\n\nThey then build a linear regression predictor that uses only other metric values as features, reflecting the reality of what is available in papers. Trained on older collections and evaluated on WT2012 to select predictors, then tested on WT2013 and WT2014, performance is measured with R2 for score accuracy and Kendall τ for preserving system rankings. The main finding is that several widely used metrics can be predicted accurately using only 2 to 3 other metrics, especially MAP, P@10, RBP(0.5), and RBP(0.8), while GMAP and ERR are consistently harder to predict and nDCG is also challenging despite sometimes good rank correlation. The authors also highlight cases where ranking agreement is high even when score accuracy is poor, underscoring the difference between predicting values versus predicting relative ordering.\n\n*Accurate prediction of MAP, P@10, and RBP can be achieved using only 2-3 other metrics.*\n\n*RBP(p=0.95) can be predicted with high accuracy using measures with only evaluation depth of 30.*\n\nFinally, they test the cost-reduction idea by treating deep targets such as RBP, MAP, nDCG, and precision at depths 1000 and 100 as high-cost measures, and using low-cost measures computed at shallow depths 10 to 50 as predictors. Results show RBP, especially with high persistence, is the most predictable deep measure from shallow evidence, reaching high Kendall τ and R2 once shallow depth reaches around 30, while precision remains the least predictable. The paper concludes that these results are a proof of concept for metric prediction to improve cross-paper comparability and potentially cut judging costs, and it calls for follow-on work with more data, tasks, and more sophisticated models.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}