{
  "paper_id": "2404.11343",
  "title": "Llm Cf Recommendation",
  "category": "product_matching",
  "status": "success",
  "summary": "The paper argues that recent LLM- and modality-based recommenders help in cold-start settings but often lose to simple collaborative filtering in warm settings because they do not directly use collaborative knowledge from user item interactions. It introduces A-LLMRec, an all-round recommender that explicitly transfers collaborative knowledge from a strong pre-trained CF recommender into an LLM while keeping both models frozen, aiming to perform well across cold items, warm items, cold users, few-shot, and cross-domain conditions. *we propose an efficient All-round LLM-based Recommender system, called A-LLMRec, that excels not only in the cold scenario but also in the warm scenario.*\n\nMethod-wise, A-LLMRec uses a two-stage alignment pipeline to bridge CF embeddings, item text, and the LLM token space without full fine-tuning. Stage 1 aligns CF item embeddings with text embeddings from SBERT using (1) an MSE matching loss, (2) item and text reconstruction losses to avoid representation collapse, and (3) a recommendation loss that preserves task-relevant collaborative signal; the result is a joint collaborative-text embedding that can fall back to a text-derived embedding for unseen or low-interaction items. Stage 2 projects user representations from the CF model and joint item embeddings into the LLM token embedding space via small MLPs and injects them as soft tokens into a structured prompt so the LLM can generate the next item title. *A-LLMRec requires the fine-tuning of neither the CF-RecSys nor the LLM, while only requiring an alignment network to be trained to bridge between them.*\n\nExperiments use four Amazon datasets (Movies and TV, Video Games, Beauty, Toys) with Hit@1 evaluation, comparing against CF baselines (including SASRec), modality-aware methods (MoRec, CTRL, RECFORMER), and LLM-based methods (LLM-Only, TALLRec, MLP-LLM), with OPT-6.7B as the LLM backbone and SASRec as the main CF backbone. Across overall results and targeted scenarios, A-LLMRec reports the best performance, showing particularly strong gains over TALLRec in warm settings while also remaining competitive or better in cold settings; variants using the text encoder at inference can help when collaborative signals are missing (cold items, few-shot, cross-domain). Ablations show each Stage 1 loss matters, freezing SBERT hurts, and removing user representations or joint embeddings from the prompt degrades Stage 2 performance; the model is also presented as backbone-agnostic and faster than LoRA fine-tuning approaches, and it can produce natural-language outputs in a favorite genre prediction task when given aligned collaborative signals.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2404.11343_llm_cf_recommendation.pdf"
}