{
  "paper_id": "2311.09008",
  "title": "End To End Tod",
  "category": "mini_program_service",
  "year": 2023,
  "timestamp": "2026-03-01T13:53:35.505084",
  "summary": "# End-to-End Task-Oriented Dialogue: A Comprehensive Survey\n\nThis paper presents a thorough survey of End-to-End Task-Oriented Dialogue (EToD) systems, which represent a significant advancement over traditional pipeline-based dialogue systems. The authors introduce a unified taxonomy that categorizes EToD approaches into two main types: Modularly EToD and Fully EToD, each with distinct characteristics in how they handle knowledge base retrieval and system response generation.\n\n## Background and Motivation\n\nTraditional task-oriented dialogue systems employ a pipeline approach consisting of four modular components: Natural Language Understanding (NLU) for extracting user intent and slot values, Dialogue State Tracking (DST) for maintaining belief states, Dialogue Policy Learning (DPL) for determining next actions, and Natural Language Generation (NLG) for producing system responses. While this modular approach has achieved impressive results, it suffers from two critical limitations that the survey identifies. First, each module is trained separately, preventing the model from leveraging shared knowledge across components. Second, errors accumulate sequentially through the pipeline, where mistakes in early stages propagate and amplify through subsequent modules, degrading overall system performance.\n\n*EToD can directly generate responses in an end-to-end fashion without modular training, which attracts escalating popularity.*\n\nEnd-to-end task-oriented dialogue systems emerged to address these drawbacks by training neural models that can process the entire dialogue workflow simultaneously. The fundamental difference lies in whether intermediate supervision is required and whether knowledge base retrieval is differentiable, creating a clear distinction between the two main categories that the survey examines in detail.\n\n## Modularly EToD\n\nModularly EToD generates system responses through sub-components like dialogue state tracking and policy learning, but trains all components end-to-end where parameters are optimized simultaneously. This approach bridges traditional pipeline systems and fully end-to-end models by maintaining modular structure while enabling joint optimization. The survey further divides this category into approaches without pre-trained language models and those leveraging pre-trained models for enhanced performance.\n\nEarly works in this space used supervised learning approaches, with Liu and Lane (2017) presenting an LSTM-based model that jointly learns belief tracking and KB retrieval. Subsequent research introduced innovations like Sequicity, which uses a two-stage CopyNet mechanism merging belief tracking and response generation in a sequence-to-sequence model. More advanced approaches incorporated reinforcement learning to supplement supervised learning for dialogue policy optimization, demonstrating reduced error propagation compared to pure SL settings.\n\nThe integration of pre-trained language models marked a significant advancement in Modularly EToD. Decoder-only models like GPT-2 were adopted as backbones, with Budzianowski and Vulic (2019) first attempting to employ pre-trained GPT for EToD by considering dialogue context, belief state, and database state as raw text inputs. Encoder-decoder architectures such as BART and T5 were also explored, with models like GALAXY introducing dialogue act prediction pre-training tasks for policy optimization and SPACE-3 employing five pre-training objectives to better understand semantic information for task-oriented dialogue.\n\nPerformance evaluations on the widely-used MultiWOZ2.0 and MultiWOZ2.1 datasets demonstrate that pre-trained models consistently outperform non-pre-trained approaches, with SPACE-3 achieving the highest combined scores of 111.0 on MultiWOZ2.0 and 110.8 on MultiWOZ2.1. The key takeaway is that knowledge inferred from pre-trained models significantly benefits EToD systems, and the shared nature of dialogue components allows modularly EToD to fully utilize cross-module knowledge.\n\n## Fully EToD\n\nFully EToD differs from modularly EToD in two crucial ways: it leverages differentiable KB retrieval instead of non-differentiable API calls, and it can generate system responses given only dialogue history and KB without requiring modular annotations. This approach dramatically reduces human annotation efforts since only dialogue-response pairs are needed, rather than intermediate supervision for dialogue state and policy actions.\n\nThe survey categorizes Fully EToD works based on knowledge base representation methods: entity triplet representation, row-level representation, and graph representation. Entity triplet representation stores each KB entity in (subject, relation, object) format, with works like KVRet employing key-value retrieval mechanisms and memory networks applied to model dependencies between related entity triplets. Row-level representation addresses the drawback of triplet approaches by capturing relationships across entities in the same row, with methods like KB-InfoBot utilizing posterior distributions over KB rows and HM2Seq storing KB rows and dialogue history in separate memories.\n\nGraph representation represents the most sophisticated approach, focusing on contextualizing entity embeddings by densely connecting entities and corresponding slot titles in dialogue history. GraphDialog applies graph-based multi-hop reasoning on entity graphs, while other works leverage transformer architectures to learn dependencies between dialogue history and KB through self-attention mechanisms. Performance on SMD and MultiWOZ2.1 datasets shows that graph-based methods like MAKER achieve the best results, with BLEU scores of 25.9 and Entity F1 of 71.3 on SMD.\n\n## Datasets and Evaluation Metrics\n\nThe survey covers commonly used datasets for both categories: CamRest676 for restaurant-domain evaluation with 408/136/136 dialogues for training/validation/testing, MultiWOZ2.0 containing over 8,000 dialogue sessions across 7 domains, and MultiWOZ2.1 as an improved version with corrected slot annotations. For Fully EToD, the Stanford Multi-domain Task-oriented Dialogue Dataset (SMD) covers navigation, weather, and calendar domains.\n\nEvaluation metrics differ between approaches: Modularly EToD uses BLEU for fluency, Inform for appropriate entity provision, Success for complete attribute satisfaction, and Combined as a comprehensive metric. Fully EToD employs BLEU and Entity F1 to evaluate generation quality and KB retrieval ability respectively.\n\n## Future Research Directions\n\nThe survey identifies several promising frontiers for EToD research. Large Language Models (LLMs) present both opportunities and challenges, including safety concerns, complex conversation management limitations, and domain adaptation difficulties. Pre-training paradigms for Fully EToD remain underexplored due to scarcity of knowledge-grounded dialogues for pre-training. Knowledge transfer from well-trained modularized ToD models to EToD systems offers an interesting research direction, as does reasoning interpretability through chain-of-thought methods in KB retrieval.\n\nAdditional challenges include multi-KB settings for real-world deployment, cross-lingual EToD for low-resource languages, and multi-modal EToD systems capable of handling both text and image inputs. The authors have created a public resource website at https://etods.net/ to help researchers stay updated on EToD progress.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}