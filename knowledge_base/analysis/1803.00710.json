{
  "paper_id": "1803.00710",
  "title": "Rl To Rank Ecommerce",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper frames e-commerce search ranking as a multi-step, session-level decision problem where ranking choices across pages are correlated, and optimizing each page independently can miss long-term value. It introduces a Search Session Markov Decision Process, or SSMDP, to model the session as an RL environment with states based on item page history, actions as ranking functions, and transitions driven by three outcomes after each page: conversion, abandonment, or continuation. The core business target is improved economic performance, so the paper connects the RL objective to expected gross merchandise volume, or GMV, and argues that future rewards must be considered to optimize session outcomes.\n\nKey ideas and contributions center on how to make RL practical for ranking at scale under sparse, skewed rewards:\n- SSMDP formalization with explicit notions of item page, item page history, and conversion, abandon, and continuing probabilities, plus a proof that SSMDP states satisfy the Markov property.\n- A transaction-focused reward design where reward is positive only on conversion and tied to deal price, making rewards both high-variance and highly imbalanced.\n- An analysis of the discount rate showing that maximizing undiscounted cumulative reward with γ = 1 aligns with maximizing expected GMV, while γ < 1 can under-optimize GMV.\n- A deterministic policy gradient method with full backup estimation, DPG-FBE, which learns with learned models of conversion probability b, continuing probability c, and expected deal price m to stabilize value estimation under sparse conversion signals, plus a deep variant DDPG-FBE for large-scale function approximation.\n\n*We theoretically prove that maximizing accumulative rewards is necessary, indicating that the different ranking steps in a session are tightly correlated rather than independent.*\n\nThe paper reports two experiment tracks: a simulator built from TaoBao item and behavior statistics, and an online deployment in the TaoBao search engine. In simulation, performance improves as γ increases, with the best results at γ = 1; DDPG-FBE substantially outperforms DDPG and several online learning-to-rank baselines, and the abstract highlights over 40 percent gain in total transaction amount in simulation. In production, the authors describe a stream-driven RL ranking system with a query planner, ranker, log center, RL training component, and an online key-value system, separating an online acting loop from an asynchronous learning loop; an A/B test over one week shows DDPG-FBE delivering about 2.7 to 4.3 percent more transaction amount per day than DDPG, and the paper cites over 30 percent GMV improvement in a Double11 festival deployment compared with an offline-trained LTR baseline.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/1803.00710_rl_to_rank_ecommerce.pdf"
}