{
  "paper_id": "2001.07853",
  "title": "Contextual Bandits Payment",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper studies a principal agent version of linear contextual bandits where a platform recommends one of N items to sequentially arriving myopic users, learns unknown item attribute vectors from bandit feedback, and can use payments or discounts to steer user choices toward exploration. Contexts can be fully adversarial and are not known to the platform when it sets payments, so naive myopic choice can create feedback loops that lock in poorly learned estimates and reduce social welfare relative to a clairvoyant benchmark.\n\n*We propose a contextual bandit based model to capture the learning and social welfare goals of a web platform in the presence of myopic users.* The core method, CBWHETEROGENIETY, uses randomized payments to effectively add Gaussian perturbations that induce covariate diversity in the perceived context, making least squares learning well behaved even under adversarial arrivals. Concretely, it samples a perturbation vector and sets per arm payments as an inner product between the perturbation and the arm estimate, so the userâ€™s argmax decision matches what it would have been under a perturbed context; the platform then updates its regression history using the perturbed context and a payment adjusted reward signal. Theoretical results include a high probability sublinear regret bound of order roughly sqrt(T N) up to polylog factors (with an initial forced exploration warm start), and a sublinear bound on expected cumulative payments scaling like N sqrt(T log(NT)); the paper also argues that with a fixed hard cap on total payments, incentives may be unable to change behavior under adversarial contexts, so performance cannot beat the best explore then commit style baselines in that regime.\n\n*Our key idea is to use payments to mimic perturbations.* Empirically, the paper compares CBWHETEROGENIETY to several baselines: a no payments greedy strategy, a scheme that pays enough to implement a LinUCB style choice, and chaining based payment methods including a restricted budget variant. On synthetic Gaussian context experiments with 8 arms and 4 dimensional contexts, no payments already achieves sublinear regret but is consistently beaten by payment based methods, while LinUCB style incentives can be costly and CBWHETEROGENIETY achieves competitive regret with relatively low total payments. On a real dataset experiment using an OpenML EEG dataset treated as a 2 arm problem with 14 dimensional features, the no payments strategy performs very well, with payment schemes close behind; the paper emphasizes that CBWHETEROGENIETY retains regret and payment guarantees without needing additional assumptions, unlike pure greedy. The conclusion highlights open directions including tighter lower bounds on payments for given regret, deeper links between algorithm efficacy and covariate diversity, extensions beyond myopic agents, and richer disclosures such as estimates plus uncertainty for risk aware users.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2001.07853_contextual_bandits_payment.pdf"
}