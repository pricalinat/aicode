{
  "paper_id": "2506.02839",
  "title": "Deepshop",
  "category": "ecommerce_evaluation",
  "year": 2025,
  "timestamp": "2026-03-01T14:00:01.477509",
  "summary": "DeepShop is an online benchmark for evaluating web agents on realistic, complex shopping tasks that require multi-attribute reasoning, applying site filters, and honoring sorting preferences. The paper argues existing benchmarks are often too simple and deterministic, so they do not reflect real shopping behavior on dynamic e-commerce sites. DeepShop is built to test whether agents can reliably complete layered queries across varied product domains and interface interactions.\n\n*Real shopping scenarios are inherently more layered, involving multi-dimensional product attributes, search filters, and user-specific sorting preferences.*  \n*Results show that RAG struggles with complex queries due to its lack of web interaction.*\n\nDeepShop is constructed in two stages: query diversity evolution (starting from 50 real user seed queries and generating broader goals across five categories: Books, Electronics, Home, Fashion, Sports) and query complexity evolution (iteratively adding product attributes, search filters, and sorting preferences for up to 5 evolution rounds, producing 600 queries, then selecting a balanced 150-query subset). The benchmark evaluates agents with an automated pipeline that decomposes each query into attribute, filter, and sort requirements, uses GPT-4o to judge each subgoal from trajectories and screenshots, and computes a holistic success only when all required components are satisfied; the paper also reports human vs GPT-4o agreement rates of 84 percent for attributes, 80 percent for filters, 82 percent for sorting, and 86 percent overall.\n\nMain experimental findings show a large gap between current methods and the benchmark demands:\n- Simple RAG using GPT-4o plus Google Search performs poorly, with all success rates under 8 percent, attributed to lacking web interaction for filters and sorting.\n- Web agents improve with interaction but still struggle to satisfy all constraints simultaneously; overall task success rises from 6.67 (Agent-E) to 10.67 (SeeAct) to 16.00 (WebVoyager) and peaks at 32.00 (Browser Use), with different agents strongest on different sub-dimensions.\n- Deep research systems (Gemini Deep Research and OpenAI Deep Research) reach higher fine-grained scores on attributes and sorting than web agents, but still achieve only 30 percent holistic task success, with filtering cited as a key weakness.\n\nThe analysis breaks down performance by category and complexity, reporting that visually driven categories like Fashion and Sports are especially hard and that success drops sharply as queries move from easy to medium to hard evolutions. Error analysis highlights recurring failure modes: weak grounding from HTML-only or vision-only perception, limited state assessment and replanning, constrained action spaces that make sliders and nested UI hard to use, lack of learning from execution across tasks, and hallucination or constraint-dropping issues in deep research systems including incorrect product attribute claims and invalid links. The paper closes with limitations (desktop-only focus, no multi-turn intent changes, incomplete coverage of cognitive shopping aspects) and provides a dataset card describing a small English dataset hosted on Hugging Face under CC BY 4.0, with fields for the main query plus attribute, filter, and sort subqueries, category, and difficulty.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}