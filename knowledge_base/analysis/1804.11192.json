{
  "paper_id": "1804.11192",
  "title": "Explainable Survey",
  "category": "ecommerce_evaluation",
  "year": 2018,
  "timestamp": "2026-03-01T14:00:53.308706",
  "summary": "This survey defines explainable recommendation as personalized recommender systems that deliver both strong recommendations and explanations that clarify why items are recommended, benefiting transparency, persuasiveness, trustworthiness, user satisfaction, and system debugging. It situates the topic within a broader 5W framing of recommender research, where explainable recommendation addresses why, alongside when, where, who, and what. It also distinguishes model-intrinsic approaches that build transparent decision mechanisms from model-agnostic or post hoc approaches that explain a black-box model after recommendations are produced, connecting both to human decision-making and cognitive perspectives.\n\nThe core contribution is a two-dimensional taxonomy of prior work: explanation information sources or display styles, and the algorithmic mechanisms that generate them. Explanation styles covered include relevant user or item explanations rooted in neighborhood collaborative filtering, feature-based explanations aligned with content-based recommendation, opinion or aspect-based explanations mined from reviews, sentence explanations via templates or natural language generation, visual explanations that highlight image regions of interest, and social explanations that leverage friend or social-network signals. On the modeling side, the survey organizes explainable methods across factorization and tensor approaches that align latent dimensions with explicit features, topic models that connect latent factors to interpretable topics, graph-based ranking and co-clustering methods, deep learning models using attention and generation components, knowledge graph methods that explain via entities and reasoning paths, rule mining approaches such as association rules, and post hoc techniques including association-rule explanations for black-box outputs, local surrogate explanations, bandit selection of explanations, reinforcement learning for explanation generation, and influence-function analysis to trace predictions back to training interactions.\n\nEvaluation is discussed as a dual objective: maintain recommendation quality while improving explanation quality, with user studies, online experiments, offline metrics, and qualitative case studies each serving different needs. The survey highlights common explanation-related goals such as transparency, scrutability, trust, effectiveness, persuasiveness, efficiency, and satisfaction, and introduces offline measures like explainability precision and recall, model fidelity for post hoc explanations, and text-generation metrics for sentence explanations when reference text is available. It also surveys application patterns in e-commerce, point-of-interest and travel, social platforms, and multimedia systems, then closes with open directions including explainable deep learning fidelity, knowledge-enhanced and multimodal explanations, context-aware and aggregated explanations, framing recommendation as reasoning, stronger NLP-based explanations and conversational why answering, improved evaluation protocols, user behavior analysis, and broader impacts such as trust, diversity, scrutability, and fairness relationships.\n\n*Explainable recommendation attempts to develop models that generate not only high-quality recommendations but also intuitive explanations.*\n\n*Explainable recommendation refers to personalized recommendation algorithms that address the problem of why.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}