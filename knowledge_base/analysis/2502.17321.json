{
  "paper_id": "2502.17321",
  "title": "Dialog Workflow Extraction",
  "category": "mini_program_service",
  "year": 2025,
  "timestamp": "2026-03-01T13:59:57.100508",
  "summary": "This paper proposes a framework to automatically extract structured dialog workflows for customer service AI agents from historical customer agent conversations, addressing the practical problem that high-quality workflows are often scarce, outdated, or undocumented. The core idea is to recover implicit procedural knowledge embedded in past interactions and convert it into reusable, decision-aware guidelines that improve response consistency and policy adherence. It also introduces an automated evaluation approach intended to scale beyond manual step-by-step judging, which the authors found to have only moderate human agreement in an initial study.\n\n*Our QA-CoT technique improves workflow extraction by 12.16% in average macro accuracy over the baseline.*\n\nThe extraction pipeline has two main stages. First, it retrieves the most relevant conversations for a given intent by extracting procedural elements from each conversation such as intent, slot values, and resolution steps, embedding those elements, clustering by intent, and selecting top conversations by similarity to an intent centroid to emphasize common, compliant procedures while filtering noise. Second, it generates workflows using a structured QA-based chain-of-thought approach called QA-CoT, which simulates a Guide and Implementer exchange to explicitly surface preconditions, branching decision points, and step dependencies; a single-pass generation of this exchange is reported to be both more accurate and more efficient than multi-turn simulation.\n\nThe paper evaluates workflow quality with an end-to-end simulation framework that decomposes workflows into sub-flows and scenarios, maps each scenario to required user and system information plus success criteria, then runs a customer bot and agent bot interaction driven by the predicted workflow and checks whether the simulated conversation meets the expected outcome. Experiments on the ABCD dataset and a synthetic, less noisy SynthABCD dataset show that selecting a subset of conversations can outperform using all conversations, that procedural-element similarity retrieval is generally strongest, and that QA-CoT outperforms common prompting baselines such as basic prompting, reflection, plan-first prompting, and ensemble methods across multiple LLMs, with especially large gains on SynthABCD for some models. Error analysis highlights frequent failure modes like confusing system-available fields with information that must be requested from the user, collapsing alternative options into a single rigid path, and mishandling conditional branching, while limitations note the Service AI focus, the need for intent labels or an extra intent classification step, and the gap between synthetic and real-world conversational complexity.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/codex/gpt-5.2",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}