{
  "paper_id": "2307.05974",
  "title": "Contrastive Cvr",
  "category": "product_matching",
  "year": 2023,
  "timestamp": "2026-03-01T13:39:52.975529",
  "summary": "# Contrastive Learning for Conversion Rate Prediction - Paper Summary\n\n## Overview\n\nThis research paper, published at SIGIR 2023 by researchers at Alibaba Group, addresses a critical challenge in online advertising systems: conversion rate (CVR) prediction under data sparsity conditions. The authors propose CL4CVR, a novel framework that combines supervised deep neural network-based CVR prediction with contrastive learning to learn better data representations from abundant unlabeled data. The key insight is that while advertising systems may have millions to billions of ads, users only click on a tiny fraction and convert on an even smaller subset, severely limiting the effectiveness of data-hungry deep learning models.\n\n## Key Innovations\n\nThe paper introduces three main technical contributions to tailor contrastive learning for CVR prediction:\n\n**Embedding Masking (EM)**: Unlike previous approaches in recommender systems that use feature masking (where some feature fields are hidden), the authors propose masking random dimensions within the embedding vectors instead. This ensures each augmented view contains all features but with different embedding dimensions masked, forcing the learned embeddings to be more representative and robust.\n\n**False Negative Elimination (FNE)**: The authors observe that in advertising systems, an ad may be shown to the same user multiple times, resulting in multiple samples with identical features but possibly different conversion labels. Traditional contrastive learning treats these as negatives, creating contradiction. FNE eliminates samples with the same features as the anchor from the negative set.\n\n**Supervised Positive Inclusion (SPI)**: Given the extreme sparsity of conversion events (precious label information), SPI includes additional positive samples for anchor samples that have conversion label = 1. This leverages the sparse but valuable conversion signal more effectively.\n\n## Experimental Results\n\nThe framework was evaluated on two real-world datasets: an industrial news feed advertising dataset from Alibaba and a public dataset from Taobao. CL4CVR achieved the best performance on both datasets, with CVR AUC improving from 0.8558 to 0.8637 on the industrial dataset (a gain of +0.0079) and from 0.6524 to 0.6590 on the public dataset (+0.0066). The paper notes that even small AUC improvements (e.g., 0.0020) can lead to significant online CVR increases (e.g., 3%). Ablation studies confirmed that each component (EM, FNE, SPI) contributes incrementally to the final performance, with all three working synergistically.\n\n## Technical Details\n\nThe framework uses ESMM (Entire Space Multi-Task Model) as the base supervised prediction model, which shares embeddings between CTR and CVR towers. The contrastive loss is combined with the supervised loss using a tunable balancing parameter α. The encoder is a simple MLP with layers of dimensions {512, 256, 128}. Temperature τ in the contrastive loss was found to work better when set larger, and the optimal α balances the supervised and contrastive tasks without over-emphasizing either.\n\n## Significance\n\nThis work represents an important advance in applying self-supervised learning techniques to structured tabular data problems in advertising. The authors make their source code publicly available at GitHub, enabling reproducibility and further research in this direction.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}