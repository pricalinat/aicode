{
  "paper_id": "2308.03869",
  "title": "Semantic Equivalence",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper tackles semantic equivalence in e-commerce search, where the same intent can be expressed through many surface variations, and where recognizing equivalence can improve both user experience and business outcomes. It frames a practical pipeline for query equivalence that combines text-level canonicalization with behavior-derived representations, and emphasizes using equivalence to choose the retrieval or ranking strategy that optimizes a target outcome such as recall or conversion. The core problems are: mapping queries to intent vectors, finding nearest-neighbor queries with equivalent or similar intent, and selecting the best alternative to optimize an objective.\n\nThe approach uses two complementary signals. Surface similarity is handled by canonicalizing queries to reduce superficial variation, focusing on word inflection, word order, compounding or decompounding, and removal of noise words, while also warning that some superficially similar pairs have different intent (for example blackberry versus blackberries, or shirtdress versus dress shirt). Behavioral similarity represents a query by aggregating vectors of products that users engage with after issuing that query, using embeddings from product titles and measuring query similarity via cosine similarity; it notes a robustness requirement such as at least 20 clicks to form reliable aggregates and reports that equivalent or near-equivalent intents often exceed a very high cosine threshold, around 0.98. To cover rare and unseen queries, it trains an online sentence-similarity model using offline-generated query pairs, oversampling pairs with cosine close to 1, adding a query-category classifier signal, and using a nearest-neighbor index (FAISS) over frequent queries to support online lookup.\n\nExperiments are evaluated indirectly because the authors state there is no standard e-commerce query-similarity benchmark, so they compare category distributions of clicks between queries and use Pearson correlation as the metric. They report results on public Amazon Shopping Queries data augmented with structured data, and on a proprietary eBay query set, with their approach outperforming a popular sentence-transformer baseline: Pearson correlation 0.85 versus 0.68 for all-MiniLM-L12-v2 on the proxy task, and a reported 0.87 on an internal eBay dataset and 0.85 on an ESCI-based setting. The conclusion argues the method generalizes beyond title-based product vectors to richer multimodal representations and calls for benchmark development to enable direct comparison across solutions.\n\n*We can then compute the similarity between two queries as the cosine similarity of their two vectors.*\n\n*In order to do so, we oversample query pairs with cosines close to 1.*",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2308.03869_semantic_equivalence.pdf"
}