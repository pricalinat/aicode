{
  "paper_id": "2202.05317",
  "title": "Bert Multitask",
  "category": "product_matching",
  "status": "success",
  "summary": "This paper proposes MLPR, an end-to-end multi-task learning framework for e-commerce product ranking that jointly models three sequential engagement signals: click, add-to-cart, and purchase. It targets two core product-search issues: vocabulary mismatch between short queries and short product titles, and sparsity when optimizing later-funnel signals like add-to-cart and purchase. *In this work, we propose a novel end-to-end multi-task learning framework for product ranking with BERT to address the above challenges.* *impression → click → add-to-cart → purchase.*\n\nThe model combines semantic matching from a domain-specific BERT with traditional learning-to-rank features in a single architecture, then reduces task interference via a mixture-of-experts style shared-bottom design and explicit task-specific experts. The pipeline has five main stages: deep and wide feature generation using BERT embeddings plus 243 normalized ranking features, two-stage extraction networks with per-task gating over shared experts and task-specific experts, per-task tower networks with attention that can transfer information from the previous task stage, probability transfer that enforces conditional dependencies across the behavior sequence, and uncertainty-weighted multi-task loss to balance task objectives during training. BERT is initialized from distilbert-base-uncased, then fine-tuned on engagement logs with negative sampling at a 1:20 relevant-to-sampled ratio, producing 256-dimensional query and item vectors and interaction features such as cosine similarity, concatenation, and Hadamard product.\n\nExperiments use one month of Walmart.com logs from October 2020, filtered to query-item pairs with more than five impressions, split 80/10/10 into train, validation, and test, totaling 467,622 queries, 4,286,211 items, 14,856,350 query-item pairs, and 312,926,929 impressions. Against baselines including XGBoost, single-task MLPs, shared-bottom MTL, ESM2, MMoE, PLE, and AITM, MLPR reports the strongest lifts over XGBoost on both prediction (AUC) and ranking (NDCG@1 and NDCG@5) for all three tasks, with statistically significant gains reported for the full model. Ablations indicate that adding task-specific experts, attention, probability transfer, and especially BERT fine-tuning each contributes to performance, and a qualitative example shows improved semantic relevance for the query half bed for kids compared with lexical matching. For efficiency, P99 latency for ranking 100 products drops from 171ms to 112ms when product embeddings are pre-computed, and the paper concludes with planned extensions such as incorporating product images and running online A/B tests.",
  "file_path": "/Users/rrp/Documents/aicode/data/papers/product_matching/2202.05317_bert_multitask.pdf"
}