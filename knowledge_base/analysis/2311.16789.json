{
  "paper_id": "2311.16789",
  "title": "Dialogue Systems Survey",
  "category": "mini_program_service",
  "year": 2023,
  "timestamp": "2026-03-01T13:53:48.056120",
  "summary": "# Survey of Language Model-Based Dialogue Systems Evolution\n\nThis comprehensive survey examines the evolution of dialogue systems in parallel with advancements in language models, tracing a path from early rule-based systems to modern large language model (LLM)-based conversational agents. The authors categorize this evolution into four distinct stages aligned with breakthroughs in language modeling: the early stage using statistical language models (1966-2015), the independent development of task-oriented and open-domain dialogue systems using neural language models (2015-2019), the fusion period using pre-trained language models (2019-2022), and the current LLM-based dialogue system era (2022-present).\n\nThe survey establishes that dialogue systems fundamentally split into two categories based on their primary objectives. Task-oriented dialogue systems (TOD) assist users in achieving specific goals such as making reservations or booking tickets, employing a modular pipeline comprising Natural Language Understanding (NLU) for intent classification and slot filling, Dialogue State Tracking (DST) for maintaining conversation state, Dialogue Policy Learning (DPL) for determining system actions, and Natural Language Generation (NLG) for producing responses. In contrast, open-domain dialogue systems (ODD) engage in free-form conversations without predefined tasks, focusing on social chit-chat, knowledge-grounded discussions, and question-answering interactions.\n\nThe evolution from neural language models to pre-trained language models marked a significant paradigm shift in dialogue system development. Pre-trained models like BERT, GPT-2, and their variants enabled the emergence of pre-trained dialogue models ( pre-train and fine-tuningPDMs) through approaches, with notable examples including DialoGPT, BlenderBot, Meena, and PLATO. This period also saw the fusion of different dialogue components—researchers began unifying sub-tasks within TOD into end-to-end architectures, while simultaneously exploring hybrid systems that combined task-oriented and open-domain capabilities.\n\nThe arrival of large language models (LLMs) such as ChatGPT, GPT-4, LLaMA, and their derivatives fundamentally transformed the landscape by blurring the boundaries between TOD and ODD. Modern LLM-based dialogue systems demonstrate three critical capabilities: internal reasoning for understanding dialogue context, external acting through interaction with databases, memory systems, and the internet, and the integration of both approaches for more sophisticated responses. Models like ChatGLM, Baichuan-Chat, and Qwen-Chat represent Chinese LLM efforts, while multilingual models extend these capabilities across languages.\n\nThe survey addresses critical challenges facing contemporary dialogue systems. Data-related issues include the need for universal data schemas and quality standards, alongside addressing linguistic and domain diversity gaps. Hallucination remains a significant concern, with researchers proposing mitigation strategies including retrieval-augmentation, model editing, and improved training techniques. Safety considerations encompass toxic language, social bias, and alignment with human values, requiring ongoing attention as systems become more capable. Multi-modality integration represents an emerging frontier, enabling dialogue systems to process and generate text, images, audio, and video content.\n\nThe authors conclude that the trajectory of dialogue system development follows what researchers term the \"bitter lesson\"—that scaling computation and data, rather than human-centric engineering, drives fundamental advances. As models become increasingly unified across tasks, languages, and modalities, they promise more natural and context-aware interactions, though challenges in learning paradigms, data formats, and architecture design persist as active research areas. *One of key lessons from the history of dialogue system is exactly the Bitter Lesson: progress has been driven by scaling computation and data, not human-centric engineering, less structure, more intelligence.*",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}