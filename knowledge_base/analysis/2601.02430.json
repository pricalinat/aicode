{
  "paper_id": "2601.02430",
  "title": "Webcoderbench",
  "category": "ecommerce_evaluation",
  "year": 2026,
  "timestamp": "2026-03-01T13:48:53.472681",
  "summary": "WebCoderBench is a comprehensive benchmark for evaluating large language models on web application generation, representing the first real-world-collected, generalizable, and interpretable benchmark in this domain. The benchmark addresses three key challenges in evaluating LLM-generated web apps: authenticity of user requirements, generality for open-ended requirements, and interpretable evaluation results without relying on ground-truth implementations.\n\nThe dataset comprises 1,572 authentic user requirements collected from an industrial partner's online LLM service, covering 20 application categories including entertainment (435), utility websites (404), online education (131), and enterprise backend (116). The requirements span diverse modalities—1,413 text-only, 123 with images, and 36 with URLs—and varying clarity levels from vague (78) to clear (764), with expression styles ranging from technical (683) to colloquial (724).\n\nThe evaluation framework employs 24 fine-grained metrics across nine perspectives: code quality, visual quality, content quality, performance quality, accessibility, maintainability, and three alignment dimensions (functional, visual, and content alignment). The authors combine rule-based metrics (static syntax checking, linting, accessibility tools) with an LLM-as-a-judge paradigm to achieve fully automated evaluation without ground-truth test cases.\n\nTo generate interpretable overall scores, the researchers conducted an internal survey with 1,076 views yielding 141 valid responses from diverse personas (data scientists, product managers, developers, designers, QA personnel). Using the Borda Count algorithm, they derived human-preference-aligned weights for each metric, ensuring the final scores reflect real-world user priorities rather than arbitrary heuristics.\n\nExperiments across 12 representative LLMs (including GPT-5-High, DeepSeek-V3, GLM-4.5, Qwen3-Coder, Gemini-2.5-Pro, and MiniMax-M2) and 2 LLM-based agents (Manus, MiniMaxAgent) revealed that no single model achieves dominant effectiveness across all 24 metrics. GPT-5-High attained the highest overall score, followed by GLM-4.5 and MiniMax-M2, with the gap between open-source and closed-source models narrowing to less than 6%. Notably, GLM-4.5 was the only model achieving above-average effectiveness across all nine perspectives, indicating balanced rather than universally strong capabilities.\n\nThe analysis also uncovered that different user personas exhibit distinct preferences: designers prioritize visual quality, code-related personas emphasize code quality and maintainability, while operations staff and legal personnel focus on content and performance quality. Requirements with images degrade model effectiveness, suggesting instability in mapping visual content to page structures, whereas colloquial requirements prove most effective as models better handle common public expressions.\n\n*no single model achieves dominant effectiveness across all 24 metrics* — this finding highlights both the comprehensiveness of the benchmark and the room for improvement in current LLM capabilities.",
  "llm_info": {
    "provider": "cli",
    "model": "cli/claude/sonnet",
    "maxCompletionTokens": null,
    "strategy": "single"
  }
}